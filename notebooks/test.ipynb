{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tokenizer import *\n",
    "from emoji.core import demojize\n",
    "#import language_tool_python\n",
    "#tool = language_tool_python.LanguageTool('it-IT')  # use a local server (automatically set up), language English\n",
    "import wordninja\n",
    "import splitter\n",
    "import enchant\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = wordninja.LanguageModel('660000_parole_italiane.txt.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.split('migrants',\"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manipolazione']\n",
      "['preoccupante']\n",
      "['io', 'ci', 'sono']\n",
      "['io', 'sto', 'conte']\n"
     ]
    }
   ],
   "source": [
    "print(lm.split('ipocritiacasaloro'))\n",
    "print(lm.split('preoccupante'))\n",
    "print(lm.split('iocisono'))\n",
    "print(lm.split('iostoconte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user Gentili, da Libia ove Es sembra contare meno proviene maggior numero migranti.Lei dov'√®, a fare campagna per s√¨?\n"
     ]
    }
   ],
   "source": [
    "s= \"@user @user Gentiloni,da Libia ove Is sembra contare meno proviene maggior numero migranti.Lei dov'√®,a fare campagna per s√¨?\"\n",
    "print(tool.correct(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPPERCASE_SENTENCE_START ['Ciao']\n",
      "MORFOLOGIK_RULE_IT_IT ['Cagno', 'Cagn√≤', 'Caino', 'Caio', 'Cairo', 'Calto', 'Camo', 'Canio', 'Canzo', 'Carlo', 'Carso', 'cablo', 'cabl√≤', 'cabro', 'cabr√≤', 'cacao', 'cacio', 'cado', 'cadr√≤', 'cago', 'cag√≤', 'calco', 'calc√≤', 'caldo', 'callo', 'calmo', 'calm√≤', 'calo', 'calvo', 'calzo', 'calz√≤', 'cal√≤', 'campo', 'camp√≤', 'cando', 'canto', 'cant√≤', 'capo', 'capro', 'capto', 'capt√≤', 'cardo', 'card√≤', 'cargo', 'cario', 'cari√≤', 'caro', 'carpo', 'carro', 'casco', 'casc√≤', 'caso', 'casso', 'cass√≤', 'casto', 'causo', 'caus√≤', 'cauto', 'cavo', 'cav√≤', 'cazzo', 'ccxx', 'ccxxi', 'ccxxv', 'ccxxx', 'cdxx', 'cdxxi', 'cdxxv', 'cdxxx', 'clxx', 'clxxi', 'clxxv', 'clxxx', 'cmxx', 'cmxxi', 'cmxxv', 'cmxxx', 'cxx', 'cxxi', 'cxxv', 'cxxx', 'cxxxi', 'cxxxv']\n"
     ]
    }
   ],
   "source": [
    "print(matches[0].ruleId, matches[0].replacements)\n",
    "print(matches[1].ruleId, matches[1].replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_text = {\n",
    "    '<kiss>': 'bacio',\n",
    "    '<happy>': 'felice',\n",
    "    '<laugh>': 'risata',\n",
    "    '<sad>': 'triste',\n",
    "    '<surprise>': 'sorpreso',\n",
    "    '<wink>': 'occhiolino',\n",
    "    '<tong>': 'faccia con lingua',\n",
    "    '<annoyed>': 'annoiato',\n",
    "    '<seallips>': 'labbra sigillate',\n",
    "    '<angel>': 'angelo',\n",
    "    '<devil>': 'diavolo',\n",
    "    '<highfive>' : 'batti il ‚Äã‚Äãcinque',\n",
    "    '<heart>': 'cuore'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ":) #nomadi no #acasaloro #wlitalia ##onu #lariachetira #iostoconsalvini #stopisis #lidlconnoi #bojano #migrants IlGiornale.it  #ParcodelleValli #benvenutoHitler?#preoccupante #IlGOverno presenta, nno!!! ??  le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN, url, user, URL, @USER\n",
      "\n",
      "felice <hashtag> nomadi </hashtag> no <hashtag> acasa loro </hashtag> <hashtag> wl italia </hashtag> <hashtag> onu </hashtag> <hashtag> laria chet ira </hashtag> <hashtag> ios to con salvini </hashtag> <hashtag> stop isis </hashtag> <hashtag> lidl con noi </hashtag> <hashtag> bojano </hashtag> <hashtag> migrants </hashtag> ilgiornale it <hashtag> parco delle valli </hashtag> <hashtag> benvenuto hitler </hashtag> ? <hashtag> preocc up ante </hashtag> <hashtag> il governo </hashtag> presenta nno ! ! ! ? ? le linee guida sulla scuola <hashtag> la buona scuola </hashtag> <url> url user url <user>\n"
     ]
    }
   ],
   "source": [
    "s = \":) #nomadi no #acasaloro #wlitalia ##onu #lariachetira #iostoconsalvini #stopisis #lidlconnoi #bojano #migrants IlGiornale.it  #ParcodelleValli #benvenutoHitler?#preoccupante #IlGOverno presenta, nno!!! ??  le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN, url, user, URL, @USER\"\n",
    "#s = \"BASTA CON QUESTI SCHIAFFI ALLA MISERIA DEGLI ITALIANI‚Ä¶.STRANIERI IRREGOLARI TUTTI FUORI E SUBITO!!!!!?ü§®üòüüò°üò°üò°üò°üáÆüáπüáÆüáπüáÆüáπ‚úåÔ∏è‚úåÔ∏è\"\n",
    "#s = \":)  @user üòÑüòÑ un rom che da‚Äô lezioni √® veramente singolare. Hai qualche decina di migliaia di consanguinei che delinquono da catechizzare prima di andare in giro a rompere le palle agli italiani. Inizia da questi due. Poi, se non stai bene in Italia, il mondo √® grande e ti aspetta üëã\"\n",
    "s = demojize(string=s, language='it')\n",
    "print()\n",
    "print(s)\n",
    "a = AlBERTo_Preprocessing(do_lower_case=True)\n",
    "b = a.preprocess(s)\n",
    "b = \" \".join(emoticons_text.get(ele, ele) for ele in b.split()) \n",
    "print()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>desc</th>\n",
       "      <th>unicode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üòÄ</td>\n",
       "      <td>Faccina Con Un Gran Sorriso</td>\n",
       "      <td>U+1F600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üòÉ</td>\n",
       "      <td>Faccina Con Un Gran Sorriso E Occhi Spalancati</td>\n",
       "      <td>U+1F603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üòÑ</td>\n",
       "      <td>Faccina Con Sorriso E Occhi Sorridenti</td>\n",
       "      <td>U+1F604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üòÅ</td>\n",
       "      <td>Faccina Raggiante Con Occhi Felici</td>\n",
       "      <td>U+1F601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üòÜ</td>\n",
       "      <td>Sorriso A Bocca Aperta Con Occhi Chiusi</td>\n",
       "      <td>U+1F606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji                                            desc  unicode\n",
       "0     üòÄ                     Faccina Con Un Gran Sorriso  U+1F600\n",
       "1     üòÉ  Faccina Con Un Gran Sorriso E Occhi Spalancati  U+1F603\n",
       "2     üòÑ          Faccina Con Sorriso E Occhi Sorridenti  U+1F604\n",
       "3     üòÅ              Faccina Raggiante Con Occhi Felici  U+1F601\n",
       "4     üòÜ         Sorriso A Bocca Aperta Con Occhi Chiusi  U+1F606"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"emojii.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001F604\n"
     ]
    }
   ],
   "source": [
    "f = open(\"emojii_dict.txt\",\"a\")\n",
    "# u':rosto_risonho:': u'\\U0001F600',\n",
    "s = \"Faccina Con Un Gran Sorriso\".lower().replace(\" \", \"_\")\n",
    "u = 'U+2620 U+FE0F'\n",
    "u = u[2:]\n",
    "len = 8\n",
    "u = u.zfill(len)\n",
    "print(x)\n",
    "f.write(\"\\tu':{}:': u'\\\\U{}'\\r\\n\".format(s, u))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"emojii_dict.txt\",\"a\")\n",
    "f.write(\"EMOJI_UNICODE_ITALIAN = {{\\r\\n\".format(s, u))\n",
    "for ind in df.index: \n",
    "    s = df['desc'][ind].lower().replace(\" \", \"_\")\n",
    "    u = \"\"\n",
    "    for code in df['unicode'][ind].split():\n",
    "        u+= \"\\\\U\"+code[2:].zfill(8)\n",
    "    f.write(\"\\tu':{}:': u'{}',\\r\\n\".format(s, u))\n",
    "f.write(\"}\\r\\n\")\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 6,839\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2066</td>\n",
       "      <td>√à terrorismo anche questo, per mettere in uno ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2045</td>\n",
       "      <td>@user @user infatti finch√© ci hanno guadagnato...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>Corriere: Tangenti, Mafia Capitale dimenticata...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1259</td>\n",
       "      <td>@user ad uno ad uno, perch√© quando i migranti ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>949</td>\n",
       "      <td>Il divertimento del giorno? Trovare i patrioti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9340</td>\n",
       "      <td>Gli stati nazionali devono essere pronti a rin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>9121</td>\n",
       "      <td>Il ministro dell'interno della Germania #Horst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>8549</td>\n",
       "      <td>#Salvini: In Italia troppi si sono montati la ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>9240</td>\n",
       "      <td>@user @user Chi giubila in buona fede non ha c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>8000</td>\n",
       "      <td>I giovani cristiani in #Etiopia sono indotti d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6839 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          sentences hs stereotype\n",
       "1     2066  √à terrorismo anche questo, per mettere in uno ...  0          0\n",
       "2     2045  @user @user infatti finch√© ci hanno guadagnato...  0          0\n",
       "3       61  Corriere: Tangenti, Mafia Capitale dimenticata...  0          0\n",
       "4     1259  @user ad uno ad uno, perch√© quando i migranti ...  0          0\n",
       "5      949  Il divertimento del giorno? Trovare i patrioti...  0          0\n",
       "...    ...                                                ... ..        ...\n",
       "6835  9340  Gli stati nazionali devono essere pronti a rin...  0          0\n",
       "6836  9121  Il ministro dell'interno della Germania #Horst...  0          0\n",
       "6837  8549  #Salvini: In Italia troppi si sono montati la ...  0          0\n",
       "6838  9240  @user @user Chi giubila in buona fede non ha c...  0          0\n",
       "6839  8000  I giovani cristiani in #Etiopia sono indotti d...  0          1\n",
       "\n",
       "[6839 rows x 4 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/haspeede2_dev/haspeede2_dev_taskAB.tsv\", delimiter=r'\\t', header=None, engine='python')\n",
    "df.columns =['id', 'sentences', 'hs', 'stereotype'] \n",
    "df = df.drop([0])\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'],\n",
    "    \n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>token_hashtags</th>\n",
       "      <th>token_hashtags_with_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2066</td>\n",
       "      <td>√à terrorismo anche questo, per mettere in uno ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2045</td>\n",
       "      <td>@user @user infatti finch√© ci hanno guadagnato...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#rom, #alemanno, #ipocriti]</td>\n",
       "      <td>[&lt;hashtag&gt; rom &lt;/hashtag&gt;, &lt;hashtag&gt; alemanno ...</td>\n",
       "      <td>[[rom], [alemanno], [ipocriti]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>Corriere: Tangenti, Mafia Capitale dimenticata...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#roma]</td>\n",
       "      <td>[&lt;hashtag&gt; roma &lt;/hashtag&gt;]</td>\n",
       "      <td>[[ro, ma]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1259</td>\n",
       "      <td>@user ad uno ad uno, perch√© quando i migranti ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>949</td>\n",
       "      <td>Il divertimento del giorno? Trovare i patrioti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#lidl]</td>\n",
       "      <td>[&lt;hashtag&gt; lidl &lt;/hashtag&gt;]</td>\n",
       "      <td>[[li, d, l]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9340</td>\n",
       "      <td>Gli stati nazionali devono essere pronti a rin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>9121</td>\n",
       "      <td>Il ministro dell'interno della Germania #Horst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#horstseehofer,sta]</td>\n",
       "      <td>[&lt;hashtag&gt; horst seehofer &lt;/hashtag&gt; , sta]</td>\n",
       "      <td>[[ho, r, s, t, s, e, e, h, o, f, e, r, sta]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>8549</td>\n",
       "      <td>#Salvini: In Italia troppi si sono montati la ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#salvini:, #iocisono, #piazzadelpopolo]</td>\n",
       "      <td>[&lt;hashtag&gt; salvini &lt;/hashtag&gt; :, &lt;hashtag&gt; io ...</td>\n",
       "      <td>[[salvi, ni], [io, ci, sono], [piazza, del, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>9240</td>\n",
       "      <td>@user @user Chi giubila in buona fede non ha c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>8000</td>\n",
       "      <td>I giovani cristiani in #Etiopia sono indotti d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[#etiopia]</td>\n",
       "      <td>[&lt;hashtag&gt; etiopia &lt;/hashtag&gt;]</td>\n",
       "      <td>[[et, io, pia]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6839 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          sentences hs stereotype  \\\n",
       "1     2066  √à terrorismo anche questo, per mettere in uno ...  0          0   \n",
       "2     2045  @user @user infatti finch√© ci hanno guadagnato...  0          0   \n",
       "3       61  Corriere: Tangenti, Mafia Capitale dimenticata...  0          0   \n",
       "4     1259  @user ad uno ad uno, perch√© quando i migranti ...  0          0   \n",
       "5      949  Il divertimento del giorno? Trovare i patrioti...  0          0   \n",
       "...    ...                                                ... ..        ...   \n",
       "6835  9340  Gli stati nazionali devono essere pronti a rin...  0          0   \n",
       "6836  9121  Il ministro dell'interno della Germania #Horst...  0          0   \n",
       "6837  8549  #Salvini: In Italia troppi si sono montati la ...  0          0   \n",
       "6838  9240  @user @user Chi giubila in buona fede non ha c...  0          0   \n",
       "6839  8000  I giovani cristiani in #Etiopia sono indotti d...  0          1   \n",
       "\n",
       "                                      hashtags  \\\n",
       "1                                         None   \n",
       "2                 [#rom, #alemanno, #ipocriti]   \n",
       "3                                      [#roma]   \n",
       "4                                         None   \n",
       "5                                      [#lidl]   \n",
       "...                                        ...   \n",
       "6835                                      None   \n",
       "6836                      [#horstseehofer,sta]   \n",
       "6837  [#salvini:, #iocisono, #piazzadelpopolo]   \n",
       "6838                                      None   \n",
       "6839                                [#etiopia]   \n",
       "\n",
       "                                         token_hashtags  \\\n",
       "1                                                    []   \n",
       "2     [<hashtag> rom </hashtag>, <hashtag> alemanno ...   \n",
       "3                           [<hashtag> roma </hashtag>]   \n",
       "4                                                    []   \n",
       "5                           [<hashtag> lidl </hashtag>]   \n",
       "...                                                 ...   \n",
       "6835                                                 []   \n",
       "6836        [<hashtag> horst seehofer </hashtag> , sta]   \n",
       "6837  [<hashtag> salvini </hashtag> :, <hashtag> io ...   \n",
       "6838                                                 []   \n",
       "6839                     [<hashtag> etiopia </hashtag>]   \n",
       "\n",
       "                              token_hashtags_with_split  \n",
       "1                                                    []  \n",
       "2                       [[rom], [alemanno], [ipocriti]]  \n",
       "3                                            [[ro, ma]]  \n",
       "4                                                    []  \n",
       "5                                          [[li, d, l]]  \n",
       "...                                                 ...  \n",
       "6835                                                 []  \n",
       "6836       [[ho, r, s, t, s, e, e, h, o, f, e, r, sta]]  \n",
       "6837  [[salvi, ni], [io, ci, sono], [piazza, del, po...  \n",
       "6838                                                 []  \n",
       "6839                                    [[et, io, pia]]  \n",
       "\n",
       "[6839 rows x 7 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_set = set()\n",
    "def find_hashtags(text):\n",
    "    result = re.findall(r'#\\S+', text)\n",
    "    result = [x.lower() for x in result]\n",
    "    if result is not None:\n",
    "        for i in result:\n",
    "            hashtag_set.add(i)\n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        return None\n",
    "def token_hashtags(hashtag_list, method=0):\n",
    "    result = []\n",
    "    if hashtag_list is not None:\n",
    "        for l in hashtag_list:\n",
    "            result.append(\" \".join(text_processor.pre_process_doc(l)))\n",
    "    return result\n",
    "\n",
    "df['hashtags'] = df['sentences'].apply(find_hashtags)\n",
    "df['token_hashtags'] = df['hashtags'].apply(token_hashtags)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_unique_hashtag = {'hashtag': [], 'splitted_hashtag': []}\n",
    "for hashtag in hashtag_set:\n",
    "    dict_unique_hashtag[\"hashtag\"].append(hashtag)\n",
    "    dict_unique_hashtag[\"splitted_hashtag\"].append(\" \".join(text_processor.pre_process_doc(hashtag)))\n",
    "\n",
    "df_unique_hashtag = pd.DataFrame.from_dict(dict_unique_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_hashtag.to_csv(\"df_unique_hashtag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Prova'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170776\n"
     ]
    }
   ],
   "source": [
    "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"/Users/Alessandro/Dev/repos/SaRaH/twitter128.bin\"), binary=True)\n",
    "print(len(wv_from_bin.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14995216,  2.592041  ,  0.28036016,  2.0142024 , -4.245268  ,\n",
       "        2.3333933 ,  1.6231191 ,  0.13999929, -3.5448651 , -2.0343199 ,\n",
       "        0.18803944,  2.7375963 , -0.76705986, -0.42937905,  0.7920406 ,\n",
       "        0.20687863,  1.3427061 , -1.7587823 , -0.2180561 , -2.0152984 ,\n",
       "        3.5882728 , -0.4098337 ,  1.4965817 ,  1.2868874 , -1.0891366 ,\n",
       "       -0.92938495, -0.9573534 ,  0.16773893, -1.9789966 ,  1.3852259 ,\n",
       "       -0.53394616, -3.0720916 , -0.17041197, -1.1238128 , -4.2002254 ,\n",
       "       -0.48587325,  0.5672549 ,  2.6026733 ,  1.0419768 , -3.1048963 ,\n",
       "       -0.81334066, -2.769505  , -1.4229465 ,  0.05092783,  1.2372069 ,\n",
       "       -1.2244759 ,  0.95990807, -0.66491586, -0.7079792 ,  1.6650658 ,\n",
       "       -2.1773558 ,  0.03435501,  0.80283904,  3.8588426 ,  0.60703945,\n",
       "       -0.7724422 ,  0.28368422,  0.81668323,  0.5851573 ,  1.8578557 ,\n",
       "       -2.0592015 ,  0.7007172 , -3.2143946 ,  1.8786803 ,  1.3005413 ,\n",
       "       -0.48632357, -0.16067211, -2.0873566 , -3.494025  ,  0.03661127,\n",
       "        0.38274238,  0.7980441 ,  0.19683117,  1.4923561 , -1.2173233 ,\n",
       "        0.30567575,  0.16244549,  0.5984168 ,  0.28994745, -2.0449805 ,\n",
       "        1.8703219 , -2.4679575 ,  3.908975  ,  3.5745702 ,  0.24699132,\n",
       "        1.6121693 ,  2.1991677 , -1.6058201 ,  0.03357302, -2.4207327 ,\n",
       "        0.96479493, -1.1497242 ,  1.3108989 ,  2.161626  ,  0.9521135 ,\n",
       "       -3.7437546 ,  2.516818  , -1.1687016 , -2.069765  , -1.0741942 ,\n",
       "       -0.58356667, -1.8145388 ,  0.45243105, -1.1584626 ,  2.030468  ,\n",
       "        1.3815118 , -3.934426  ,  0.27601218,  1.0647941 , -0.66337097,\n",
       "        1.0608333 ,  1.268259  ,  1.497997  ,  0.64449894, -1.5517865 ,\n",
       "        1.4228326 ,  3.2381248 ,  1.0947272 , -0.52075577,  0.5092861 ,\n",
       "        0.3331656 , -1.1907934 , -1.1601201 ,  2.490533  ,  1.372548  ,\n",
       "        0.19808948,  2.1067927 ,  1.3661578 ], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_bin['pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>text_length</th>\n",
       "      <th>%CAPS-LOCK words</th>\n",
       "      <th>esclamations</th>\n",
       "      <th>questions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokens</th>\n",
       "      <th>%bad_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['terrorismo', 'mettere', 'stato', 'soggezione...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['#rom', '#alemanno', '#ipocriti']</td>\n",
       "      <td>['infatti', 'finche', 'guadagnato', 'campi', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['#roma']</td>\n",
       "      <td>['corriere', 'tangenti', 'mafia', 'capitale', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['perche', 'quando', 'migranti', 'israeliti', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>949</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['#lidl']</td>\n",
       "      <td>['divertimento', 'giorno', 'trovare', 'patriot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>9340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['stati', 'nazionali', 'devono', 'essere', 'pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>9121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['#horst']</td>\n",
       "      <td>['ministro', 'interno', 'germania', '&lt;hashtag&gt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>8549</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['#salvini', '#iocisono', '#piazzadelpopolo']</td>\n",
       "      <td>['&lt;hashtag&gt;', 'salvini', '&lt;/hashtag&gt;', 'italia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['giubila', 'buona', 'fede', 'capito', 'niente...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>8000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>283</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['#etiopia']</td>\n",
       "      <td>['giovani', 'cristiani', '&lt;hashtag&gt;', 'etiopia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6837 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  hs  stereotype  text_length  %CAPS-LOCK words  esclamations  \\\n",
       "0     2066   0           0          118                 5             0   \n",
       "1     2045   0           0           93                 0             0   \n",
       "2       61   0           0           84                 0             0   \n",
       "3     1259   0           0          114                 0             0   \n",
       "4      949   0           0          138                 0             0   \n",
       "...    ...  ..         ...          ...               ...           ...   \n",
       "6832  9340   0           0          283                 0             0   \n",
       "6833  9121   0           0          277                 0             1   \n",
       "6834  8549   0           0          233                 0             0   \n",
       "6835  9240   0           0          198                 2             0   \n",
       "6836  8000   0           1          283                 4             1   \n",
       "\n",
       "      questions                                       hashtags  \\\n",
       "0             0                                            NaN   \n",
       "1             0             ['#rom', '#alemanno', '#ipocriti']   \n",
       "2             0                                      ['#roma']   \n",
       "3             0                                            NaN   \n",
       "4             1                                      ['#lidl']   \n",
       "...         ...                                            ...   \n",
       "6832          0                                            NaN   \n",
       "6833          1                                     ['#horst']   \n",
       "6834          0  ['#salvini', '#iocisono', '#piazzadelpopolo']   \n",
       "6835          0                                            NaN   \n",
       "6836          0                                   ['#etiopia']   \n",
       "\n",
       "                                                 tokens  %bad_words  \n",
       "0     ['terrorismo', 'mettere', 'stato', 'soggezione...           0  \n",
       "1     ['infatti', 'finche', 'guadagnato', 'campi', '...           0  \n",
       "2     ['corriere', 'tangenti', 'mafia', 'capitale', ...           0  \n",
       "3     ['perche', 'quando', 'migranti', 'israeliti', ...           0  \n",
       "4     ['divertimento', 'giorno', 'trovare', 'patriot...           0  \n",
       "...                                                 ...         ...  \n",
       "6832  ['stati', 'nazionali', 'devono', 'essere', 'pr...           0  \n",
       "6833  ['ministro', 'interno', 'germania', '<hashtag>...           0  \n",
       "6834  ['<hashtag>', 'salvini', '</hashtag>', 'italia...           0  \n",
       "6835  ['giubila', 'buona', 'fede', 'capito', 'niente...           0  \n",
       "6836  ['giovani', 'cristiani', '<hashtag>', 'etiopia...           0  \n",
       "\n",
       "[6837 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = '/Users/Alessandro/Dev/repos/SaRaH/'\n",
    "df = pd.read_csv(project_path+\"dataset/haspeede2/preprocessed/dev/dev.csv\", sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/haspeede2/preprocessed/dev/dev.csv\", sep=',')\n",
    "df = df[['tokens']]\n",
    "data_tokenized = []\n",
    "for index, row in df.iterrows():\n",
    "    data_tokenized.append(' '.join(ast.literal_eval(row['tokens'])))\n",
    "data_tokenized = [sentence.split() for sentence in data_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(size=128, window=5, min_count=5, sample=1e-4, negative=5, hs=0, workers=4)\n",
    "model.build_vocab(data_tokenized)\n",
    "total_examples = model.corpus_count\n",
    "model.build_vocab([list(wv_from_bin.vocab.keys())], update=True)\n",
    "model.intersect_word2vec_format(datapath(\"/Users/Alessandro/Dev/repos/SaRaH/twitter128.bin\"), binary=True, lockf=1.0)\n",
    "model.train(data_tokenized, total_examples=total_examples,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7de3ab8eeb89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"#salvini\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model[\"pc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"/Users/Alessandro/Dev/repos/SaRaH/model/word2vec/twitter128.bin\"), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique items in corpora are: 16810\n"
     ]
    }
   ],
   "source": [
    "unique_word = set([word for words in data_tokenized for word in words]) \n",
    "print(\"Unique items in corpora are:\", len(unique_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in unique_word:\n",
    "    try:\n",
    "        wv_from_bin[word]\n",
    "    except:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word[\"#immigrati\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
