{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tokenizer import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import ast\n",
    "\n",
    "import emoji\n",
    "import unicodedata\n",
    "\n",
    "import gzip\n",
    "\n",
    "import spacy_udpipe\n",
    "import language_tool_python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of files to read from\n",
    "train_val_AB_TSV = '../../../SaRaH/dataset/haspeede2/raw/haspeede2_dev/haspeede2_dev_taskAB.tsv'\n",
    "\n",
    "italian_words = '../../../SaRaH/dataset/words/parole_uniche.txt'\n",
    "bad_words = '../../../SaRaH/dataset/words/lista_badwords.txt'\n",
    "\n",
    "test_tweets_AB_TSV = '../../../SaRaH/dataset/haspeede2/raw/haspeede2_test/haspeede2_test_taskAB-tweets.tsv'\n",
    "test_news_AB_TSV = '../../../SaRaH/dataset/haspeede2/raw/haspeede2_test/haspeede2-test_taskAB-news.tsv'\n",
    "\n",
    "reference_tweets_AB_TSV = '../../../SaRaH/dataset/haspeede2/raw/haspeede2_reference/haspeede2_reference_taskAB-tweets.tsv'\n",
    "reference_news_AB_TSV = '../../../SaRaH/dataset/haspeede2/raw/haspeede2_reference/haspeede2_reference_taskAB-news.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Italian dictionary\n",
    "f1 = open(italian_words, 'r', encoding='utf8')\n",
    "\n",
    "italian_dict = [] #list of lowercase words\n",
    "\n",
    "for x in f1:\n",
    "    y = x.rstrip()\n",
    "    y = y.lower()\n",
    "    if y != '':\n",
    "        italian_dict.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bad Words\n",
    "f2 = open(bad_words, 'r', encoding='utf8')\n",
    "\n",
    "bad_words_dict = [] #list of lowercase words\n",
    "\n",
    "for x in f2:\n",
    "    y = x.rstrip()\n",
    "    y = y.lower()\n",
    "    if y != '':\n",
    "        bad_words_dict.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "df = pd.read_csv(train_val_AB_TSV, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Fixing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"text \": \"text\"}, inplace=True) #the text column is identified by 'text ' (with a space at the end), change  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2066</td>\n",
       "      <td>È terrorismo anche questo, per mettere in uno ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2045</td>\n",
       "      <td>@user @user infatti finché ci hanno guadagnato...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>Corriere: Tangenti, Mafia Capitale dimenticata...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1259</td>\n",
       "      <td>@user ad uno ad uno, perché quando i migranti ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>949</td>\n",
       "      <td>Il divertimento del giorno? Trovare i patrioti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>9340</td>\n",
       "      <td>Gli stati nazionali devono essere pronti a rin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>9121</td>\n",
       "      <td>Il ministro dell'interno della Germania #Horst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>8549</td>\n",
       "      <td>#Salvini: In Italia troppi si sono montati la ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9240</td>\n",
       "      <td>@user @user Chi giubila in buona fede non ha c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>8000</td>\n",
       "      <td>I giovani cristiani in #Etiopia sono indotti d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6837 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  hs  stereotype\n",
       "0     2066  È terrorismo anche questo, per mettere in uno ...   0           0\n",
       "1     2045  @user @user infatti finché ci hanno guadagnato...   0           0\n",
       "2       61  Corriere: Tangenti, Mafia Capitale dimenticata...   0           0\n",
       "3     1259  @user ad uno ad uno, perché quando i migranti ...   0           0\n",
       "4      949  Il divertimento del giorno? Trovare i patrioti...   0           0\n",
       "...    ...                                                ...  ..         ...\n",
       "6832  9340  Gli stati nazionali devono essere pronti a rin...   0           0\n",
       "6833  9121  Il ministro dell'interno della Germania #Horst...   0           0\n",
       "6834  8549  #Salvini: In Italia troppi si sono montati la ...   0           0\n",
       "6835  9240  @user @user Chi giubila in buona fede non ha c...   0           0\n",
       "6836  8000  I giovani cristiani in #Etiopia sono indotti d...   0           1\n",
       "\n",
       "[6837 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(text):\n",
    "    return re.sub(r'URL', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tag(text):\n",
    "    return re.sub(r'@user', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feature extraction: length of the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_length(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['text'].apply(text_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Translation of emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_emoticon(text):\n",
    "    text_result = emoji.demojize(text, language='it')\n",
    "    return text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(translate_emoticon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hashtags(text):\n",
    "    result = re.findall(r'#\\S+', text)\n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags'] = df['text'].apply(find_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/cbaziotis/ekphrasis\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'],\n",
    "    \n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtags(text):\n",
    "    return \" \".join(text_processor.pre_process_doc(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing number, percent, money, time, email, date and phone (and others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_to_remove = ['<number>', '<percent>', '<money>', '<time>', '<email>', '<date>', '<phone>', '<br/>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers1(text):\n",
    "    text_words = text.split()\n",
    "    new_words  = [word for word in text_words if word not in numbers_to_remove]\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Convert all emoticons written in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_text = {\n",
    "    '<kiss>': 'bacio',\n",
    "    '<happy>': 'felice',\n",
    "    '<laugh>': 'risata',\n",
    "    '<sad>': 'triste',\n",
    "    '<surprise>': 'sorpreso',\n",
    "    '<wink>': 'occhiolino',\n",
    "    '<tong>': 'faccia con lingua',\n",
    "    '<annoyed>': 'annoiato',\n",
    "    '<seallips>': 'labbra sigillate',\n",
    "    '<angel>': 'angelo',\n",
    "    '<devil>': 'diavolo',\n",
    "    '<highfive>' : 'batti il cinque',\n",
    "    '<heart>': 'cuore',\n",
    "    '<user>' : 'persona'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_emoticon_text(text):\n",
    "    text_words = text.split()\n",
    "    new_words  = [emoticons_text.get(ele, ele) for ele in text_words]\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_emoticon_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Replace the characters ‘&’, ‘@’ respectively in the letters ‘e’, ‘a’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_atand(text):\n",
    "    text_words = text.split()\n",
    "    \n",
    "    for i, word in enumerate(text_words):\n",
    "        if word == '&':\n",
    "            text_words[i] =  'e'\n",
    "        elif word == '@':\n",
    "            text_words[i] =  'a'\n",
    "            \n",
    "    return ' '.join(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(replace_atand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Add space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space(text):\n",
    "    words = text.split()\n",
    "    newwords = []\n",
    "    for word in words:\n",
    "        for i in range(0, len(word)):\n",
    "            if i != len(word)-1 and word[i] != ' ':\n",
    "                if word[i].islower() and word[i+1].isupper():\n",
    "                    word = word[:i+1] + ' ' + word[i+1:]\n",
    "        newwords.append(word)\n",
    "    return ' '.join(newwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(add_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feature extraction: percentage of words written in CAPS-LOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caps_lock_words(text):\n",
    "    words = text.split()\n",
    "    count_caps_lock = 0\n",
    "    number_of_words = len(words)\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isupper() == True:\n",
    "            count_caps_lock = count_caps_lock + 1\n",
    "            \n",
    "    return ((count_caps_lock*100)//number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['%CAPS-LOCK words'] = df['text'].apply(caps_lock_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feature extraction: number of ‘!’ inside the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esclamations(text):\n",
    "    return text.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['esclamations'] = df['text'].apply(esclamations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feature extraction: number of ‘?’ inside the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions(text):\n",
    "    return text.count('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['questions'] = df['text'].apply(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Convert all characters into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    return str(text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Normalizing Words #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_norm = {'wi-fi':'wifi'}\n",
    "\n",
    "def normalizing_words1(text):   \n",
    "    text_words = text.split()\n",
    "    new_words  = [word_norm.get(ele, ele) for ele in text_words]\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(normalizing_words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Punctuation #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation1(text):\n",
    "    text = re.sub(r'[•]', ' ', text) \n",
    "    text = re.sub(r'&lt;', ' ', text) #<\n",
    "    text = re.sub(r'&gt;', ' ', text) #>\n",
    "    return re.sub(r'[’]', ' ', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_punctuation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Replacement of accented characters with their unaccented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(strip_accents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cleaning Censured Bad Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_censured_bad_words(text):\n",
    "    text = \" \" + text + \" \"\n",
    "    text = re.sub(r' c[.x*@%#$^]+i ', ' coglioni ', text)\n",
    "    text = re.sub(r' c[.x*@%#$^]+e ', ' coglione ', text)\n",
    "    text = re.sub(r' c[.x*@%#$^]+o ', ' cazzo ', text) \n",
    "    text = re.sub(r' c[.x*@%#$^]+i ', ' cazzi ', text) \n",
    "    text = re.sub(r' m[.x*@%#$^]+a ', ' merda ', text) \n",
    "    text = re.sub(r' m[.x*@%#$^]+e ', ' merde ', text) \n",
    "    text = re.sub(r' c[.x*@%#$^]+ulo ', ' culo ', text) \n",
    "    text = re.sub(r' p[.x*@%#$^]+a ', ' puttana ', text)\n",
    "    text = re.sub(r' p[.x*@%#$^]+e ', ' puttane ', text)\n",
    "    text = re.sub(r' t[.x*@%#$^]+a ', ' troia ', text)\n",
    "    text = re.sub(r' t[.x*@%#$^]+e ', ' troie ', text)\n",
    "    text = re.sub(r' s[.x*@%#$^]+o ', ' stronzo ', text)\n",
    "    text = re.sub(r' s[.x*@%#$^]+i ', ' stronzi ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_censured_bad_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Punctuation #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation2(text):\n",
    "    text = re.sub(r\"[()[\\]{}]\", ' ', text) \n",
    "    text = re.sub(r\"[.,:;_-]\", ' ', text) \n",
    "    text = re.sub(r\"[+*]\", ' ', text) \n",
    "    text = re.sub(r\"[!?]\", ' ', text) \n",
    "    text = re.sub(r\"[£$%&'~\\\\|`=^]\", ' ', text) \n",
    "    return re.sub(r'[\"@]', ' ', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_punctuation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing nearby equal vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "def clean_vowels(text):\n",
    "    new_text = text\n",
    "    words = text.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in italian_dict:\n",
    "            new_string = word[0]\n",
    "            for i in range(1, len(word)):\n",
    "                if word[i] not in vowels:\n",
    "                    new_string = new_string + word[i]\n",
    "                else:\n",
    "                    if(word[i] != word[i-1]):\n",
    "                        new_string = new_string + word[i] \n",
    "\n",
    "            new_text = new_text.replace(word, new_string)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_vowels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing nearby equal consonants if they are more than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "consonants = ['b','c','d','f','g','h','k','l','m','n','p','q','r','s','t','v','x','y','z']\n",
    "\n",
    "def clean_consonants(text):\n",
    "    new_text = text\n",
    "    words = text.split()\n",
    "    \n",
    "    for word in words:\n",
    "        new_string = word[0]\n",
    "        for i in range(1, len(word)):\n",
    "            if word[i] not in consonants:\n",
    "                new_string = new_string + word[i]\n",
    "            else:\n",
    "                if(word[i] != word[i-1]):\n",
    "                    new_string = new_string + word[i]\n",
    "                elif i>=2 and (word[i] != word[i-2]):\n",
    "                    new_string = new_string + word[i]\n",
    "\n",
    "        new_text = new_text.replace(word, new_string)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_consonants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset\n",
    "df.to_csv('df_new1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "df = pd.read_csv('df_new1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>text_length</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>%CAPS-LOCK words</th>\n",
       "      <th>esclamations</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2066</td>\n",
       "      <td>e terrorismo anche questo   per mettere in uno stato di soggezione le persone e renderle innocue   mentre qualcuno</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2045</td>\n",
       "      <td>infatti finche ci hanno guadagnato con i campi &lt;hashtag&gt; rom &lt;/hashtag&gt; tutto era ok con &lt;hashtag&gt; alemanno &lt;/hashtag&gt; &lt;hashtag&gt; ipocriti &lt;/hashtag&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>['#rom', '#Alemanno', '#Ipocriti']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>corriere   tangenti   mafia capitale dimenticata mazzette su buche e campi rom &lt;hashtag&gt; roma &lt;/hashtag&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>['#roma']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1259</td>\n",
       "      <td>ad uno ad uno   perche quando i migranti israeliti arrivarono in terra di canan fecero fuori tutti i cananiti</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>949</td>\n",
       "      <td>il divertimento del giorno   trovare i patrioti italiani che inneggiano contro i rom facendo la spesa alla &lt;hashtag&gt; lidl &lt;/hashtag&gt;   multinazionale tedesca</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>['#Lidl']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>9340</td>\n",
       "      <td>gli stati nazionali devono essere pronti a rinunciare alla propria sovranita   lo ha detto la merkel   che ha aggiunto che gli stati nazionali non devono ascoltare la volonta dei loro cittadini quando si tratta di questioni che riguardano immigrazione   confini   o persino sovranita</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>9121</td>\n",
       "      <td>il ministro dell   interno della germania &lt;hashtag&gt; horst sehofer &lt;/hashtag&gt;   sta facendo la proposta di dare soldi agli immigrati che vogliono tornare a casa e aiutarli a creare un   attivita a casa loro e fare business con la germania   chi paga   una parte i crucchi e il resto l   europa   cioe io e voi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>['#HorstSeehofer,sta']</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>8549</td>\n",
       "      <td>&lt;hashtag&gt; salvini &lt;/hashtag&gt;   in italia troppi si sono montati la testa   io ringrazio dio e voi per questi mesi straordinari   vi raccontavano che su immigrazione non si poteva fare nulla   e bastato usare buonsenso e coraggio   &lt;hashtag&gt; io ci sono &lt;/hashtag&gt; &lt;hashtag&gt; piazza del popolo &lt;/hashtag&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "      <td>['#Salvini:', '#iocisono', '#piazzadelpopolo']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9240</td>\n",
       "      <td>chi giubila in buona fede non ha capito niente   purtroppo credo che i piu non siano in buona fede   i migranti sono un grosso business e chi finora li ha voluti non vuole perdere questo guadagno</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>8000</td>\n",
       "      <td>i giovani cristiani in &lt;hashtag&gt; etiopia &lt;/hashtag&gt; sono indotti dagli islamisti a convertirsi all   islam con promesse di lavoro   istruzione e aiuti abitativi   alcune miniere impiegano solo musulmani   lo riferisce ad acs un leader cristiano locale   anonimo per motivi di sicurezza   preghiamo per loro</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>283</td>\n",
       "      <td>['#Etiopia']</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6837 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0     2066   \n",
       "1     2045   \n",
       "2       61   \n",
       "3     1259   \n",
       "4      949   \n",
       "...    ...   \n",
       "6832  9340   \n",
       "6833  9121   \n",
       "6834  8549   \n",
       "6835  9240   \n",
       "6836  8000   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                          text  \\\n",
       "0                                                                                                                                                                                                    e terrorismo anche questo   per mettere in uno stato di soggezione le persone e renderle innocue   mentre qualcuno          \n",
       "1                                                                                                                                                                        infatti finche ci hanno guadagnato con i campi <hashtag> rom </hashtag> tutto era ok con <hashtag> alemanno </hashtag> <hashtag> ipocriti </hashtag>    \n",
       "2                                                                                                                                                                                                                    corriere   tangenti   mafia capitale dimenticata mazzette su buche e campi rom <hashtag> roma </hashtag>    \n",
       "3                                                                                                                                                                                                             ad uno ad uno   perche quando i migranti israeliti arrivarono in terra di canan fecero fuori tutti i cananiti      \n",
       "4                                                                                                                                                           il divertimento del giorno   trovare i patrioti italiani che inneggiano contro i rom facendo la spesa alla <hashtag> lidl </hashtag>   multinazionale tedesca        \n",
       "...                                                                                                                                                                                                                                                                                                                        ...   \n",
       "6832                              gli stati nazionali devono essere pronti a rinunciare alla propria sovranita   lo ha detto la merkel   che ha aggiunto che gli stati nazionali non devono ascoltare la volonta dei loro cittadini quando si tratta di questioni che riguardano immigrazione   confini   o persino sovranita    \n",
       "6833   il ministro dell   interno della germania <hashtag> horst sehofer </hashtag>   sta facendo la proposta di dare soldi agli immigrati che vogliono tornare a casa e aiutarli a creare un   attivita a casa loro e fare business con la germania   chi paga   una parte i crucchi e il resto l   europa   cioe io e voi      \n",
       "6834            <hashtag> salvini </hashtag>   in italia troppi si sono montati la testa   io ringrazio dio e voi per questi mesi straordinari   vi raccontavano che su immigrazione non si poteva fare nulla   e bastato usare buonsenso e coraggio   <hashtag> io ci sono </hashtag> <hashtag> piazza del popolo </hashtag>    \n",
       "6835                                                                                                                      chi giubila in buona fede non ha capito niente   purtroppo credo che i piu non siano in buona fede   i migranti sono un grosso business e chi finora li ha voluti non vuole perdere questo guadagno    \n",
       "6836     i giovani cristiani in <hashtag> etiopia </hashtag> sono indotti dagli islamisti a convertirsi all   islam con promesse di lavoro   istruzione e aiuti abitativi   alcune miniere impiegano solo musulmani   lo riferisce ad acs un leader cristiano locale   anonimo per motivi di sicurezza   preghiamo per loro      \n",
       "\n",
       "      hs  stereotype  text_length  \\\n",
       "0      0           0          118   \n",
       "1      0           0           93   \n",
       "2      0           0           84   \n",
       "3      0           0          114   \n",
       "4      0           0          138   \n",
       "...   ..         ...          ...   \n",
       "6832   0           0          283   \n",
       "6833   0           0          277   \n",
       "6834   0           0          233   \n",
       "6835   0           0          198   \n",
       "6836   0           1          283   \n",
       "\n",
       "                                            hashtags  %CAPS-LOCK words  \\\n",
       "0                                                NaN                 4   \n",
       "1                 ['#rom', '#Alemanno', '#Ipocriti']                 0   \n",
       "2                                          ['#roma']                 0   \n",
       "3                                                NaN                 0   \n",
       "4                                          ['#Lidl']                 0   \n",
       "...                                              ...               ...   \n",
       "6832                                             NaN                 0   \n",
       "6833                          ['#HorstSeehofer,sta']                 0   \n",
       "6834  ['#Salvini:', '#iocisono', '#piazzadelpopolo']                 0   \n",
       "6835                                             NaN                 2   \n",
       "6836                                    ['#Etiopia']                 3   \n",
       "\n",
       "      esclamations  questions  \n",
       "0                0          0  \n",
       "1                0          0  \n",
       "2                0          0  \n",
       "3                0          0  \n",
       "4                0          1  \n",
       "...            ...        ...  \n",
       "6832             0          0  \n",
       "6833             1          1  \n",
       "6834             0          0  \n",
       "6835             0          0  \n",
       "6836             1          0  \n",
       "\n",
       "[6837 rows x 9 columns]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Fixing Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_hashtags = {'pala di na':'paladina', 'ponti na':'pontina', 'ms na':'msna', 'drago na':'dragona', 'sp accio':'spaccio',\n",
    "                  'bianco enero':'bianco nero', 'laria chet ira':'lariachetira', 'nom as':'nomas', 'mi granti':'migranti', \n",
    "                  'as roma':'associazione sportiva roma', 'as li erdogan':'asli erdogan', 'hates pech':'hate speech',\n",
    "                  'razz ismo':'razzismo', 'prima gl italiani':'prima italiani', 'ios to con':'io sto con', 'sp rar':'sprar',\n",
    "                  'str as burgo':'strasburgo', 'is is':'isis', 'aria che ti rala':'lariachetira', 'immi grati':'immigrati',\n",
    "                  'blas femia':'blasfemia', 'imp icc agi one':'impiccagione', 'multicultural ismo':'multicultural ismo',\n",
    "                  'don neve late':'donne velate', 'bambi ne':'bambine', 'in fibula zi oni':'infibulazioni',\n",
    "                  'audi zi one':'audizione', 'mig razi one':'migrazione', 'so luzi one':'soluzione', 'stu pri':'stupri',\n",
    "                  'sosti tuzi one':'sostituzione', 'lariachetirala':'lariachetira', 'an is amri':'anis amri', \n",
    "                  'different is possible':'differente possibile', 'cassim at is':'cassimatis', 'chalie hebdo':'charlie hebdo',\n",
    "                  'terror is to':'terrorista', 'terror is ta':'terrorista', 'terrorist id is tratti':'terroristi distratti',\n",
    "                  'sopra vv is sut':'sopravvissut', 'is petto rio nu':'ispettori onu', 'islam ici':'islamici',\n",
    "                  'is lame violenza':'islam violenza', 'islam ed is educazione':'islam diseducazione', 'bo gabo xi':'bogaboxi',\n",
    "                  'islam ep rev aric azione':'islam prevaricazione', 'sp ese':'spese', 'prost it uzi one':'prostituzione',\n",
    "                  'chil ha visto':'chilhavisto', 'acalciinculo':'calci culo', 'clan destin':'clandestin', \n",
    "                  'wl italia':'viva italia', 'wil popolo':'viva popolo', 'toc chi':'tocchi', 'luss em burgo':'lussemburgo',\n",
    "                  'otto em ezo':'ottoemezzo', 'ipo crit':'ipocrit', 'ipo crisia':'ipocrisia', 'ipo cris ie':'ipocrisie',\n",
    "                  'sb arcano':'sbarcano', 'terror ist':'terrorist', 'cog lioni':'coglioni', 'sap eva telo':'sapevatelo',\n",
    "                  'piazza puli ta':'piazzapulita', 'nuove rosor se':'nuove risorse', 'per if eri':'periferi', 'in ps':'inps', \n",
    "                  'de grado':'degrado', 'prof ugh if vg':'profughi friuli venezia giulia', 'das po':'daspo', \n",
    "                  'animal if an at ici':'animali fanatici', 'if antas midi porto palo':'fantasmi porto palo',\n",
    "                  'crimini mmi grati':'crimini immigrati', 'face bok':'facebook', 'occulta men to':'occultamento',\n",
    "                  'infant ici d':'infanticid', 'reda zion ale':'redazionale', 'braccia let to':'braccialetto',\n",
    "                  'ri sorsa':'risorsa', 'stu prato r':'stuprator', 'per messi rim patri':'permessi rimpatri', \n",
    "                  'loc azione':'locazione', 'as tener si':'astenersi', 'appart amenti':'appartamenti', 'razzi sti':'razzisti',\n",
    "                  'servizi mmo bili ari':'servizi immobiliari', 'strag emigranti':'strage migranti', 'im be cilli':'imbecilli',\n",
    "                  'responsa bil it as oggetti va':'responsabilita soggettiva', 'centrisociale':'centri sociali',\n",
    "                  'as segni assist enzi ali':'assegni assistenziali', 'inter dici amol as an tanche':'interdiciamo santanche',\n",
    "                  'libere tco gitans':'liberetcogitans', 'sb archi':'sbarchi', 'rifugi ati':'rifugiati', 'sb ronzi':'sbronzi',\n",
    "                  'stocco lma':'stoccolma', 'pid dino':'piddino', 'com pl ici':'complici', 'pale rm':'palermo',\n",
    "                  'porta porta':'portaaporta', 'decretosalvini':'decreto salvini', 'decretosicurezza':'decreto sicurezza',\n",
    "                  'governodelcambiamento':'governo cambiamento', 'quartarepubblica':'quarta repubblica', 'asiabibi':'asia bibi',\n",
    "                  'desiremariottini':'desiree mariottini', 'virginiaraggi':'virginia raggi', 'fur ti':'furti',\n",
    "                  'multicultural ism':'multiculturalism', 'arrest at':'arrestat', 'differenz iata':'differenziata', \n",
    "                  'rifi uti':'rifiuti', 'mer de':'merde', 'quartarepubblica':'quarta repubblica', 'de linqu enza':'delinquenza',\n",
    "                  'fem mini st':'femminist', 'solid arieta':'solidarieta', 'acc og lie':'accogliere',\n",
    "                  'dirittiumani':'diritti umani', 'davidegerbino':'davide gerbino', 'whitegenocide':'genocidio bianco',\n",
    "                  'robertosaviano':'roberto saviano', 'lad re':'ladre', 'attu alita':'attualita', 'cial tron':'cialtron',\n",
    "                  'tu rust':'turist', 'sc hifo':'schifo', 'romaostia':'roma ostia', 'legal izzi amo':'legalizziamo',\n",
    "                  'risco priamo':'riscopriamo', 'fal sari':'falsari', 'rapina tor':'rapinator', 'tu rist':'turist',\n",
    "                  'se questra tor':'sequestrator', 'patron agg io':'patronaggio', 'acc oglio ni':'accoglioni',\n",
    "                  're at odium anita':'reato umanita', 'pid occhi':'pidocchi', 'dis occupazione':'disoccupazione',\n",
    "                  'fec cia':'feccia', 'rif lettere':'riflettere', 'tru ffa tor':'truffator', 'nazis ti':'nazisti',\n",
    "                  'civil tal los bando':'civilta allo sbando', 'imm grazi one':'immigrazione', 'populi sti':'populisti',\n",
    "                  'sovran isti':'sovranisti', 'fasciorazzisti':'fascisti razzisti', 'fintosinistra':'finto sinistra',\n",
    "                  'imp icc at':'impiccat', 'populis mo':'populismo', 'migrant ima italiani':'migranti mai italiani',\n",
    "                  'migrant our':'migrantour', 'sionis mo':'sionismo', 'olocau sto':'olocausto',\n",
    "                  'giulianopisapia':'giuliano pisapia', 'traffic ant':'trafficant', 'schiavi st':'schiavist', \n",
    "                  'immi gr ofili':'immigrofili', 'preocc up ante':'preoccupante', 'legit tima':'legittima',\n",
    "                  'terrorist ifil occidentali':'terroristi filoccidentali', 'cimb ro':'cimbro', 'traffic ant':'trafficant',\n",
    "                  'schiavi st':'schiavist', 'tru ffa':'truffa', 'freddoimmigrati':'freddo immigrati',\n",
    "                  'riscaldamentoitaliani':'riscaldamento italiani', 'dopoillegali':'dopo illegali',\n",
    "                  'vediamoterromotati':'vediamo terremotati', 'maisicurezza':'mai sicurezza', 'bar coni':'barconi', \n",
    "                  'approvatoitalexit':'approvato italexit', 'bar caccia':'barcaccia',\n",
    "                  'islam izz azione':'islamizzazione', 'organ izz azione':'organizzazione', 'immi ingrati':'immigrati ingrati',\n",
    "                  'imm irati':'immigrati', 'islamstop':'islam stop', 'combat ti amo':'combattiamo',\n",
    "                  'bastabergoglio':'basta bergoglio', 'dit tatura':'dittatura', 'sus sidi':'sussidi', 'pag are':'pagare',\n",
    "                  'spa rite':'sparite', 'pen sioni':'pensioni', 'imp unita':'impunita', 'primi tivi':'primitivi',\n",
    "                  'vio lenze':'violenze', 'eli mini amolo':'eliminiamolo', 'nazis mo':'nazismo', \n",
    "                  'uman it aris mo':'umanitarismo', 'ger archi':'gerarchi', 'imm ingrati':'immigrati ingrati',\n",
    "                  'iostoconsalvini':'io sto con salvini', 'stopinvasione':'stop invasione', \n",
    "                  'bastamoraledelcaxxo':'basta morale del cazzo', 'iostocon':'io sto con', 'lintervista':'intervista'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_fix(text):\n",
    "    for word in fixed_hashtags:\n",
    "        text = text.replace(word, fixed_hashtags[word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(hashtag_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(text):\n",
    "    text = \" \" + text + \" \"\n",
    "    #text = re.sub(r'\\d+[.,]\\d+', ' ', text) \n",
    "    #text = re.sub(r'\\d+/\\d+/\\d', ' ', text)\n",
    "    #return re.sub(r' [\\d]+', ' ', text) \n",
    "    return re.sub(r' \\d+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = ['uno', 'due', 'tre', 'quattro', 'cinque', 'sette', 'otto', 'nove', 'dieci', 'undici', 'dodici', 'tredici',\n",
    "           'quattordici', 'quindici', 'sedici', 'diciasette', 'diciotto', 'diciannove', 'venti', 'venticinque', 'trenta', 'quaranta', \n",
    "           'cinquanta', 'sessanta', 'settanta', 'ottanta', 'novanta', 'cento', 'mille', 'milioni', 'miliardi']\n",
    "\n",
    "def clean_letters_numbers(text):\n",
    "    text_words = text.split()\n",
    "    resultwords  = [word for word in text_words if word not in numbers]\n",
    "    return ' '.join(resultwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(clean_letters_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing laughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughs = ['ah', 'eh', 'he' 'ih', 'hi'] #non elimina ahahahah, ma solo ah\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "def clean_laughs(text):\n",
    "    #s = \"ahahahah ho fame io, eh eh\" -> \" ho fame io,\"\n",
    "    text_words = text.split()\n",
    "    new_words  = [word for word in text_words if word not in laughs]\n",
    "    \n",
    "    new_text = ' '.join(new_words)\n",
    "    \n",
    "    for i in new_words:\n",
    "        for k in vowels:\n",
    "            if ('h' in i) and (len(i) >= 4):\n",
    "                if (len(i) - 2) <= (i.count(k) + i.count('h')):\n",
    "                    new_text = new_text.replace(i, '')\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_laughs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing articles and preposition #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['il', 'lo', 'i', 'gli', 'la', 'le', 'un', 'uno', 'una', 'l']\n",
    "prepositions = ['del', 'della', 'dello', 'dell', 'dei', 'degli', 'delle', 'al', 'allo', 'alla', 'all', 'ai', 'alle', 'agli',\n",
    "                'dal', 'dallo', 'dalla', 'dall', 'dai', 'dagli', 'dalle', 'nel', 'nello', 'nella', 'nell', 'nei', 'negli',\n",
    "                'nelle', 'sul', 'sullo', 'sulla', 'sull', 'sui', 'sugli', 'sulle', 'di', 'a', 'da', 'in', 'con', 'per', 'su',\n",
    "                'fra', 'tra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_articles_prepositions1(text):\n",
    "    text_words = text.split()\n",
    "    resultwords  = [word for word in text_words if word not in articles and word not in prepositions]\n",
    "    return ' '.join(resultwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_articles_prepositions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Stopwords #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con', 'col', 'coi', 'da', 'dal', 'dallo', 'dai', 'dagli', 'dall', 'dagl', 'dalla', 'dalle', 'di', 'del', 'dello', 'dei', 'degli', 'dell', 'degl', 'della', 'delle', 'in', 'nel', 'nello', 'nei', 'negli', 'nell', 'negl', 'nella', 'nelle', 'su', 'sul', 'sullo', 'sui', 'sugli', 'sull', 'sugl', 'sulla', 'sulle', 'per', 'tra', 'contro', 'io', 'tu', 'lui', 'lei', 'noi', 'voi', 'loro', 'mio', 'mia', 'miei', 'mie', 'tuo', 'tua', 'tuoi', 'tue', 'suo', 'sua', 'suoi', 'sue', 'nostro', 'nostra', 'nostri', 'nostre', 'vostro', 'vostra', 'vostri', 'vostre', 'mi', 'ti', 'ci', 'vi', 'lo', 'la', 'li', 'le', 'gli', 'ne', 'il', 'un', 'uno', 'una', 'ma', 'ed', 'se', 'perché', 'anche', 'come', 'dov', 'dove', 'che', 'chi', 'cui', 'non', 'più', 'quale', 'quanto', 'quanti', 'quanta', 'quante', 'quello', 'quelli', 'quella', 'quelle', 'questo', 'questi', 'questa', 'queste', 'si', 'tutto', 'tutti', 'a', 'c', 'e', 'i', 'l', 'o', 'ho', 'hai', 'ha', 'abbiamo', 'avete', 'hanno', 'abbia', 'abbiate', 'abbiano', 'avrò', 'avrai', 'avrà', 'avremo', 'avrete', 'avranno', 'avrei', 'avresti', 'avrebbe', 'avremmo', 'avreste', 'avrebbero', 'avevo', 'avevi', 'aveva', 'avevamo', 'avevate', 'avevano', 'ebbi', 'avesti', 'ebbe', 'avemmo', 'aveste', 'ebbero', 'avessi', 'avesse', 'avessimo', 'avessero', 'avendo', 'avuto', 'avuta', 'avuti', 'avute', 'sono', 'sei', 'è', 'siamo', 'siete', 'sia', 'siate', 'siano', 'sarò', 'sarai', 'sarà', 'saremo', 'sarete', 'saranno', 'sarei', 'saresti', 'sarebbe', 'saremmo', 'sareste', 'sarebbero', 'ero', 'eri', 'era', 'eravamo', 'eravate', 'erano', 'fui', 'fosti', 'fu', 'fummo', 'foste', 'furono', 'fossi', 'fosse', 'fossimo', 'fossero', 'essendo', 'faccio', 'fai', 'facciamo', 'fanno', 'faccia', 'facciate', 'facciano', 'farò', 'farai', 'farà', 'faremo', 'farete', 'faranno', 'farei', 'faresti', 'farebbe', 'faremmo', 'fareste', 'farebbero', 'facevo', 'facevi', 'faceva', 'facevamo', 'facevate', 'facevano', 'feci', 'facesti', 'fece', 'facemmo', 'faceste', 'fecero', 'facessi', 'facesse', 'facessimo', 'facessero', 'facendo', 'sto', 'stai', 'sta', 'stiamo', 'stanno', 'stia', 'stiate', 'stiano', 'starò', 'starai', 'starà', 'staremo', 'starete', 'staranno', 'starei', 'staresti', 'starebbe', 'staremmo', 'stareste', 'starebbero', 'stavo', 'stavi', 'stava', 'stavamo', 'stavate', 'stavano', 'stetti', 'stesti', 'stette', 'stemmo', 'steste', 'stettero', 'stessi', 'stesse', 'stessimo', 'stessero', 'stando']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('italian')) #Non ci sono tutte\n",
    "\n",
    "def clean_stopwords(text):\n",
    "    text_words = text.split()\n",
    "    resultwords  = [word for word in text_words if word not in stop_words]\n",
    "    return ' '.join(resultwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Punctuation #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation3(text):\n",
    "    return re.sub(r'#', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_punctuation3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    tknzr=SocialTokenizer(lowercase=True)\n",
    "    return tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].apply(tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Punctuation #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation4(tokens):\n",
    "    punctuation = ['/','<','>']\n",
    "    resultwords  = [word for word in tokens if word not in punctuation]\n",
    "    return resultwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(clean_punctuation4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Replacement of the abbreviations with the respective words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_word = {'cmq':'comunque', 'gov':'governatori', 'fb':'facebook', 'tw':'twitter', 'juve':'juventus', 'ing':'ingegnere', \n",
    "             'sx':'sinistra', 'qdo':'quando', 'rep':'repubblica', 'grz':'grazie', 'ita':'italia', 'mln':'milioni', \n",
    "             'mld':'miliardi', 'pke':'perche', 'anke':'anche', 'cm':'come', 'dlla':'della', 'dlle':'delle', 'qst':'questa',\n",
    "             'ke':'che', 'nn':'non', 'sn':'sono', 'cn':'con', 'xk':'perche', 'xke':'perche'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abbreviation(tokens):\n",
    "    new_tokens  = [abbr_word.get(ele, ele) for ele in tokens]\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(replace_abbreviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_ita = {'europe':'europa', 'migration':'migrazione', 'muslim':'musulmano', 'immigration':'immigrazione',\n",
    "           'fuck':'cazzo', 'problem':'problema', 'refuges':'rifugiati', 'immigrants':'immigrati', 'attack':'attacco',\n",
    "           'error':'errore', 'illegal':'illegale', 'muslims':'musulmani', 'bastard':'bastardo', 'migrants':'migranti', \n",
    "           'wedding':'matrimonio', 'gipsy':'rom', 'belgium':'belgio', 'strasbourg':'strasburgo', 'death':'morte',\n",
    "           'invasion':'invasione', 'uman':'umano', 'rights':'diritti', 'earth':'terra', 'civil':'civile', 'terror':'terrore',\n",
    "           'angels':'angeli', 'low':'basso', 'cost':'costo', 'welcome':'benvenuto', 'attacks':'attacchi', 'berlin':'berlino',\n",
    "           'scum':'feccia', 'action':'azione'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tokens(tokens):\n",
    "    new_tokens  = [eng_ita.get(ele, ele) for ele in tokens]\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(translate_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word correction #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_correct1 = {'facebok':'facebook', 'retweted':'retweeted', 'coperative':'cooperative', 'coperazione':'cooperazione',\n",
    "                  'gogle':'google', 'creranno':'creeranno', 'twet':'tweet', 'bok':'book', 'mediterrane':'mediterranee',\n",
    "                  'kep':'keep', 'microare':'microaree', 'ise':'isee', 'desire':'desiree', 'temporane':'temporanee',\n",
    "                  'wekend':'weekend', 'coperativa':'cooperativa', 'vodo':'voodoo', 'cop':'coop', 'laure':'lauree',\n",
    "                  'canan':'canaan', 'cananiti':'canaaniti', 'bomerang':'boomerang', 'feyenord':'feyenoord',\n",
    "                  'scoter':'scooter', 'blomberg':'bloomberg'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_correction1(tokens):\n",
    "    new_tokens  = [error_correct1.get(ele, ele) for ele in tokens]\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(word_correction1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word correction #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_correct2 = {'pdioti':'pidioti', 'caxxo':'cazzo', 'dev':'deve', 'merd':'merda', 'terrorist':'terrorista',\n",
    "                  'multicultural':'multiculturale', 'litalia':'italia', 'mezz':'mezzo', 'responsab':'responsabile',\n",
    "                  'difender':'difendere', 'merdamma':'merda', 'mantener':'mantenere', 'uropa':'europa', 'strupri':'stupri',\n",
    "                  'mettetevelo':'mettere', 'uccideteli':'uccidere', 'laggente':'gente', 'kompagni':'compagni',\n",
    "                  'deiterroristi':'terroristi', 'leuropa':'europa', 'pdiota':'pidiota', 'kienge':'kyenge',\n",
    "                  'poracci':'poveracci', 'lislam':'islam', 'acquarius':'aquarius', 'pdoti':'pidioti', 'pdote':'pidiote',\n",
    "                  'democraziae':'democrazia', 'capode':'capodanno', 'eur':'euro', 'munnezz':'spazzatura', 'renzie':'renzi',\n",
    "                  'accoglioni':'coglioni', 'demmerda':'merda', 'tagliagolla':'tagliagola', 'pidoti':'pidioti', \n",
    "                  'pidote':'pidiote', 'immigrat':'immigrato', 'terroris':'terrorista', 'quant':'quanto', 'quanno':'quando',\n",
    "                  'tornar':'tornare', 'selfini':'selfie', 'selfi':'selfie', 'delinquent':'delinquenti', 'metidate':'meditate',\n",
    "                  'primat':'primato', 'perfavore':'favore', 'portono':'portano', 'atendono':'attendono',\n",
    "                  'cladestini':'clandestini', 'terrotisti':'terroristi', 'rosika':'rosica', 'piddiota':'pidiota', \n",
    "                  'aiutoamoli':'aiutiamoli', 'penicillin':'penicillina', 'invasone':'invasione', 'imprendittore':'imprenditore',\n",
    "                  'nindagini':'indagini', 'nferito':'ferito', 'sntenze':'sentenze', 'sullideologia':'ideologia',\n",
    "                  'ilcapoluogo':'capoluogo', 'quaeda':'qaeda', 'kossovaro':'kosovaro', 'nellappartamento':'appartamento',\n",
    "                  'attacheremo':'attaccheremo', 'chepalle':'palle', 'ilterrorismo':'terrorismo', 'partecipenti':'partecipanti',\n",
    "                  'sosituzione':'sostituzione', 'qaresima':'quaresima', 'carabineri':'carabinieri', 'ebbravo':'bravo',\n",
    "                  'alitalia':'italia', 'ridistrubuzione':'ridistribuzione', 'espusione':'espulsione', 'lonu':'onu',\n",
    "                  'leggittima':'legittima', 'glimmigrati':'immigrati', 'giovenissni':'giovanissimi', 'caxxi':'cazzi', \n",
    "                  'italini':'italiani', 'colgioni':'coglioni', 'marzagrato':'massacrato', 'inmersa':'immersa', \n",
    "                  'ilegali':'illegali', 'laddri':'ladri', 'santache':'santanche', 'quatar':'qatar', 'amazo':'ammazzo', \n",
    "                  'botiglia':'bottiglia', 'spaco':'spacco', 'assasino':'assassino', 'xfavore':'favore', \n",
    "                  'xbenismo':'perbenismo', 'kenia':'kenya', 'minkia':'minchia', 'cojone':'coglione', \n",
    "                  'immigratti':'immigrati', 'slvini':'salvini', 'xpregare':'pregare', 'mediocro':'mediocre', \n",
    "                  'tortutati':'torturati', 'antocostituzionale':'anticostituzionale', 'rassista':'razzista', \n",
    "                  'milirdi':'miliardi', 'eccerto':'certo', 'gnurant':'ignorante', 'immigrzione':'immigrazione', \n",
    "                  'spigazione':'spiegazione', 'medeglia':'medaglia', 'poltici':'politici', 'scomettiamo':'scommettiamo', \n",
    "                  'senegalwsi':'senegalesi', 'matrattano':'maltrattano', 'scapperano':'scapperanno', \n",
    "                  'mygranti':'migranti', 'djffondetelo':'diffondetelo', 'autirizzare':'autorizzare', 'notiziaro':'notiziario', \n",
    "                  'rivistano':'rovistano', 'immigrazi':'immigrati', 'islamophobia':'islamofobia', 'pisqua':'pasqua', \n",
    "                  'ncazzi':'cazzi', 'burqua':'burqa', 'halilovich':'halilovic', 'monologale':'monolocale', \n",
    "                  'pablovic':'pavlovic', 'schenghen':'schengen', 'incazare':'incazzare', 'spacare':'spaccare', \n",
    "                  'botilia':'bottiglia', 'faciamo':'facciamo', 'bizanzio':'bisanzio', 'nonvuole':'vuole',\n",
    "                  'leggittimati':'legittimati', 'ribiombare':'ripiombare', 'inportando':'importando', 'chidere':'chiedere',\n",
    "                  'specialnente':'specialmente', 'abbettere':'abbattere', 'ijaidisti':'jihadisti', 'ilpd':'pd', \n",
    "                  'giudce':'giudice', 'islamizzazzione':'islamizzazione', 'itaiani':'italiani', 'continuamo':'continuiamo', \n",
    "                  'mantenereli':'mantenerli', 'nonostate':'nonostante', 'bomberdate':'bombardate', 'buonusti':'buonisti', \n",
    "                  'riempendola':'riempiendola', 'rassisti':'razzisti', 'exstacomunitari':'extracomunitari', \n",
    "                  'conksciuto':'conosciuto', 'dimmerda':'merda', 'retroguadie':'retroguardie', 'terrosristi':'terroristi', \n",
    "                  'bussiness':'business', 'lsciati':'lasciati', 'sopresa':'sorpresa', 'mejo':'meglio', 'muiono':'muoiono', \n",
    "                  'distrugere':'distruggere', 'riepire':'riempire', 'deficente':'deficiente', 'fabblicato':'fabbricato', \n",
    "                  'sfrontatazza':'sfrontatezza', 'mohomed':'mohamed', 'srmplice':'semplice', 'ocvupazioni':'occupazioni', \n",
    "                  'guadeloupean':'guadeloupe', 'costuzionale':'costituzionale', 'drgli':'dirgli', 'kondividi':'condividi', \n",
    "                  'lancifiamme':'lanciafiamme', 'cosegnati':'consegnati', 'indistrubati':'indisturbati', 'behqti':'behati', \n",
    "                  'nabellezza':'bellezza', 'divertinento':'divertimento', 'smartphon':'smartphone', 'herzegovina':'erzegovina', \n",
    "                  'corroti':'corrotti', 'decelebrato':'decerebrato', 'namo':'andiamo', 'lavete':'levati', 'marzuk':'marzouk', \n",
    "                  'vaxxngul':'vaffanculo', 'excomunista':'comunista', 'vaccinan':'vaccinano', 'accusadi':'accusa', \n",
    "                  'insegnav':'insegnavano', 'jne':'jnews', 'stacon':'sta', 'kompagno':'compagno', 'compactfor':'compact',\n",
    "                  'sueddeutsche':'suddeutsche', 'regahliamo':'regaliamo', 'uccidrrebbero':'ucciderebbero', \n",
    "                  'starsburgo':'strasburgo', 'spantaneamente':'spontaneamente', 'strasburg':'strasburgo', 'lapacchia':'pacchia',\n",
    "                  'deglitaliani':'italiani', 'convinvono':'convivono', 'gravementne':'gravemente', 'kompagne':'compagne',\n",
    "                  'disitegreranno':'disintegreranno', 'pwrcentuali':'percentuali', 'judei':'giudei', 'piddioti':'pidioti',\n",
    "                  'pdidioti':'pidioti', 'professionoste':'professioniste', 'prwticanti':'praticanti', 'ammore':'amore',\n",
    "                  'ovviamemte':'ovviamente', 'ammaxxa':'ammazza', 'buddah':'buddha', 'invadeare':'invadere',\n",
    "                  'organizzaziobe':'organizzazione', 'pdjoti':'pidioti', 'tagadala':'tagcda', 'fankulo':'fanculo',\n",
    "                  'tru':'truffa', '<rubare>':'rubare'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_correction2(tokens):\n",
    "    new_tokens  = [error_correct2.get(ele, ele) for ele in tokens]\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(word_correction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_words(tokens):\n",
    "    words = ['ns', 'bla', 'slarp', 'rt', 'gt', 'lt', 'amp', 'xv', 'tutt', 'alors', 'na', 'cpt', 'je', 'suis', \n",
    "             'jesuis', 'gen', 'gl', 'enne', 'gogo', 'iv', 'perchi', 'enere', 'sennare', 'sap', 'dic', 'feb', 'xo', 'dere', \n",
    "             'daje', 'bom', 'cll', 'dll', 'ctta', 'tut', 'lol', 'cds', 'cvd', 'urc', 'tiv', 'nov', 'xix', 'ele', 'ndo', \n",
    "             'bau', 'xvi']\n",
    "    resultwords  = [word for word in tokens if word not in words]\n",
    "    return resultwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(removing_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Replacing Acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = {'unhcr':['alto', 'commissariato', 'nazioni', 'unite', 'rifugiati'], \n",
    "            'onu':['organizzazione', 'nazioni', 'unite'],\n",
    "            'fdi':['fratelli', 'italia'], \n",
    "            'msna':['minori', 'stranieri', 'accompagnati'], \n",
    "            'rdc':['reddito', 'cittadinanza'],\n",
    "            'gus':['gruppo', 'umana', 'solidarieta'], \n",
    "            'sprar':['sistema', 'protezione', 'richiedenti', 'asilo'],\n",
    "            'anpi':['associazione', 'nazionale', 'partigiani', 'italia'], \n",
    "            'anac':['autorita', 'nazionale', 'anticorruzione'],\n",
    "            'lgbt':['lesbiche', 'gay', 'bisessuali', 'transgender'], \n",
    "            'ln':['lega', 'nord'], \n",
    "            'ue':['unione', 'europea'],\n",
    "            'msf':['medici','senza','frontiere'], \n",
    "            'ispi':['istituto','studi','politica','internazionale'],\n",
    "            'cpr':['centri','permanenza','rimpatri'], \n",
    "            'pd':['partito', 'democratico'], \n",
    "            'gc':['guardia', 'costiera'],\n",
    "            'inps':['istituto','nazionale','previdenza','sociale'],\n",
    "            'cdm':['consiglio', 'ministri'], \n",
    "            'pdl':['popolo', 'liberta'], \n",
    "            'atac':['azienda', 'tramvie', 'autobus', 'comune', 'roma'],\n",
    "            'tav':['treno', 'alta', 'velocita'], \n",
    "            'isee':['situazione', 'economica', 'equivalente'],\n",
    "            'usa':['stati', 'uniti', 'america'], \n",
    "            'onlus':['organizzazione', 'lucrativa', 'utilita', 'sociale'],\n",
    "            'acsim':['associazione', 'centro', 'servizi', 'immigrati', 'marche'], \n",
    "            'aids':['sindrome', 'immuno', 'deficienza', 'acquisita'], \n",
    "            'eu':['unione', 'europea'],\n",
    "            'ong':['organizzazione', 'governativa'], \n",
    "            'nwo':['nuovo', 'ordine', 'mondiale'],\n",
    "            'pil':['prodotto', 'interno', 'lordo'], \n",
    "            'cgil':['confederazione', 'generale', 'lavoro'],\n",
    "            'cdt':['corriere', 'ticino'], \n",
    "            'ptv':['societa', 'televisiva', 'pakistan'],\n",
    "            'syriza':['coalizione', 'sinistra', 'radicale'], \n",
    "            'fiom':['federazione', 'impiegati', 'operai', 'metallurgici'],\n",
    "            'lgbtq':['lesbiche', 'gay', 'bisessuali', 'transgender', 'queer'], \n",
    "            'rpl':['radio', 'padania', 'libera'],\n",
    "            'arci':['associazione', 'ricreativa', 'culturale', 'italiana'],\n",
    "            'ofcs':['osservatorio', 'focus', 'cultura', 'sicurezza'],\n",
    "            'm5s':['movimento', 'cinque', 'stelle'],\n",
    "            'wm5s':['movimento', 'cinque', 'stelle']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_acronyms(tokens):\n",
    "    for i in range(0, len(tokens)):\n",
    "        word = tokens[i]\n",
    "        if word in acronyms:\n",
    "            tokens[i] = acronyms[word][0]\n",
    "            if len(acronyms[word]) > 1:\n",
    "                for j in range(1, len(acronyms[word])):\n",
    "                    tokens.insert(i+j, acronyms[word][j])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(replace_acronyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing tokens of lenth <= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_length1(tokens):\n",
    "    resultwords  = [word for word in tokens if len(word)>2]\n",
    "    return resultwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(clean_length1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing articles and preposition #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['il', 'lo', 'i', 'gli', 'la', 'le', 'un', 'uno', 'una', 'l']\n",
    "prepositions = ['del', 'della', 'dello', 'dell', 'dei', 'degli', 'delle', 'al', 'allo', 'alla', 'all', 'ai', 'alle', 'agli',\n",
    "                'dal', 'dallo', 'dalla', 'dall', 'dai', 'dagli', 'dalle', 'nel', 'nello', 'nella', 'nell', 'nei', 'negli',\n",
    "                'nelle', 'sul', 'sullo', 'sulla', 'sull', 'sui', 'sugli', 'sulle', 'di', 'a', 'da', 'in', 'con', 'per', 'su',\n",
    "                'fra', 'tra']\n",
    "\n",
    "def clean_articles_prepositions2(tokens):\n",
    "    resultwords  = [word for word in tokens if word not in articles and word not in prepositions]\n",
    "    return resultwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(clean_articles_prepositions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing Stopwords #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('italian')) #Non ci sono tutte\n",
    "\n",
    "def clean_stopwords2(tokens):\n",
    "    resultwords  = [word for word in tokens if word not in stop_words]\n",
    "    return resultwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(clean_stopwords2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stop_words = set(stopwords.words('english')) #Non ci sono tutte\n",
    "\n",
    "def clean_eng_stopwords(tokens):\n",
    "    resultwords  = [word for word in tokens if word not in eng_stop_words]\n",
    "    return resultwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(clean_eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feature extraction: percentage of Bad Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_bad_words(tokens):\n",
    "    n_words = 0\n",
    "    n_bad_words = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word != '<hashtag>' and word != '</hashtag>':\n",
    "            n_words = n_words + 1\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in bad_words_dict:\n",
    "            n_bad_words = n_bad_words + 1\n",
    "        \n",
    "    return ((n_bad_words*100)//n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['%bad_words'] = df['tokens'].apply(percentage_bad_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    for word in df['tokens'][i]:\n",
    "        if word not in inverted_index and word != '<hashtag>' and word != '</hashtag>':\n",
    "            inverted_index[word] = 1\n",
    "        elif word != '<hashtag>' and word != '</hashtag>':\n",
    "            inverted_index[word] = inverted_index[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16845"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9306"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1 = 0\n",
    "for x in inverted_index.values():\n",
    "    if x == 1:\n",
    "        count_1 = count_1 + 1\n",
    "count_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Spelling Error Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_error = ['lidl', 'isis', 'aquarius', 'soros', 'macron', 'hebdo', 'kyenge', 'jihadisti', 'caritas', 'ilgiornale',\n",
    "            'pidioti', 'sky', 'tagada', 'voxnews', 'casapound', 'immigrazionisti', 'assad', 'selfie', 'iussoli', 'webitalia',\n",
    "            'reddit', 'tgcom', 'padania', 'afroislamici', 'brexit', 'shariah', 'twitter', 'pride', 'censis', 'skytg', 'obama',\n",
    "            'borsoni', 'times', 'today', 'fanpage', 'piddina', 'iphone', 'marenostrum', 'vauro', 'immigrazionista', 'uk', \n",
    "            'mediaset', 'silicon', 'valley', 'santanche', 'repubblicait', 'cyber', 'scampia', 'mondialista', 'lambrate',\n",
    "            'araboafricani', 'juncker', 'italietta', 'thuram', 'euronews', 'mosul', 'zuckerberg', 'sportnews', 'vladimir',\n",
    "            'luxuria', 'ciociaria', 'liberoquotidiano', 'trudeau', 'agenparl', 'schengen', 'mahershala', 'radiorpl', \n",
    "            'primavalle', 'lulic', 'masod', 'lepen', 'gheddafi','lahore', 'afroislamica', 'feyen', 'allahu', 'adolf',\n",
    "            'fakenews', 'bilderberg', 'wifi', 'civilissima', 'bunga', 'salviniani', 'notiziona', 'portaaporta', 'coop',\n",
    "            'cattivisti', 'huffpost', 'rohingya','piddine', 'dirottarli', 'novax', 'profit', 'complottista', 'microaree',\n",
    "            'peppista', 'piddino', 'globalisti','sophia', 'arms', 'facebook', 'juventus', 'frontex', 'social', 'retweeted', \n",
    "            'lariachetira', 'google', 'updated', 'tweet', 'chilhavisto', 'ottoemezzo', 'amnesty', 'piazzapulita', 'maxirissa', \n",
    "            'anticorruzione', 'fake', 'mediterranee', 'keep', 'liberetcogitans', 'daspo', 'rolls', 'royce', 'salvini', \n",
    "            'quintacolonna', 'openarms', 'sovranista', 'sovranisti', 'asia', 'bibi', 'desiree', 'mariottini', 'rugantina',\n",
    "            'virginia', 'raggi', 'bloc', 'boldrina', 'multiculturale', 'porrajmos', 'nomadare', 'tpi', 'cornigliano',\n",
    "            'migrantes', 'straparlare', 'sapevatelo', 'sovranista', 'sovranisti', 'politically', 'globalcompactformigration',\n",
    "            'crocerossa', 'linate', 'iovotono', 'musulmania', 'domenicalive', 'opensociety', 'roberto', 'saviano', 'pidiota',\n",
    "            'cuffiette', 'gender', 'road', 'rss', 'nomas', 'sinistronzi', 'corcolle', 'unar', 'bolzaneto', 'tracing',\n",
    "            'dateci', 'portateli', 'rimandiamoli', 'rimandarli', 'accoglierne', 'pagarci', 'educarli', 'usarli', 'integrarci',\n",
    "            'eliminarlo', 'perdetela', 'mandateli', 'impugnamo', 'apparte', 'cacciamoli', 'sottometterci', 'imporci', \n",
    "            'mandiamola', 'ricordiamolo', 'rimpatriarli', 'riportateli', 'omnimilano', 'riprenderci', 'iuventa', 'alerts',\n",
    "            'synthe', 'pig', 'fdo', 'act', 'jobs', 'work', 'dott', 'selfie', 'mandandoli', 'strafulmini', 'reductio',\n",
    "            'riacesi', 'jane', 'alil', 'vardar', 'testaccio', 'jaunes', 'lets', 'watch', 'pidiote', 'like', 'schmidt', 'bremme',\n",
    "            'caricarli', 'costruirselo', 'malvedenti', 'concertone', 'lineapress', 'costringendoci', 'indebitarci', 'imbonirsi', \n",
    "            'depenalizzazioni', 'startup', 'softair', 'hillary', 'padovaoggi', 'morning', 'dortmund', 'soffocandomi', \n",
    "            'macchinoni', 'mattinonline', 'spiegatelo', 'convertirli', 'sgozzarci', 'rispondetegli', 'segnalarne',\n",
    "            'birmingham', 'raqqa', 'sudtirol', 'carrefour', 'farceli', 'disilluderla', 'definiteli', 'finanziarli', 'domandone',\n",
    "            'dusseldorf', 'radicalizzazioni', 'consegnateci', 'billionaire', 'conviverci', 'sgomberarlo', 'syria', \n",
    "            'rinchiuderle', 'riprenderle', 'mangiarmeli', 'rigopiano', 'xfactor', 'sorelline', 'volevasi', 'erasmus', \n",
    "            'tgpuglia', 'dategliela', 'levatela', 'dubai', 'tagliarli', 'facciamoli', 'riportarli', 'candidabili', \n",
    "            'donarle', 'sterilizzarlo', 'comandarci', 'mattinodinapoli', 'vestirli', 'fornirne', 'aiutiamoli', 'principalmete',\n",
    "            'sputacchiarlo', 'trattiamoli', 'qatar', 'rinchiuderli', 'rimandarle', 'alcatraz', 'rapinarci', 'dominarci',\n",
    "            'cinquestelle', 'mondialisti', 'infilartelo', 'dateli', 'cercasi', 'riprenderseli', 'accogliamoli', 'glorificarli',\n",
    "            'piazzarci', 'eliminandole', 'chiudervi', 'mettervelo', 'allontaniamoli', 'zalando', 'occultandoli', 'bitcoin', \n",
    "            'ultraricchi', 'mostrandola', 'estorcergli', 'furbissimo', 'rolex', 'microimpresa', 'sopraffarci', 'pagarla', \n",
    "            'fenicenews', 'imporlo', 'riflettiamoci', 'gridiamolo', 'sbuffetto', 'spalmandola', 'condizionarle', 'venderti',\n",
    "            'sminuirli', 'spaventatissimo', 'immigrazioniste', 'boldriniana', 'teneteveli', 'descrittoci', 'spenderne',\n",
    "            'gestirseli', 'rosicone', 'promuoverne', 'chiuderlo', 'ciaone', 'votarla', 'cagnolina', 'potenziarsi', \n",
    "            'tutelarli', 'conquistarlo', 'ucciderli', 'integriamoli', 'scriviamolo', 'condannarli', 'confiscarla', 'adibirla',\n",
    "            'cacciandoli', 'afroislamiche', 'trasmetterli', 'contraddirlo', 'ripulirlo', 'popoliamole', 'guardateli', \n",
    "            'mortirolo', 'mastercard', 'isolarci', 'statene', 'visitarne', 'normalissima', 'gestendoli', 'affondatela', \n",
    "            'confondiamoli', 'salvateli', 'risollevarmi', 'uccidili', 'giornaloni', 'imponendoci', 'pulirselo', \n",
    "            'costringendoli', 'dotandoli', 'nostrum', 'chiediamogli', 'mandiamogli', 'costarci', 'diffonderli', 'fourquet',\n",
    "            'cresciutelli', 'follower', 'darglieli', 'dicesi', 'ingozzarti', 'sgombrarli', 'complottisti', 'maxirata', \n",
    "            'guess', 'pluripregiudicata', 'incontrarci', 'qaeda', 'tenerveli', 'bananalandia', 'prendersele', 'heidelberg', \n",
    "            'shabaab','rovistaggio', 'essersela', 'sumaya', 'besiktas', 'risvoltini', 'abbassargli', 'masharipov', 'burqa', \n",
    "            'halilovic', 'ciociariareport', 'buhari', 'ilfattoquotidiano', 'pavlovic', 'volkswagen', 'frustalo',\n",
    "            'sborroni', 'teleuniverso', 'porsche', 'karlov', 'lamezia', 'rispediamoli', 'trattienili', 'luttwak', 'schengen',\n",
    "            'ahadith', 'hamas', 'piegarci', 'samhain', 'jebreal', 'portateveli', 'comunistelli', 'choudary', 'hollande', \n",
    "            'sfigurarla', 'coranisti', 'sterminarci', 'istigarli', 'rimandateli', 'gestirli', 'razzaccia', 'polverizziamoli',\n",
    "            'fermiamoli', 'eliminarli', 'chiudiamoli', 'islamofobi', 'sovvenzionamenti', 'evitiamola', 'islamicamente', \n",
    "            'chiudetele', 'vushaj', 'adidas', 'esodati', 'sajida', 'salvarne', 'excelsior', 'ogboru', 'rapinarlo',  \n",
    "            'liberiamola', 'massou', 'cacciateli', 'scarichiamoli', 'amatissimi', 'accollarseli', 'espelleteli', 'occuparvi', \n",
    "            'sfamarli', 'films', 'telefilms', 'mohamed', 'impietosirvi', 'tuscolana', 'bergogliani', 'guadeloupe', \n",
    "            'estinguervi', 'filoimmigrazionisti', 'raccattarli', 'rapinarmi', 'bruciargli', 'pippone', 'chiamiamoli', \n",
    "            'diamogliela', 'corvacci', 'tirargli', 'cacciarla', 'schiavizzarci', 'maledettissime', 'behati', 'kothen', \n",
    "            'allnews', 'qaradawi', 'proteggerti', 'regalargli', 'renziana', 'leuca', 'accogliergli', 'tornarvi', 'ricostruirli',\n",
    "            'arsizio', 'prendeteli', 'difendervi', 'sparkasse', 'disciplinarla', 'ignorantismo', 'bogaboxi', 'salvargli',\n",
    "            'soundcheck', 'erzegovina', 'sovranismi', 'nagativamente', 'venghino', 'interculturalita', 'contenerne', 'capitelo',\n",
    "            'condividili', 'sports', 'aufstehen', 'presuntuosamente', 'wojtyla', 'salvinisti', 'frenarla', 'froidiano', \n",
    "            'ficcatelo', 'afferrali', 'offrila', 'picchiarla', 'portiamone', 'jinping', 'tijuana', 'succhiasangue', 'bep',\n",
    "            'mortacci', 'digital', 'bowl', 'panigarola', 'marzouk', 'sigonella', 'equitalia', 'auchan', 'pulirti', 'orlandik',\n",
    "            'hotspot', 'qibad', 'senadid', 'boomerang', 'canaaniti', 'canaan', 'giornalettismo', 'stef', 'filoccidentali', \n",
    "            'hate', 'cimbro', 'imbecillissima', 'moviment', 'pound', 'feyenoord', 'italexit', 'herrou', 'rothschild',\n",
    "            'fogliettone', 'jnews', 'riccastri', 'verhofstadt', 'suddeutsche', 'zeitung', 'update', 'bokwango', 'cedric',  \n",
    "            'passeur', 'beslan', 'maranella', 'convertirgli', 'rimandarne', 'prendeteveli', 'cucciolini', 'rubarla',             \n",
    "            'sbarcateli', 'castrarci', 'arrestarli', 'tranquillissima', 'totalitaristi', 'arrivateci', 'bloomberg', 'midterms',\n",
    "            'indignamo', 'impedirvi', 'avengers', 'ricontattarli', 'organizzandoli', 'controllatissimi', 'sottocategoria',\n",
    "            'rimpatriamoli', 'salviniane', 'diffondetelo', 'rimpatriate', 'metterglielo' ,'firmarlo', 'chiuderci', 'cicciolina',\n",
    "            'insegnarglielo', 'accettandone', 'pagarli', 'buddha', 'portarseli', 'rasserenarli', 'sosteniamola', 'concedervi',\n",
    "            'trasformateci', 'stanarli', 'arrestarla', 'deutsche', 'ramallah', 'euroburocrati', 'libya', 'ultimora',\n",
    "            'gabbiaopen', 'la7', 'webitalia360', 'tagadala7', 'rai2', 'tgla7', 'rete4', 'mattino5', 'tg24', 'tg24news', \n",
    "            'tgcom24', 'lariachetira7', 'tg1', 'tg2', 'tg5', 'portaporta', 'news24', 'papamilano2017', 'road2sport',\n",
    "            'dimartedi', 'onsci', 'presadiretta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_errors = {} #word: (frequency, is_error), is_error = 1(yes), 0(no)\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    for word in df['tokens'][i]:\n",
    "        if word not in spelling_errors and word != '<hashtag>' and word != '</hashtag>':\n",
    "            if word not in italian_dict and word not in no_error:\n",
    "                spelling_errors[word] = (inverted_index[word], 1)\n",
    "            elif word not in italian_dict and word in no_error:\n",
    "                spelling_errors[word] = (inverted_index[word], 0)\n",
    "            elif word in italian_dict and word not in no_error:\n",
    "                spelling_errors[word] = (inverted_index[word], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelling_words = {} #{word_error1: frequency1, word_error2: frequency2, ...}\n",
    "\n",
    "for word in spelling_errors.keys():\n",
    "    if spelling_errors[word][1] == 1:\n",
    "        misspelling_words[word] = spelling_errors[word][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1162 misspelled words.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} misspelled words.\".format(len(misspelling_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "freq_error = {} #{freq1: [word1, word2, ...], freq2: [word5], ...}\n",
    "\n",
    "for x in misspelling_words:\n",
    "    if misspelling_words[x] not in freq_error:\n",
    "        freq_error[misspelling_words[x]] = [x]\n",
    "    else:\n",
    "        freq_error[misspelling_words[x]].append(x)\n",
    "        \n",
    "print(sorted(freq_error.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['migrant', 'castelvolturno', 'filcams']\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(freq_error[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(text):\n",
    "    if 'svuotacantine' in text:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1694\n",
      "1792\n",
      "#Lintervista ad Assad La guerra finirà quando elimineremo tutti i terroristi dalla Siria ... URL \n",
      "['<hashtag>', 'lintervista', '</hashtag>', 'assad', 'guerra', 'finira', 'quando', 'elimineremo', 'terroristi', 'siria']\n"
     ]
    }
   ],
   "source": [
    "#Dataset\n",
    "df2 = pd.read_csv(train_val_AB_TSV, sep='\\t')\n",
    "\n",
    "j = None\n",
    "for i in range(0, len(df)):\n",
    "    if find(df.iloc[i]['tokens']):\n",
    "        j = i\n",
    "        print(i)\n",
    "    \n",
    "print(df2.iloc[j]['text '])\n",
    "print(df.iloc[j]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "df2 = pd.read_csv(train_val_AB_TSV, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#ItaliaSicura #ONSCI “Violenza insopportabile Bisogna far conoscere anche l’islam… URL '"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.iloc[i]['text ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<hashtag>',\n",
       " 'italia',\n",
       " 'sicura',\n",
       " '</hashtag>',\n",
       " '<hashtag>',\n",
       " 'onsci',\n",
       " '</hashtag>',\n",
       " 'violenza',\n",
       " 'insopportabile',\n",
       " 'bisogna',\n",
       " 'far',\n",
       " 'conoscere',\n",
       " 'islam']"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[i]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saving DF with hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../../SaRaH/dataset/haspeede2/preprocessed/dev/dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving DF with hashtags and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "df1 = pd.read_csv('../../../SaRaH/dataset/haspeede2/preprocessed/dev/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@user @user infatti finché ci hanno guadagnato con i campi #rom tutto era ok con #Alemanno #Ipocriti '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['infatti', 'finche', 'guadagnato', 'campi', '<hashtag>', 'rom', '</hashtag>', '<hashtag>', 'alemanno', '</hashtag>', '<hashtag>', 'ipocriti', '</hashtag>']\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['tokens'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert the string into an array \"['vado', 'casa']\"-> ['vado', 'casa']\n",
    "def clean_string(tokens):\n",
    "    return ast.literal_eval(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tokens'] = df1['tokens'].apply(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('italian')\n",
    "\n",
    "def stemming(tokens):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word != '<hashtag>' and word != '</hashtag>':\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            result.append(stemmed_word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tokens'] = df1['tokens'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('../../../SaRaH/dataset/haspeede2/preprocessed/dev/dev_stemmed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing hashtag tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_hashtag_tags(tokens):\n",
    "    words = ['<hashtag>', '</hashtag>']\n",
    "    resultwords  = [word for word in tokens if word not in words]\n",
    "    return resultwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "df2 = pd.read_csv('../../../SaRaH/dataset/haspeede2/preprocessed/dev/dev.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['tokens'] = df2['tokens'].apply(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['tokens'] = df2['tokens'].apply(removing_hashtag_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saving DF without hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('../../../SaRaH/dataset/haspeede2/preprocessed/dev/dev_no_hashtag.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving DF without hashtags and with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "df3 = pd.read_csv('../../../SaRaH/dataset/haspeede2/preprocessed/dev/dev_no_hashtag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['tokens'] = df3['tokens'].apply(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['tokens'] = df3['tokens'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('../../../SaRaH/dataset/haspeede2/preprocessed/dev/dev_no_hashtag_stemmed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
