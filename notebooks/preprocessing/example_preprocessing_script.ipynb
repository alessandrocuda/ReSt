{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'it' language\n",
      "Reading english - 1grams ...\n",
      "842\n",
      "3303\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../..')\n",
    "sys.path.append('..')\n",
    "from ReSt.src.data.preprocessing import Preprocessing\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of files to read from\n",
    "train_val_AB_TSV = '../../../ReSt/dataset/haspeede2/raw/haspeede2_dev/haspeede2_dev_taskAB.tsv'\n",
    "\n",
    "test_tweets_AB_TSV = '../../../ReSt/dataset/haspeede2/raw/haspeede2_test/haspeede2_test_taskAB-tweets.tsv'\n",
    "test_news_AB_TSV = '../../../ReSt/dataset/haspeede2/raw/haspeede2_test/haspeede2-test_taskAB-news.tsv'\n",
    "\n",
    "reference_tweets_AB_TSV = '../../../ReSt/dataset/haspeede2/raw/haspeede2_reference/haspeede2_reference_taskAB-tweets.tsv'\n",
    "reference_news_AB_TSV = '../../../ReSt/dataset/haspeede2/raw/haspeede2_reference/haspeede2_reference_taskAB-news.tsv'\n",
    "\n",
    "italian_words = '../../../ReSt/dataset/words/parole_uniche.txt'\n",
    "italian_gzip = '../../../ReSt/dataset/words/italian_words.txt.gz'\n",
    "bad_words = '../../../ReSt/dataset/words/lista_badwords.txt'\n",
    "word_polarity_xml = '../../../ReSt/dataset/it-sentiment_lexicon.lmf.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_dict = [word.rstrip().lower() for word in open(italian_words, 'r', encoding='utf8') if word.rstrip().lower() != '']\n",
    "bad_words_dict = [word.rstrip().lower() for word in open(bad_words, 'r', encoding='utf8') if word.rstrip().lower() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['negative', 'neutral', 'positive', 'nneutral'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word polarity dict\n",
    "\n",
    "# Reading the data inside the xml \n",
    "# file to a variable under the name  \n",
    "# data \n",
    "with open(word_polarity_xml, 'r') as f: \n",
    "    data = f.read() \n",
    "\n",
    "# Passing the stored data inside \n",
    "# the beautifulsoup parser, storing \n",
    "# the returned object  \n",
    "Bs_data = BeautifulSoup(data, \"xml\") \n",
    "\n",
    "word_polarity = {}\n",
    "\n",
    "lemma_unique = Bs_data.find_all('Lemma')         #Finding all instances of tag 'Lemma'\n",
    "sentiment_unique = Bs_data.find_all('Sentiment') \n",
    "\n",
    "if len(lemma_unique) != len(sentiment_unique):\n",
    "    print('ERRORE')\n",
    "\n",
    "for i in range(0, len(lemma_unique)):\n",
    "    word = lemma_unique[i].get('writtenForm') #Extracting the data stored in a specific attributes of the 'Lemma' tag\n",
    "    word = re.sub(r'_', ' ', word)\n",
    "    \n",
    "    polarity = sentiment_unique[i].get('polarity')\n",
    "    if polarity is None:\n",
    "        polarity = 'neutral'\n",
    "    \n",
    "    word_polarity[word] = polarity\n",
    "inv_map = {v: k for k, v in word_polarity.items()}\n",
    "inv_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspettare\n"
     ]
    }
   ],
   "source": [
    "print(inv_map['nneutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['negative', 'neutral', 'positive'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix word polarity typo\n",
    "word_polarity['aspettare'] = 'neutral'\n",
    "inv_map = {v: k for k, v in word_polarity.items()}\n",
    "inv_map.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'it' language\n",
      "Reading english - 1grams ...\n",
      "842\n",
      "3303\n",
      "Number of dev sentences: 6,839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(train_val_AB_TSV, sep='\\t')\n",
    "#df.rename(columns={\"text \": \"text\"}, inplace=True)\n",
    "df = pd.read_csv(train_val_AB_TSV, delimiter=r'\\t', header=None, engine='python')\n",
    "df.columns =['id', 'text', 'hs', 'stereotype'] \n",
    "df = df.drop([0])\n",
    "print('Number of dev sentences: {:,}\\n'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Tweets\n",
    "df_tweets_test = pd.read_csv(test_tweets_AB_TSV, delimiter=r'\\t', header=None, engine='python')\n",
    "\n",
    "#Test News\n",
    "df_news_test = pd.read_csv(test_news_AB_TSV, delimiter=r'\\t', header=None, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                               text  hs  stereotype\n",
      "0     11834  @user A me pare una scelta politica suicida pu...   1           0\n",
      "1     12113  @user e' un perfetto musulmano!!! chi dice il ...   1           1\n",
      "2     11770  Mai Tg e i giornaloni hanno parlato di questa ...   1           1\n",
      "3     11937  @user Ipocriti farabutti. Fanno morire i terre...   1           1\n",
      "4     11870  @user @user @user L'IMMIGRAZIONE C'E' STATO UN...   1           1\n",
      "...     ...                                                ...  ..         ...\n",
      "1258  10091  Il Razzismo gli viene inculcato sin da bambini...   1           1\n",
      "1259  10046  PERCHÉ DI QUESTO ORRENDO STUPRO NON DI PARLA? ...   1           1\n",
      "1260  10377  FERMIAMO L'IMPOSTORE. Forse saranno necesarie ...   1           1\n",
      "1261  10199  @user @user @user @user @user @user @user @use...   1           1\n",
      "1262  10574  Ci vuole \"coraggio\" ad abbinare la parola Egua...   1           1\n",
      "\n",
      "[1263 rows x 4 columns]\n",
      "        id                                               text  hs  stereotype\n",
      "0    11976  \"Andate pure là, tanto quei fessi degli italia...   1           0\n",
      "1    12142  \"Che fine spero che faccia il killer nigeriano...   1           1\n",
      "2    12088  \"Così i profughi ci svuotano i negozi a Porden...   1           1\n",
      "3    12030  \"Così umiliano gli italiani e coccolano i clan...   1           1\n",
      "4    11775  \"Danno soldi ai clandestini, ma ai disabili in...   1           1\n",
      "..     ...                                                ...  ..         ...\n",
      "495  10085  Sea Watch, il pm fa sbarcare i migranti. Salvi...   0           0\n",
      "496  10044  Il pm fa sbarcare i migranti Il capo leghista ...   0           0\n",
      "497  10602  Pisa, il poster di Salvini con i migranti fatt...   0           0\n",
      "498  10193  Sea Watch e lo sbarco del migrante con una sol...   0           0\n",
      "499  10282  Decreto Sicurezza Bis, multe più salate per ch...   0           0\n",
      "\n",
      "[500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Reference Tweets\n",
    "df_tweets_reference = pd.read_csv(reference_tweets_AB_TSV, delimiter=r'\\t', header=None, engine='python')\n",
    "df_tweets_reference.columns =['id', 'text', 'hs', 'stereotype'] \n",
    "print(df_tweets_reference)\n",
    "\n",
    "#Reference News\n",
    "df_news_reference = pd.read_csv(reference_news_AB_TSV, delimiter=r'\\t', header=None, engine='python')\n",
    "df_news_reference.columns =['id', 'text', 'hs', 'stereotype'] \n",
    "print(df_news_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for windows\n",
    "#Reference Tweets\n",
    "#df_tweets_reference = pd.read_csv(reference_tweets_AB_TSV, delimiter=r'\\t', header=None, engine='python')\n",
    "#df_app1 = pd.DataFrame([[11834, \"@user A me pare una scelta politica suicida puntare tutto su una battaglia sicuramente perdente in favore dell'immigrazione incontrollata...Meglio così, spariranno più velocemente!\", 1, 0]], columns=['id', 'text', 'hs', 'stereotype'])\n",
    "#df_tweets_reference.rename(columns={'11834': 'id', \"@user A me pare una scelta politica suicida puntare tutto su una battaglia sicuramente perdente in favore dell'immigrazione incontrollata...Meglio così, spariranno più velocemente!\": 'text', '1':'hs', '0':'stereotype'}, inplace=True)\n",
    "#df_tweets_reference = df_tweets_reference.append(df_app1, ignore_index=True)\n",
    "\n",
    "#Reference News\n",
    "#df_news_reference = pd.read_csv(reference_news_AB_TSV, delimiter=r'\\t', header=None, engine='python')\n",
    "#df_app2 =pd.DataFrame([[11976, \"Andate pure là, tanto quei fessi degli italiani.... Capito perché ci invadono? Il clandestino confessa\", 1, 0]], columns=['id', 'text', 'hs', 'stereotype'])\n",
    "#df_news_reference.rename(columns={'11976': 'id', \"Andate pure là, tanto quei fessi degli italiani.... Capito perché ci invadono? Il clandestino confessa\": 'text', '1':'hs', '0':'stereotype'}, inplace=True)\n",
    "#df_news_reference = df_news_reference.append(df_app2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prp = Preprocessing(italian_words=italian_dict, italian_words_gz=italian_gzip, bad_words=bad_words_dict, word_polarity_dict=word_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'it' language\n",
      "Reading english - 1grams ...\n",
      "842\n",
      "3303\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>7465</td>\n",
       "      <td>@user @user @user @user @user @user @user @use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4805</th>\n",
       "      <td>9006</td>\n",
       "      <td>@user @user @user @user @user @user @user @use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text hs stereotype\n",
       "4804  7465  @user @user @user @user @user @user @user @use...  0          0\n",
       "4805  9006  @user @user @user @user @user @user @user @use...  0          0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.loc[4804:4805].copy()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__normalize_numbers\n",
      " Come ? ? La pacchia non doveva finire per gli immigrati ? E il reddito di cittadinanza ? E quota 100 ? \n",
      " Come ? ? La pacchia non doveva finire per gli immigrati ? E il reddito di cittadinanza ? E quota @Dg@Dg@Dg ? \n",
      "__normalize_numbers\n",
      " E ' un rimpatrio di migranti economici \n",
      " E ' un rimpatrio di migranti economici \n"
     ]
    }
   ],
   "source": [
    "prp.preprocess_df(df=df_test, column_name='text', path='../../../ReSt/dataset/haspeede2/preprocessed/dev/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Come ? ? La pacchia non doveva finire per gli immigrati ? E il reddito di cittadinanza ? E quota @Dg@Dg@Dg ? \n"
     ]
    }
   ],
   "source": [
    "print(df_test[\"text\"][4804])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Reference Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing URLs\n",
      "Removing Tags\n",
      "Feature extraction: length of the comment\n",
      "Translation of emoji\n",
      "Translating emoticons\n",
      "Adding space before hashtag symbol '#'\n",
      "Feature extraction: number of hashtags\n",
      "Replacing hashtags\n",
      "Normalizing hashtags\n",
      "Fixing hashtags\n",
      "Normalizing numbers\n",
      "clean_some_punctuation\n",
      "Adding space between lowercase and uppercase\n",
      "Converting all emoticons written in text\n",
      "Feature extraction: percentage of words written in CAPS-LOCK\n",
      "Normalizing text\n",
      "Feature extraction: esclamations\n",
      "Feature extraction: number of questions mark\n",
      "Uncensoring the bad words\n",
      "Removing hashtag symbol\n",
      "Removing laughs\n",
      "Removing nearby equal vowels\n",
      "Removing nearby equal consonants if they are more than 2\n",
      "Sticking the apostrophe (text)\n",
      "generate Lemma\n",
      "genererate PoS\n",
      "Generate Dep\n",
      "Generate Word polarity\n",
      "Tokenization\n",
      "Sticking the apostrophe (tokens)\n",
      "Generate Stemming\n",
      "Replacement of the abbreviations with the respective words\n",
      "Replacing Acronyms\n",
      "Feature extraction: percentage of Bad Words\n"
     ]
    }
   ],
   "source": [
    "prp.preprocess_df(df=df_tweets_reference, column_name='text', path='../../../ReSt/dataset/haspeede2/preprocessed/reference/reference_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Reference News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing URLs\n",
      "Removing Tags\n",
      "Feature extraction: length of the comment\n",
      "Translation of emoji\n",
      "Translating emoticons\n",
      "Adding space before hashtag symbol '#'\n",
      "Feature extraction: number of hashtags\n",
      "Replacing hashtags\n",
      "Normalizing hashtags\n",
      "Fixing hashtags\n",
      "Normalizing numbers\n",
      "clean_some_punctuation\n",
      "Adding space between lowercase and uppercase\n",
      "Converting all emoticons written in text\n",
      "Feature extraction: percentage of words written in CAPS-LOCK\n",
      "Normalizing text\n",
      "Feature extraction: esclamations\n",
      "Feature extraction: number of questions mark\n",
      "Uncensoring the bad words\n",
      "Removing hashtag symbol\n",
      "Removing laughs\n",
      "Removing nearby equal vowels\n",
      "Removing nearby equal consonants if they are more than 2\n",
      "Sticking the apostrophe (text)\n",
      "generate Lemma\n",
      "genererate PoS\n",
      "Generate Dep\n",
      "Generate Word polarity\n",
      "Tokenization\n",
      "Sticking the apostrophe (tokens)\n",
      "Generate Stemming\n",
      "Replacement of the abbreviations with the respective words\n",
      "Replacing Acronyms\n",
      "Feature extraction: percentage of Bad Words\n"
     ]
    }
   ],
   "source": [
    "prp.preprocess_df(df=df_news_reference, column_name='text', path='../../../ReSt/dataset/haspeede2/preprocessed/reference/reference_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
