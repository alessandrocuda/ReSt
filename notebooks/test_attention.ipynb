{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giulia\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#from tensorflow.keras.engine import Layer, InputSpec, InputLayer\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Embedding, concatenate\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Conv2D, MaxPool2D, ZeroPadding1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Concatenate, Dot, Concatenate, Multiply, RepeatVector\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "root_project = '../../SaRaH/'\n",
    "sys.path.append(root_project)\n",
    "sys.path.append('.')\n",
    "from src.data.utils import load_data, set_unkmark\n",
    "from src.features.word_embedding import get_index_key_association, get_int_seq, build_keras_embedding_matrix, get_data_to_emb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path   = root_project + \"dataset/haspeede2/preprocessed/dev/dev.csv\"\n",
    "w2v_path       = root_project + \"results/model/word2vec/word2vec.wordvectors\"\n",
    "\n",
    "df = pd.read_csv(dataset_path, sep=',')\n",
    "dataset = load_data(df, True)\n",
    "w2v = KeyedVectors.load(w2v_path)\n",
    "set_unkmark(dataset[\"tokens\"], w2v)\n",
    "index_to_key, key_to_index = get_index_key_association(w2v)\n",
    "dataset[\"int_tokens\"] = get_int_seq(dataset[\"tokens\"], index_to_key)  # for embedding layer\n",
    "X = get_data_to_emb(dataset[\"tokens\"], w2v, 40, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5845, 40, 128)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_train2, X_val2, y_train, y_val = train_test_split(X, dataset[\"extra_features\"] , dataset[\"target\"], test_size=0.10, random_state=128)\n",
    "X_train, X_test, X_train2, X_test2, y_train, y_test = train_test_split(X_train, X_train2 , y_train, test_size=0.05, random_state=128)\n",
    "y_train = np.asarray(y_train)\n",
    "y_val = np.asarray(y_val)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_val = np.asarray(X_val)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "X_train2 = np.asarray(X_train2)\n",
    "X_val2 = np.asarray(X_val2)\n",
    "X_test2 = np.asarray(X_test2)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(print_model=False):\n",
    "  \"\"\" HyperParameters \"\"\"\n",
    "  FILTERS = 256\n",
    "  pooling_units = 10\n",
    "  output_dims = 1\n",
    "  hidden_dims= 1\n",
    "\n",
    "  text_seq_input = Input(shape=(40,128,), name=\"text\")\n",
    "  #text_embedding = Embedding(vocab_size, WORD_EMB_SIZE, input_length=MAX_TEXT_LEN,\n",
    "  #                            weights=[embedding_matrix], trainable=False)(text_seq_input)\n",
    "  extra_feature = Input(shape=(5,), name = \"extra\")\n",
    "\n",
    "  #text_embedding = Embedding(vocab_size, WORD_EMB_SIZE, input_length=MAX_TEXT_LEN,\n",
    "  #                            weights=[embedding_matrix], trainable=False)(text_seq_input)\n",
    "  #text_dropout = Dropout(0.25)(text_embedding)\n",
    "\n",
    "  filter_sizes = [2,3,4]\n",
    "  convs = []\n",
    "  for filter_size in filter_sizes:\n",
    "      l_conv = Conv1D(filters=FILTERS, kernel_size=filter_size, activation='relu')(text_seq_input)\n",
    "      POOL_SIZE = l_conv.get_shape()[-2] // pooling_units\n",
    "      l_pool = MaxPool1D(pool_size=POOL_SIZE, strides =3, padding='valid')(l_conv)   #Dynamic pooling\n",
    "      #l_conv = Conv1D(filters=64, kernel_size=filter_size, activation='relu')(l_pool)\n",
    "      #POOL_SIZE = l_conv.get_shape()[-2] // pooling_units\n",
    "      #l_pool = MaxPool1D(pool_size=POOL_SIZE, strides =1, padding='valid')(l_conv)   #Dynamic pooling\n",
    "      convs.append(l_pool)\n",
    "\n",
    "  l_merge = Concatenate(axis=1)(convs)\n",
    "  l_cov1= Conv1D(110, 5, activation='relu')(l_merge)\n",
    "  # since the text is too long we are maxooling over 100\n",
    "  # and not GlobalMaxPool1D\n",
    "  l_pool1 = MaxPool1D(10)(l_cov1)\n",
    "  l_flat = Flatten()(l_pool1)\n",
    "  l_flat = Concatenate(axis=1)([l_flat, extra_feature])\n",
    "  l_hidden = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.007))(l_flat)\n",
    "  l_hidden = Dense(64, activation='relu',  kernel_regularizer=tf.keras.regularizers.l2(0.007))(l_hidden)\n",
    "  l_out = Dense(1, activation='sigmoid')(l_hidden)  #dims output\n",
    "  model_cnn = Model(inputs=[text_seq_input, extra_feature], outputs=l_out)\n",
    "  if print_model:\n",
    "    model_cnn.summary()\n",
    "    tf.keras.utils.plot_model(model_cnn, \"my_first_model.png\", show_shapes=True)\n",
    "  return model_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, 40, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 39, 256)      65792       text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 38, 256)      98560       text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 37, 256)      131328      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 13, 256)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 12, 256)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 12, 256)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 37, 256)      0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 33, 110)      140910      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 3, 110)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 330)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "extra (InputLayer)              [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 335)          0           flatten[0][0]                    \n",
      "                                                                 extra[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          43008       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            65          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 487,919\n",
      "Trainable params: 487,919\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn = build_model(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Attention 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class peel_the_layer(tf.keras.layers.Layer): \n",
    "    def __init__(self, units):    \n",
    "        # Nothing special to be done here\n",
    "        #super(peel_the_layer, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape): #input_shape = (?,19,256) = (batch_size, #time_steps, #features)\n",
    "        # Define the shape of the weights and bias in this layer\n",
    "        \n",
    "        #input_shape[-1] = last index of the input_shape (256) = #weights in the layer\n",
    "        self.inp_dimensions = input_shape[-1]\n",
    "        \n",
    "        #input_shape[-2] = 19 = #time steps\n",
    "        self.seq_length = input_shape[-2]\n",
    "        \n",
    "        # As we discussed the layer has just 1 lonely neuron\n",
    "        #num_units = 1 (self.units)\n",
    "        \n",
    "        #LAYER-WEIGHT \n",
    "        self.w=self.add_weight(shape=(inp_dimensions, self.units), initializer=\"normal\")\n",
    "        \n",
    "        #ATTENTION WEIGHTS\n",
    "        self.b=self.add_weight(shape=(seq_length, self.units), initializer=\"zeros\") \n",
    "        \n",
    "        #super(peel_the_layer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # x is the input tensor of 256 dimensions (19*256)\n",
    "        \n",
    "        # Below is the main processing done during training\n",
    "        # K is the Keras Backend import\n",
    "        #‘w’ is the weight of the layer (256*1), and ‘a’ is the attention weights\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.w)+self.b) #x*w = (19*1)\n",
    "        \n",
    "        #You may need to explicitly ‘squeeze’ the (19 * 1) attention vector that you got above... \n",
    "        #...into a 1-D array of (19) before computing the softmax. -> Flatten()\n",
    "        e = Flatten()(e)\n",
    "        \n",
    "        #'a' are the 19 attention weights\n",
    "        #a = K.softmax(e, axis=1) #Softmax squashes 'e' into values in the range between 0, and 1 whose sum is 1.\n",
    "        a = Activation('softmax')(e)\n",
    "        \n",
    "        #After calculating the softmax, you need to ‘expand’ back the... \n",
    "        #...attention weights from (19) to (19 * 1)  \n",
    "        #Don't manipulate 'a'. It needs to be 'return'ed intact.\n",
    "        temp = RepeatVector(256)(a)   #(?,19) becomes (?,256,19)\n",
    "        temp = Permute([2,1])(temp)   #change from (?,256,19) to (?,19,256)\n",
    "        \n",
    "        #We now need to multiply the first word (by word, I mean all of its 256 dimensions)...\n",
    "        #...with a0, 2nd word with a1 and so on for all 19 words. This is done by the below 3 lines.\n",
    "        #We multiply each attention weight by the respective word...   \n",
    "        output = Multiply()([x,temp])  #Apply weight to each of the 256 dim\n",
    "        \n",
    "        # ...and sum up and we are done!\n",
    "        #The Lambda layer wraps up any arbitrary function and gives it a ‘layer-like’ look.\n",
    "        output = Lambda(lambda values: K.sum(values, axis=1))(output)\n",
    "        \n",
    "        # the second variable is the 'attention adjusted o/p state' ready to be fed to the next layer.\n",
    "        return a, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To invoke this layer between others\n",
    "\n",
    "# lstm_out is o/p of step 3 and is an i/p to step 3.5\n",
    "a, attn_adjusted_op = peel_the_layer()(lstm_out)\n",
    "# attn_adjusted_op is o/p of step 3.5 and is an i/p to step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Attention 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same of Attention 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Flatten, Activation, RepeatVector, Permute, Multiply, Lambda, Dense, merge\n",
    "\n",
    "# Define a regular layer instead of writing a custom layer\n",
    "# This layer should have just one neuron - like before\n",
    "# The weights and bias shapes are automatically calculated \n",
    "# by the Framework, based on the input\n",
    "# This layer is defined at step 3.5 directly\n",
    "e=Dense(1, activation='tanh')(lstm_out) #lstm_out = (?,19,256)\n",
    "\n",
    "# Now do all the softmax business taking the above o/p\n",
    "e=Flatten()(e)\n",
    "a=Activation('softmax')(e)\n",
    "temp=RepeatVector(256)(a)\n",
    "temp=Permute([2, 1])(temp)\n",
    "\n",
    "# multiply weight with lstm layer o/p\n",
    "output = merge.Multiply()([lstm_out, temp])\n",
    "\n",
    "# Get the attention adjusted output state\n",
    "output = Lambda(lambda values: K.sum(values, axis=1))(output)\n",
    "\n",
    "# Pass output to step 4 and 'a' to any nice display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Attention 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With 'context' (is supposed to best summarize the sentiment of the sentence or the tweet in one word).\n",
    "<p>Let the additional feedforward layer determine the value of this context along with the weights and bias during training.\n",
    "<p>(one line in build method to define the context ‘u’, another in call method to do the dot-product of the output with the self.u)\n",
    "<p>Note that the product of 2 vectors gives a measure of their similarity. Softmaxing it returns a set of 19 probabilities adding up to 1. Each probability indicates how close the word is to the context vector. The rest of the processing is the same and we finally end up with the ‘attention adjusted output’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/sermakarevich/hierarchical-attention-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Attention 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Yang et al. proposed the hierarchical model here (https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf), which uses two levels of attention — one at the word level and one at the sentence level.\n",
    "\n",
    "<p>The attention mechanism here can also function as a pure ‘reduction’ operation, which could be used in place of any pooling step. This is because the ‘context’ that is derived, is 1 word and it best summarises the sentiment of the 19-word tweet — a classic ‘reduction’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
