{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from collections import Counter\n",
    "import operator\n",
    "import collections\n",
    "\n",
    "\n",
    "#import fasttext.util\n",
    "\n",
    "#utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "#src\n",
    "#root_project = \"/content/ReSt/\"\n",
    "root_project = \"/Users/Alessandro/Dev/repos/ReSt/\"\n",
    "#root_project = \"/home/jupyter/ReSt/\"\n",
    "\n",
    "sys.path.append(root_project)\n",
    "from src.data.utils import load_csv_to_dict, dtype, dtype_transformation, set_unkmark_token\n",
    "from src.data.word_embedding import get_index_key_association, get_int_seq, build_keras_embedding_matrix, get_data_to_emb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path\n",
    "#dataset_path    = \"/Users/Alessandro/Dev/repos/ReSt/dataset/haspeede2/preprocessed/reference/reference_tweets.csv\"\n",
    "dataset_path    = root_project + 'dataset/haspeede2/preprocessed/dev/dev.csv'\n",
    "w2v_bin_path    = root_project + 'results/model/word2vec/twitter128.bin'\n",
    "#w2v_bin_path    = root_project + 'results/model/word2vec/tweets_2019_Word2Vect.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data 'lemma' String in list of String\n",
      "Converting data 'pos' String in list of String\n",
      "Converting data 'dep' String in list of String\n",
      "Converting data 'word_polarity' String in list of String\n",
      "Converting data 'tokens' String in list of String\n",
      "Converting data 'stem' String in list of String\n"
     ]
    }
   ],
   "source": [
    "dataset = load_csv_to_dict(dataset_path)\n",
    "senteces = dataset[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Italiani',\n",
       " '!',\n",
       " '!',\n",
       " 'via',\n",
       " 'da',\n",
       " \"l'\",\n",
       " 'Italia',\n",
       " 'questo',\n",
       " 'territorio',\n",
       " 'ormai',\n",
       " '√®',\n",
       " 'di',\n",
       " 'i',\n",
       " 'profughi',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senteces[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                   <class 'int'>\n",
      "text                 <class 'str'>\n",
      "hs                   <class 'int'>\n",
      "stereotype           <class 'int'>\n",
      "processed_text       <class 'str'>\n",
      "text_length          <class 'int'>\n",
      "hashtags             <class 'int'>\n",
      "%CAPS-LOCK words     <class 'int'>\n",
      "esclamations         <class 'int'>\n",
      "questions            <class 'int'>\n",
      "tokens               <class 'list'>\n",
      "lemma                <class 'list'>\n",
      "pos                  <class 'list'>\n",
      "dep                  <class 'list'>\n",
      "word_polarity        <class 'list'>\n",
      "sentence_positive    <class 'float'>\n",
      "sentence_negative    <class 'float'>\n",
      "sentence_neutral     <class 'float'>\n",
      "stem                 <class 'list'>\n",
      "%bad_words           <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "dtype(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples sentence: +++#Siriaüá∏üáæ Evacuati civili dalla citt√† terroristi #IS avanzano dopo violenti scontri con Esercito #siriano a #Palmira ma non √® finita+++ \n",
      "To tokens: ['+', '+', '<', 'Siria', '>', 'bandiera', 'siria', 'Evacuati', 'civili', 'da', 'la', 'citt√†', 'terroristi', '<', 'Is', '>', 'avanzano', 'dopo', 'violenti', 'scontri', 'con', 'Esercito', '<', 'siriano', '>', 'a', '<', 'Palmira', '>', 'ma', 'non', '√®', 'finita', '+', '+']\n"
     ]
    }
   ],
   "source": [
    "sentece_i = 53\n",
    "print(\"Examples sentence: {}\".format(dataset[\"text\"][sentece_i]))\n",
    "print(\"To tokens: {}\".format(dataset[\"tokens\"][sentece_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - #sentences: 6839\n",
      " - Unique word on the datset: 20594\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "n_sentences = len(senteces)\n",
    "unique_words = set([word for words in senteces for word in words])\n",
    "unique_words_freq = dict(Counter(i for sub in senteces for i in set(sub)))\n",
    "n_unique_words = len(unique_words)\n",
    "\n",
    "#print data\n",
    "print(\" - #sentences: {}\".format(n_sentences))\n",
    "print(\" - Unique word on the datset: {}\".format(n_unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_setences = dataset[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model = Word2Vec.load(w2v_bin_path)\n",
    "wv = KeyedVectors.load_word2vec_format(datapath(w2v_bin_path), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.5743259e-01, -1.0227908e+00,  8.0411959e-01, -1.8877728e+00,\n",
       "       -1.1589552e+00, -2.5145683e-01, -7.0758271e-01, -9.4391364e-01,\n",
       "       -4.7587892e-01, -2.1063483e+00,  1.3505214e-01, -7.4882346e-01,\n",
       "        1.1972415e+00, -4.1316342e-02,  1.3334475e-01, -1.0462364e+00,\n",
       "        4.5099264e-01,  2.5281479e+00,  4.4037056e-01,  1.6277286e+00,\n",
       "        2.7041452e+00, -1.5561391e+00, -9.4343483e-01, -2.3908161e-01,\n",
       "        9.4067627e-01,  5.3680116e-01, -2.2288663e+00, -5.9790820e-01,\n",
       "       -3.4088647e-01, -6.6892159e-01, -1.4420565e+00, -1.6446360e+00,\n",
       "       -2.1486742e+00,  4.2901948e-01, -6.0259920e-01, -2.3585441e+00,\n",
       "        8.4915322e-01,  2.4764334e-01,  2.6220727e-01,  7.2281873e-01,\n",
       "        2.0849390e+00,  4.0762094e-01, -1.5244249e+00,  2.2978106e+00,\n",
       "       -1.9623476e-01, -8.4033400e-01, -1.5208732e+00, -1.0290638e+00,\n",
       "        2.3683529e+00,  1.1123921e-01,  3.0179641e-01, -1.2741308e-01,\n",
       "        2.6779404e+00, -4.0727234e-01, -1.4980392e+00,  2.2269785e+00,\n",
       "       -1.6030633e+00, -1.2264123e+00,  3.0762956e+00, -3.0512181e-01,\n",
       "        3.6540759e-01, -1.5799649e+00,  4.2730847e-01, -9.6544802e-02,\n",
       "        7.9311639e-01, -1.8734335e-03,  1.6710782e-01,  1.9801726e+00,\n",
       "       -3.5047150e-01, -1.0707135e+00,  1.4273087e+00,  1.9472381e-01,\n",
       "       -2.9020753e+00, -1.1358604e+00,  7.5679868e-01, -1.3664665e+00,\n",
       "       -3.8635805e-01, -8.8556272e-01,  7.2334898e-01,  4.0498538e+00,\n",
       "        1.2464230e+00, -3.5739067e-01,  9.3289417e-01,  3.5284731e+00,\n",
       "        3.1419727e-01, -5.8174437e-01,  1.3629420e+00,  1.3193891e+00,\n",
       "       -4.7327155e-01,  1.2631086e+00, -1.2198174e+00,  2.7985694e+00,\n",
       "        5.4129934e-01,  3.4408149e-01, -1.4111370e+00,  2.5871656e+00,\n",
       "        1.9635196e-01, -2.0182128e+00, -2.2201358e-01, -4.3922582e-01,\n",
       "       -1.3246504e+00,  1.6948735e+00, -6.2564695e-01, -1.7491929e-01,\n",
       "       -6.4797807e-01,  1.7266259e+00, -6.8133980e-01,  3.5254198e-01,\n",
       "        6.0369883e-02,  1.9622555e-01, -1.6597621e+00, -1.3250060e+00,\n",
       "        1.1746325e+00,  2.0952647e+00,  8.6454976e-01, -5.8001614e-01,\n",
       "        1.2303641e+00,  2.5228450e+00, -2.8666463e+00,  2.5794270e+00,\n",
       "       -2.4237154e+00, -2.0227592e+00,  1.0253716e+00, -7.3918045e-01,\n",
       "       -2.4555669e+00, -2.1167028e+00,  1.8539254e-01, -1.7874559e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv[\"africani\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1170776"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(w2v_model.wv.vocab.keys())\n",
    "len(wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1170776, 128)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know words: 19483\n",
      "unknow words: 1111\n"
     ]
    }
   ],
   "source": [
    "know_words   = []\n",
    "unknow_words = []\n",
    "for word in unique_words:\n",
    "    if word in wv.vocab.keys():\n",
    "        know_words.append(word)\n",
    "    else:\n",
    "        unknow_words.append(word)\n",
    "print(\"know words: {}\".format(len(know_words)))\n",
    "print(\"unknow words: {}\".format(len(unknow_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"po'\", 29),\n",
       " ('integrar', 14),\n",
       " ('webitalia', 7),\n",
       " ('chiamarli', 7),\n",
       " ('sottometter', 6),\n",
       " ('prostituirsi', 5),\n",
       " ('accoglierne', 4),\n",
       " ('centrisociale', 4),\n",
       " ('derub√≤', 4),\n",
       " ('facendoli', 3),\n",
       " ('Merdo', 3),\n",
       " ('ladruncole', 3),\n",
       " ('usarli', 3),\n",
       " ('sostituirli', 3),\n",
       " ('derubarono', 3),\n",
       " ('educarli', 3),\n",
       " ('comportar', 3),\n",
       " ('araboafricani', 3),\n",
       " ('darli', 3),\n",
       " ('rapinar', 3),\n",
       " ('smaltiva', 3),\n",
       " ('Randieri', 3),\n",
       " ('cartabianca', 3),\n",
       " ('costituirsi', 3),\n",
       " ('Afroislamica', 2),\n",
       " ('Dioti', 2),\n",
       " ('Megalizzi', 2),\n",
       " ('vaccinale', 2),\n",
       " ('Tornerebbero', 2),\n",
       " ('fregandosene', 2),\n",
       " ('accoglioni', 2),\n",
       " ('sgozzar', 2),\n",
       " ('impuni', 2),\n",
       " ('Metapolitica', 2),\n",
       " ('farne', 2),\n",
       " ('scaricarli', 2),\n",
       " ('fottiti', 2),\n",
       " ('mert', 2),\n",
       " ('cercarti', 2),\n",
       " ('dimarted', 2),\n",
       " ('Tracing', 2),\n",
       " ('Mantener', 2),\n",
       " ('ricattar', 2),\n",
       " ('lasciandoci', 2),\n",
       " ('diciotti', 2),\n",
       " ('Integrar', 2),\n",
       " ('Liberar', 2),\n",
       " ('microaree', 2),\n",
       " ('identificarli', 2),\n",
       " ('riconoscersi', 2),\n",
       " ('adeguar', 2),\n",
       " ('mandandoli', 2),\n",
       " ('Impugnamo', 2),\n",
       " ('Iuventa', 2),\n",
       " ('ammetterlo', 2),\n",
       " ('salvarli', 2),\n",
       " ('Masod', 2),\n",
       " ('criticarli', 2),\n",
       " ('mercia', 2),\n",
       " ('Rugantina', 2),\n",
       " ('Coranocomunque', 1),\n",
       " ('vaxxngul', 1),\n",
       " ('attrvit√†', 1),\n",
       " ('adibir', 1),\n",
       " ('preconciliari', 1),\n",
       " ('Massou', 1),\n",
       " ('Tesfahun', 1),\n",
       " ('Perathoner', 1),\n",
       " ('istigarli', 1),\n",
       " ('dirottar', 1),\n",
       " ('Attracchi', 1),\n",
       " ('Rateo', 1),\n",
       " ('infogneremo', 1),\n",
       " ('sofranista', 1),\n",
       " ('Chibunna', 1),\n",
       " ('antidemocrati', 1),\n",
       " ('mezziladri', 1),\n",
       " ('dryfusiani', 1),\n",
       " ('portiamone', 1),\n",
       " ('trovarli', 1),\n",
       " ('amministravate', 1),\n",
       " ('Sispara', 1),\n",
       " ('fugito', 1),\n",
       " ('quens', 1),\n",
       " ('spettacolume', 1),\n",
       " ('cimbro', 1),\n",
       " ('nazium', 1),\n",
       " ('geaxue', 1),\n",
       " ('Slarp', 1),\n",
       " ('copong', 1),\n",
       " ('Betorinon', 1),\n",
       " ('incint√†', 1),\n",
       " ('C–ærr—ñ–µr–µ', 1),\n",
       " ('Olbes', 1),\n",
       " ('sbrigar', 1),\n",
       " ('affannar', 1),\n",
       " ('etnicamen', 1),\n",
       " ('Dissacrando', 1),\n",
       " ('revanchista', 1),\n",
       " ('trattiamoli', 1),\n",
       " ('marzagrato', 1),\n",
       " ('principalmete', 1),\n",
       " ('Prendersela', 1),\n",
       " ('volonterosa', 1),\n",
       " ('Aguzzino', 1),\n",
       " ('uccidili', 1),\n",
       " ('Nabellezza', 1),\n",
       " ('Alil', 1),\n",
       " ('evangelizzati', 1),\n",
       " ('Rimpatriar', 1),\n",
       " ('dedicarti', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknow_words_freq = {word: unique_words_freq[word] for word in unknow_words}                     \n",
    "unknow_words_freq_sorted = sorted(unknow_words_freq.items(),key=operator.itemgetter(1),reverse=True)\n",
    "unknow_words_freq_sorted[:111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build keras embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_key_association(wv):\n",
    "    key_to_index = {\"<UNK>\": 0}\n",
    "    index_to_key = {0: \"<UNK>\"}\n",
    "    for idx, word in enumerate(sorted(wv.vocab)):\n",
    "        key_to_index[word]  = idx+1 # which row in `weights` corresponds to which word?\n",
    "        index_to_key[idx+1] = word # which row in `weights` corresponds to which word?\n",
    "    return index_to_key, key_to_index\n",
    "\n",
    "def build_keras_embedding_matrix(wv, index_to_key=None):\n",
    "    print('Vocab_size is {}'.format(len(wv.vocab)))\n",
    "    vec_size = wv.vector_size\n",
    "    vocab_size = len(wv.vocab) + 1 # plus the unknown word\n",
    "    \n",
    "    if index_to_key is None:\n",
    "        index_to_key, _ = get_index_key_association(wv)\n",
    "    # Create the embedding matrix where words are indexed alphabetically\n",
    "    embedding_matrix = np.zeros(shape=(vocab_size, vec_size))\n",
    "    for idx in index_to_key: \n",
    "        #jump the first, words not found in embedding int 0 and will be all-zeros\n",
    "        if idx != 0:\n",
    "            embedding_matrix[idx] = wv.get_vector(index_to_key[idx])\n",
    "\n",
    "    print('Embedding_matrix with unk word loaded')\n",
    "    print('Shape {}'.format(embedding_matrix.shape))\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size is 1170776\n",
      "Embedding_matrix loaded\n",
      "Shape (1170777, 128)\n"
     ]
    }
   ],
   "source": [
    "index_to_key, key_to_index = get_index_key_association(wv)\n",
    "embedding_matrix, vocab_size = build_keras_embedding_matrix(wv, index_to_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
