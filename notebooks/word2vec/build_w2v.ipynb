{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "#import fasttext.util\n",
    "\n",
    "#utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "#src\n",
    "#root_project = \"/content/SaRaH/\"\n",
    "root_project = \"/Users/Alessandro/Dev/repos/SaRaH/\"\n",
    "#root_project = \"/home/jupyter/SaRaH/\"\n",
    "sys.path.append(root_project)\n",
    "from src.data.utils import load_csv_to_dict, dtype, dtype_transformation, set_unkmark_token\n",
    "from src.features.word_embedding import get_index_key_association, get_int_seq, build_keras_embedding_matrix, get_data_to_emb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path\n",
    "dataset_path    = root_project+'dataset/haspeede2/preprocessed/dev/dev.csv'\n",
    "w2v_bin_path    = root_project+'results/model/word2vec/twitter128.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data 'lemma' String in list of String\n",
      "Converting data 'pos' String in list of String\n",
      "Converting data 'dep' String in list of String\n",
      "Converting data 'word_polarity' String in list of String\n",
      "Converting data 'tokens' String in list of String\n",
      "Converting data 'stem' String in list of String\n",
      "id                   <class 'int'>\n",
      "text                 <class 'str'>\n",
      "hs                   <class 'int'>\n",
      "stereotype           <class 'int'>\n",
      "text_length          <class 'int'>\n",
      "hashtags             <class 'int'>\n",
      "%CAPS-LOCK words     <class 'int'>\n",
      "esclamations         <class 'int'>\n",
      "questions            <class 'int'>\n",
      "lemma                <class 'list'>\n",
      "pos                  <class 'list'>\n",
      "dep                  <class 'list'>\n",
      "word_polarity        <class 'list'>\n",
      "tokens               <class 'list'>\n",
      "stem                 <class 'list'>\n",
      "%bad_words           <class 'int'>\n",
      "400\n",
      "['+', '+', '+', '<', 'Siria', '>', 'üá∏', 'üáæ', 'Evacuati', 'civili', 'dalla', 'citt√†', 'terroristi', '<', 'Is', '>', 'avanzano', 'dopo', 'violenti', 'scontri', 'con', 'Esercito', '<', 'siriano', '>', 'a', '<', 'Palmira', '>', 'ma', 'non', '√®', 'finita', '+', '+', '+']\n",
      "['Italiani', '!', '!', '!', '!', '!', '!', 'via', \"dall'\", 'Italia', 'questo', 'territorio', 'ormai', '√®', 'dei', 'profughi', '.']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_csv_to_dict(dataset_path)\n",
    "dtype(dataset)\n",
    "print(len(dataset[\"tokens\"]))\n",
    "print(dataset[\"tokens\"][53])\n",
    "print(dataset[\"tokens\"][29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî\n",
      "‚úî\n"
     ]
    }
   ],
   "source": [
    "print(\"\\u2714\")\n",
    "print(\"\\U00002714\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI_PATTERN = re.compile(\n",
    "    \"([\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"])\"\n",
    ")\n",
    "    \n",
    "def add_space_between_emojies(text):\n",
    "    text = re.sub(EMOJI_PATTERN, r' \\1 ', text)\n",
    "    return text\n",
    "\n",
    "def findall_emojies(text):\n",
    "  # Ref: https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1#gistcomment-3208085\n",
    "  # Ref: https://en.wikipedia.org/wiki/Unicode_block\n",
    "    text = re.findall(EMOJI_PATTERN, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚úî', '‚ù§']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'\\\\u2714': '\\\\U00002714', '\\\\u2764': '\\\\U00002764'}"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"+++#Siria Evacuati civili ‚úî‚ù§\"\n",
    "text = add_space_between_emojies(text)\n",
    "decode_text = text.encode('unicode-escape').decode(\"latin_1\")\n",
    "\n",
    "all_extra_emojii = findall_emojies(text)\n",
    "\n",
    "print(all_extra_emojii)\n",
    "extra_emoji = []\n",
    "for elem in all_extra_emojii:\n",
    "    extra_emoji.append(elem.encode('unicode-escape').decode(\"latin_1\"))\n",
    "extra_emoji = {elem : \"\\\\U0000\"+elem[2:] for elem in extra_emoji}\n",
    "extra_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++#Siria Evacuati civili \\u2714 \\u2764\n"
     ]
    }
   ],
   "source": [
    "sentence = ' '.join([emoji.UNICODE_EMOJI_ALIAS_ENGLISH[extra_emoji[word]] if word in extra_emoji and extra_emoji[word] in emoji.UNICODE_EMOJI_ALIAS_ENGLISH else word for word in decode_text.split()])\n",
    "print(sentence)\n",
    "new_sentence_encoding = (sentence.encode(\"latin_1\")\n",
    "   .decode(\"raw_unicode_escape\")\n",
    "   .encode('utf-16', 'surrogatepass')\n",
    "   .decode('utf-16')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842\n",
      "3303\n"
     ]
    }
   ],
   "source": [
    "missing_emoji = {}\n",
    "for key in emoji.UNICODE_EMOJI_ENGLISH:\n",
    "    if key not in emoji.UNICODE_EMOJI_ITALIAN:\n",
    "        missing_emoji[key.encode('unicode-escape').decode(\"latin_1\")] = key\n",
    "print(len(missing_emoji))\n",
    "print(len(emoji.UNICODE_EMOJI_ITALIAN))\n",
    "decoded_unicode_ita = {k.encode('unicode-escape').decode(\"latin_1\"): emoji.UNICODE_EMOJI_ITALIAN[k] for k in emoji.UNICODE_EMOJI_ITALIAN}\n",
    "\n",
    "missing_emoji_tralation = {}\n",
    "\n",
    "for elem in missing_emoji:\n",
    "    for k in decoded_unicode_ita:\n",
    "        if elem in k:\n",
    "            missing_emoji_tralation[missing_emoji[elem]] = decoded_unicode_ita[k].replace(':','').replace('_', ' ')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_emoji_tralation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++#Siriaüá∏üáæ Evacuati civili ‚úî‚ù§ üëéüëé\n",
      "+++#Siria bandiera siria Evacuati civili segno di spunta nero cuore rosso pollice verso pollice verso\n"
     ]
    }
   ],
   "source": [
    "text = \"+++#Siriaüá∏üáæ Evacuati civili ‚úî‚ù§ üëéüëé\"\n",
    "print(text)\n",
    "text_result = emoji.demojize(text, language='it', delimiters=(\" \", \" \"))\n",
    "text_result = add_space_between_emojies(text_result)\n",
    "text_result = text_result.split()\n",
    "text_result = [elem.replace(':','').replace('_', ' ') if \":\"+elem+\":\" in emoji.EMOJI_UNICODE_ITALIAN else elem for elem in text_result]\n",
    "text_result = [ missing_emoji_tralation[word] if word in missing_emoji_tralation else word for word in text_result]\n",
    "' '.join(text_result)\n",
    "print(' '.join(text_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### examples data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"la violenza simbolica pu√≤ fare molti pi√π danni di qualche auto bruciata . . . . i profughi , i rom non hanno l' assicurazione . . .\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ['la', 'violenza', 'simbolica', 'pu√≤', 'fare', 'molti', 'pi√π', 'danni', 'di', 'qualche', 'auto', 'bruciata', '.', '.', '.', '.', 'i', 'profughi', ',', 'i', 'rom', 'non', 'hanno', \"l'\", 'assicurazione', '.', '.', '.']\n",
    "' '.join(map(str, s)).split()\n",
    "' '.join([str(elem) for elem in s]) \n",
    "#print(ast.literal_eval(s))\n",
    "#' '.join(ast.literal_eval(s)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'neutral', None, 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'l', \"'\", 'uomo', \"'\", 'ciao']\n",
      "[\"'\", \"l'\", \"uomo'\", 'ciao']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"'\",\"l\", \"'\", \"uomo\",\"'\",\"ciao\"]\n",
    "print(tokens)\n",
    "apostrophes = [\"'\", \"‚Äô\"]\n",
    "\n",
    "result_tokens = []\n",
    "pos = 0\n",
    "while pos < len(tokens):\n",
    "    if pos !=(len(tokens)-1) and tokens[pos+1] in apostrophes:\n",
    "        result_tokens.append(tokens[pos] + \"\\'\")\n",
    "        pos+=2\n",
    "    else:\n",
    "        result_tokens.append(tokens[pos])\n",
    "        pos+=1\n",
    "print(result_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prova\n",
      "0  [l']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "',prova\\n0,\"[\"\"l\\'\"\"]\"\\n'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tokens = [\"l'\"]\n",
    "dc = {\"prova\":[result_tokens]}\n",
    "df = pd.DataFrame(dc)\n",
    "print(df)\n",
    "df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[', l', uomo', 'ciao']\n"
     ]
    }
   ],
   "source": [
    "print(\"[\"\"\\'\"\", \"\"l\\'\"\", \"\"uomo\\'\"\", \\'ciao\\']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"[', l', uomo', 'ciao']\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"[\"\"\\'\"\", \"\"l\\'\"\", \"\"uomo\\'\"\", \\'ciao\\']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
