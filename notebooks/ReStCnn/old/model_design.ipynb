{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcx-h8sODpMJ",
    "outputId": "bceea2fc-bbe6-4365-b3b0-70c19847dc48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SaRaH'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
      "remote: Total 419 (delta 4), reused 23 (delta 2), pack-reused 394\u001b[K\n",
      "Receiving objects: 100% (419/419), 143.21 MiB | 30.10 MiB/s, done.\n",
      "Resolving deltas: 100% (206/206), done.\n",
      "Checking out files: 100% (60/60), done.\n",
      "--2021-02-11 17:41:11--  http://www.italianlp.it/twitter128.bin\n",
      "Resolving www.italianlp.it (www.italianlp.it)... 146.48.92.46, 2a00:1620:c0:5c::8\n",
      "Connecting to www.italianlp.it (www.italianlp.it)|146.48.92.46|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 613808129 (585M) [application/octet-stream]\n",
      "Saving to: ‘twitter128.bin’\n",
      "\n",
      "twitter128.bin      100%[===================>] 585.37M  27.4MB/s    in 25s     \n",
      "\n",
      "2021-02-11 17:41:37 (23.3 MB/s) - ‘twitter128.bin’ saved [613808129/613808129]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install fasttext\n",
    "!rm -rf ReST/\n",
    "!git clone https://github.com/alessandrocuda/SaRaH\n",
    "!wget http://www.italianlp.it/twitter128.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foTfySiPDfOl",
    "outputId": "9fe565dc-0e2e-43ed-aa05-df5cdef2c655"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#from tensorflow.keras.engine import Layer, InputSpec, InputLayer\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Embedding, concatenate\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Conv2D, MaxPool2D, ZeroPadding1D, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Concatenate, Dot, Concatenate, Multiply, RepeatVector\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Lambda, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#import fasttext.util\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import sys\n",
    "#root_project = \"/content/ReSt/\"\n",
    "root_project = \"/Users/Alessandro/Dev/repos/ReSt/\"\n",
    "#root_project = \"/home/jupyter/SaRaH/\"\n",
    "sys.path.append(root_project)\n",
    "from src.data.utils import load_csv_to_dict, set_unkmark_token\n",
    "from src.data.word_embedding import get_index_key_association, get_int_seq, build_keras_embedding_matrix, get_data_to_emb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vVGZwnHDfOo",
    "outputId": "2565e405-89d0-481a-bb4b-a0edc7543ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH\n",
    "dataset_dev_path           = root_project + \"dataset/haspeede2/preprocessed/dev/dev.csv\"\n",
    "dataset_test_tweets_path   = root_project + \"dataset/haspeede2/preprocessed/reference/reference_tweets.csv\"\n",
    "#w2v_path                   = \"/content/twitter128.bin\"\n",
    "w2v_bin_path               = root_project + 'results/model/word2vec/twitter128.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size is 1170776\n",
      "Embedding_matrix with unk word loaded\n",
      "Shape (1170777, 128)\n"
     ]
    }
   ],
   "source": [
    "#load word2vec and embedding_matrix\n",
    "w2v = KeyedVectors.load_word2vec_format(datapath(w2v_bin_path), binary=True)\n",
    "index_to_key, key_to_index = get_index_key_association(w2v)\n",
    "embedding_matrix, vocab_size = build_keras_embedding_matrix(w2v, index_to_key)\n",
    "\n",
    "WORD_EMB_SIZE = 128\n",
    "VOCAB_SIZE = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>\n",
      "\"faccio\n",
      "0\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test embedding\n",
    "print(index_to_key[0])\n",
    "print(index_to_key[100])\n",
    "print(key_to_index[index_to_key[0]])\n",
    "print(key_to_index[index_to_key[100]])\n",
    "emb1 = w2v[index_to_key[100]]\n",
    "\n",
    "inp = Input((2,))\n",
    "out = Embedding(vocab_size, WORD_EMB_SIZE, input_length=2, weights=[embedding_matrix], trainable=False)(inp)\n",
    "model = Model(inp, out)\n",
    "emb2 = model.predict(np.array([0,100]).reshape((1, 2)))\n",
    "np.array_equal(emb1, emb2[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I4VbMeODfOp"
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset dictionary\n",
    "dataset_dev = load_csv_to_dict(dataset_dev_path)\n",
    "dataset_test_tweets = load_csv_to_dict(dataset_test_tweets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dict, w2v, key_to_index, max_text_len):\n",
    "    #TODO: deve ritornare anche tutto il resto, extra, lemma, stem, ...\n",
    "    senteces = dataset_dict[\"tokens\"]\n",
    "    X = dataset_dict[\"tokens\"]\n",
    "    X = set_unkmark_token(X, w2v)\n",
    "    X = get_int_seq(X, key_to_index)\n",
    "    X = pad_sequences(X, maxlen=MAX_TEXT_LEN, padding='post', truncating='post')\n",
    "    y = np.array(dataset_dict[\"stereotype\"])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dev/test\n",
    "MAX_TEXT_LEN = 65\n",
    "\n",
    "X, y = load_data(dataset_dev, w2v, key_to_index, MAX_TEXT_LEN)\n",
    "X_test, y_test = load_data(dataset_test_tweets, w2v, key_to_index, MAX_TEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUXOihCQbH-7",
    "outputId": "bf776179-976b-4db6-a591-41f2798aca20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check:\n",
      "['È', 'terrorismo', 'anche', 'questo', ',', 'per', 'mettere', 'in', 'uno', 'stato', 'di', 'soggezione', 'le', 'persone', 'e', 'render', 'le', 'innocue', ',', 'mentre', 'qualcuno', '.', '.']\n",
      "[1164233 1128251  819363 1052617  186735 1028966  994467  953216 1142852\n",
      " 1113630  892534 1102765  975681 1030781  903822 1059772  975681  959793\n",
      "  186735  992860 1051691  188041  188041       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0]\n",
      "È terrorismo anche questo , per mettere in uno stato di soggezione le persone e render le innocue , mentre qualcuno . . <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> "
     ]
    }
   ],
   "source": [
    "print(\"Check:\")\n",
    "print(dataset_dev[\"tokens\"][0])\n",
    "print(X[0])\n",
    "for index in X[0]:\n",
    "    print(\"{} \".format(index_to_key[index]) ,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZLD5i2095pp"
   },
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u3Y3BXjJDfOu"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5SRBV3K99Mh"
   },
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "cQbbgMnVDfOu"
   },
   "outputs": [],
   "source": [
    "class FCallback(tf.keras.callbacks.Callback):\n",
    "  \n",
    "    def __init__(self, validation = (), verbose = 0):\n",
    "        self.validation = validation\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f1 = []\n",
    "        self.val_f1 = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_t =  self.validation[1]\n",
    "        y_p =  np.where(self.model.predict(self.validation[0])[0] > 0.5, 1, 0)\n",
    "        logs['val_f1'] =  f1_score(y_t, y_p, average='macro')\n",
    "        if self.verbose >0:\n",
    "          print(\"— val_f1: {}\".format(logs['val_f1']))\n",
    "\n",
    "class ReturnBestEarlyStopping(EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KIM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kim_cnn(print_model=False):\n",
    "    \"\"\" HyperParameters \"\"\"\n",
    "    FILTERS = 128\n",
    "    pooling_units = 3\n",
    "    output_dims = 1\n",
    "    hidden_dims= 1\n",
    "\n",
    "    text_seq_input = Input(shape=(MAX_TEXT_LEN,), name=\"text\")\n",
    "    text_embedding = Embedding(VOCAB_SIZE, WORD_EMB_SIZE, input_length=MAX_TEXT_LEN,\n",
    "                               weights=[embedding_matrix], trainable=False)(text_seq_input)\n",
    "\n",
    "    filter_sizes = [2,3,4]\n",
    "    convs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=FILTERS, kernel_size=filter_size)(text_embedding)\n",
    "        l_relu = Activation(\"relu\")(l_conv)\n",
    "        l_pool = GlobalMaxPool1D()(l_relu)   \n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "    # since the text is too long we are maxooling over 100\n",
    "    # and not GlobalMaxPool1D\n",
    "    l_flat = Flatten()(l_merge)\n",
    "\n",
    "    l_hidden = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002))(l_flat )\n",
    "    l_hidden = Dense(64, activation='relu',  kernel_regularizer=tf.keras.regularizers.l2(0.002))(l_hidden)\n",
    "    l_out_st = Dense(1, activation='sigmoid', name=\"st\")(l_hidden)  #dims output\n",
    "\n",
    "    model_cnn = Model(inputs=text_seq_input, outputs=l_out_st)\n",
    "    if print_model:\n",
    "        model_cnn.summary()\n",
    "        tf.keras.utils.plot_model(model_cnn, \"my_first_model.png\", show_shapes=True)\n",
    "    return model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, 65)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 65, 128)      149859456   text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 64, 128)      32896       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 63, 128)      49280       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 62, 128)      65664       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 128)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 63, 128)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 62, 128)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 384)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          49280       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           8256        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "st (Dense)                      (None, 1)            65          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 150,064,897\n",
      "Trainable params: 205,441\n",
      "Non-trainable params: 149,859,456\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kim_cnn(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "46/46 [==============================] - 9s 170ms/step - loss: 1.8691 - val_loss: 1.2196\n",
      "— val_f1: 0.5336517988151319\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 6s 130ms/step - loss: 1.2111 - val_loss: 1.1865\n",
      "— val_f1: 0.4505053405147544\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 6s 128ms/step - loss: 1.1782 - val_loss: 1.2020\n",
      "— val_f1: 0.5087548615811754\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 6s 132ms/step - loss: 1.1627 - val_loss: 1.2026\n",
      "— val_f1: 0.4345861346853301\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 6s 135ms/step - loss: 1.1368 - val_loss: 1.1182\n",
      "— val_f1: 0.658462468202927\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 1.1268 - val_loss: 1.1162\n",
      "— val_f1: 0.5417538245018749\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 7s 162ms/step - loss: 1.0976 - val_loss: 1.2862\n",
      "— val_f1: 0.33615734385724094\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 7s 151ms/step - loss: 1.0896 - val_loss: 1.1910\n",
      "— val_f1: 0.4183614236977443\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 7s 160ms/step - loss: 1.0492 - val_loss: 1.0893\n",
      "— val_f1: 0.6048149361283983\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 7s 157ms/step - loss: 1.0369 - val_loss: 1.1018\n",
      "— val_f1: 0.541827322497497\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 1.0045 - val_loss: 1.0447\n",
      "— val_f1: 0.677657058642081\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 7s 144ms/step - loss: 0.9854 - val_loss: 1.0095\n",
      "— val_f1: 0.6889948869290691\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 6s 135ms/step - loss: 0.9616 - val_loss: 1.0926\n",
      "— val_f1: 0.6403237004746242\n",
      "Epoch 14/200\n",
      "25/46 [===============>..............] - ETA: 3s - loss: 0.9390"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5b7fd3bc6beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbest_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReturnBestEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_f1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf1_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#model_cnn = load_model('best_model.h5', custom_objects={'f1_macro': f1_macro})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    #optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=False),\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.07, momentum=0, nesterov=False),\n",
    "    #metrics=[f1_macro]\n",
    ")\n",
    "#mc = ModelCheckpoint('best_model.h5', monitor='val_f1_macro', mode='max', save_best_only=True, verbose=1)\n",
    "#es = EarlyStopping(monitor=\"val_f1_macro\", min_delta=0, patience=200, verbose=1, mode=\"max\", restore_best_weights=False)\n",
    "f1_callback = FCallback(validation = (X_val, y_val), verbose=True)                                   \n",
    "best_callback = ReturnBestEarlyStopping(monitor=\"val_f1\", min_delta=0, patience=100, verbose=1, mode=\"max\", restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=200, validation_split=0.05, callbacks=[f1_callback, best_callback], verbose = 1)\n",
    "#model_cnn = load_model('best_model.h5', custom_objects={'f1_macro': f1_macro})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------\n",
    "# OLD\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kim_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_O2PADdMr8m"
   },
   "source": [
    "# model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XYR51KuDfOv",
    "outputId": "2acaf42f-a072-4d0c-980a-0e07f6e73b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "22\n",
      "22\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, 70, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 69, 128)      32896       text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, 68, 128)      49280       text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, 67, 128)      65664       text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 69, 128)      512         conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 68, 128)      512         conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 67, 128)      512         conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 69, 128)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 68, 128)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 67, 128)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling1D) (None, 5, 128)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling1D) (None, 5, 128)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling1D) (None, 5, 128)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 15, 128)      0           max_pooling1d_75[0][0]           \n",
      "                                                                 max_pooling1d_76[0][0]           \n",
      "                                                                 max_pooling1d_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, 10, 256)      196864      concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 256)          0           conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 256)          0           global_max_pooling1d_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "extra (InputLayer)              [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 261)          0           flatten_25[0][0]                 \n",
      "                                                                 extra[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 128)          33536       concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 128)          33536       concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 64)           8256        dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 64)           8256        dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "st (Dense)                      (None, 1)            65          dense_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hs (Dense)                      (None, 1)            65          dense_99[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 429,954\n",
      "Trainable params: 429,186\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(print_model=False):\n",
    "  \"\"\" HyperParameters \"\"\"\n",
    "  FILTERS = 128\n",
    "  pooling_units = 3\n",
    "  output_dims = 1\n",
    "  hidden_dims= 1\n",
    "\n",
    "  text_seq_input = Input(shape=(MAX_TEXT_LEN, WORD_EMB_SIZE,), name=\"text\")\n",
    "  text_embedding = Embedding(VOCAB_SIZE, WORD_EMB_SIZE, input_length=MAX_TEXT_LEN,\n",
    "                              weights=[embedding_matrix], trainable=False)(text_seq_input)\n",
    "  extra_feature = Input(shape=(5,), name = \"extra\")\n",
    "\n",
    "  #text_embedding = Embedding(vocab_size, WORD_EMB_SIZE, input_length=MAX_TEXT_LEN,\n",
    "  #                            weights=[embedding_matrix], trainable=False)(text_seq_input)\n",
    "  #text_dropout = Dropout(0.25)(text_embedding)\n",
    "\n",
    "  filter_sizes = [2,3,4]\n",
    "  convs = []\n",
    "  for filter_size in filter_sizes:\n",
    "      l_conv = Conv1D(filters=FILTERS, kernel_size=filter_size)(text_seq_input)\n",
    "      l_batchnorm = BatchNormalization()(l_conv)\n",
    "      l_relu = Activation(\"relu\")(l_batchnorm)\n",
    "      POOL_SIZE = l_conv.get_shape()[-2] // pooling_units\n",
    "      print(POOL_SIZE)\n",
    "      l_pool = MaxPool1D(pool_size=POOL_SIZE, strides =10, padding='valid')(l_relu)   #Dynamic pooling\n",
    "      #l_conv = Conv1D(filters=64, kernel_size=filter_size, activation='relu')(l_pool)\n",
    "      #POOL_SIZE = l_conv.get_shape()[-2] // pooling_units\n",
    "      #l_pool = MaxPool1D(pool_size=POOL_SIZE, strides =1, padding='valid')(l_conv)   #Dynamic pooling\n",
    "      convs.append(l_pool)\n",
    "\n",
    "  l_merge = Concatenate(axis=1)(convs)\n",
    "  l_cov1= Conv1D(256, 6, activation='relu')(l_merge)\n",
    "  # since the text is too long we are maxooling over 100\n",
    "  # and not GlobalMaxPool1D\n",
    "  l_pool1 = GlobalMaxPool1D()(l_cov1)\n",
    "  l_flat = Flatten()(l_pool1)\n",
    "\n",
    "  l_flat = Concatenate(axis=1)([l_flat, extra_feature])\n",
    "\n",
    "  l_hidden = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002))(l_flat)\n",
    "  l_hidden = Dense(64, activation='relu',  kernel_regularizer=tf.keras.regularizers.l2(0.002))(l_hidden)\n",
    "  l_out_hs = Dense(1, activation='sigmoid', name=\"hs\")(l_hidden)  #dims output\n",
    "\n",
    "  l_hidden = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002))(l_flat )\n",
    "  l_hidden = Dense(64, activation='relu',  kernel_regularizer=tf.keras.regularizers.l2(0.002))(l_hidden)\n",
    "  l_out_st = Dense(1, activation='sigmoid', name=\"st\")(l_hidden)  #dims output\n",
    "\n",
    "  model_cnn = Model(inputs=[text_seq_input, extra_feature], outputs=[l_out_st, l_out_hs])\n",
    "  if print_model:\n",
    "    model_cnn.summary()\n",
    "    tf.keras.utils.plot_model(model_cnn, \"my_first_model.png\", show_shapes=True)\n",
    "  return model_cnn\n",
    "\n",
    "model_cnn = build_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdftbD5AEtwJ",
    "outputId": "f51ce9fe-ac71-4fe1-b2ff-2fca189eb482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, 70, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 67, 1024)     525312      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 33, 1024)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 30, 1024)     4195328     max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 10, 1024)     0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 77, 1024)     0           max_pooling1d_16[0][0]           \n",
      "                                                                 conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 74, 256)      1048832     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 25, 256)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 6400)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "extra (InputLayer)              [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 6405)         0           flatten_5[0][0]                  \n",
      "                                                                 extra[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1024)         6559744     concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 512)          524800      dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            513         dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 12,854,529\n",
      "Trainable params: 12,854,529\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model1(print_model=False):\n",
    "  text_seq_input = Input(shape=(70,128,), name=\"text\")\n",
    "  extra_feature = Input(shape=(5,), name = \"extra\")\n",
    "\n",
    "  l_conv = Conv1D(filters=1024, kernel_size=4, activation='relu')(text_seq_input)\n",
    "  l_pool = MaxPool1D(2)(l_conv)   #Dynamic pooling\n",
    "  l_cov1= Conv1D(1024, 4, activation='relu')(l_pool)\n",
    "  l_pool1 = MaxPool1D(3, strides=3,  padding=\"valid\")(l_cov1)\n",
    "  conc = Concatenate(axis=1)([l_pool1, l_conv])\n",
    "  l_cov1= Conv1D(256, 4, activation='relu')(conc)\n",
    "  l_pool1 = MaxPool1D(2, strides=3,  padding=\"valid\")(l_cov1)\n",
    "  l_flat = Flatten()(l_pool1)\n",
    "  l_flat = Concatenate(axis=1)([l_flat, extra_feature])\n",
    "  l_hidden = Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(l_flat)\n",
    "  l_hidden = Dense(512, activation='relu',  kernel_regularizer=tf.keras.regularizers.l2(0.01))(l_hidden)\n",
    "  l_out = Dense(1, activation='sigmoid')(l_hidden)  #dims output\n",
    "  model_cnn = Model(inputs=[text_seq_input, extra_feature], outputs=l_out)\n",
    "  if print_model:\n",
    "    model_cnn.summary()\n",
    "    tf.keras.utils.plot_model(model_cnn, \"my_first_model.png\", show_shapes=True)\n",
    "  return model_cnn\n",
    "\n",
    "model_cnn = build_model1(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkAEibN9rmW8",
    "outputId": "b06ab86e-2a18-4f9a-ca62-74fd926f344b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, 70, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512)          788480      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "extra (InputLayer)              [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 517)          0           bidirectional_1[0][0]            \n",
      "                                                                 extra[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 256)          132608      concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 1)            257         dense_80[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 921,345\n",
      "Trainable params: 921,345\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model2():\n",
    "  text_seq_input = Input(shape=(70,128,), name=\"text\")\n",
    "  extra_feature = Input(shape=(5,), name = \"extra\")\n",
    "  convs = []\n",
    "\n",
    "  sentence_encoder = Bidirectional(LSTM(256, activation=\"relu\", return_sequences=False))(text_seq_input)\n",
    "  l_flat = Concatenate(axis=1)([sentence_encoder, extra_feature])\n",
    "  fc_layer =Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.007))(l_flat)\n",
    "  output_layer = Dense(1,activation=\"sigmoid\")(fc_layer)\n",
    "\n",
    "  model = Model(inputs=[text_seq_input, extra_feature], outputs=output_layer)\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "model_cnn = build_model2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlsEdaszMt-U"
   },
   "source": [
    "# model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pBEgvL1MwBz"
   },
   "outputs": [],
   "source": [
    "\"\"\" HyperParameters \"\"\"\n",
    "FILTERS = 256\n",
    "pooling_units = 10\n",
    "output_dims = 1\n",
    "hidden_dims= 1\n",
    "\n",
    "text_seq_input = Input(shape=(40,128,), name=\"text\")\n",
    "extra_feature = Input(shape=(5,), name = \"extra\")\n",
    "\n",
    "#text_embedding = Embedding(vocab_size, WORD_EMB_SIZE, input_length=MAX_TEXT_LEN,\n",
    "#                            weights=[embedding_matrix], trainable=False)(text_seq_input)\n",
    "#text_dropout = Dropout(0.25)(text_embedding)\n",
    "\n",
    "filter_sizes = [1,2,3]\n",
    "convs = []\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = Conv1D(filters=FILTERS, kernel_size=filter_size, activation='relu')(text_seq_input)\n",
    "    POOL_SIZE = l_conv.get_shape()[-2] // pooling_units\n",
    "    l_pool = MaxPool1D(pool_size=POOL_SIZE, strides =3, padding='valid')(l_conv)   #Dynamic pooling\n",
    "    l_conv = Conv1D(filters=64, kernel_size=filter_size, activation='relu')(l_pool)\n",
    "    POOL_SIZE = l_conv.get_shape()[-2] // pooling_units\n",
    "    l_pool = MaxPool1D(pool_size=POOL_SIZE, strides =1, padding='valid')(l_conv)   #Dynamic pooling\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_global_pool = GlobalMaxPool1D()(l_merge)\n",
    "l_cov1= Conv1D(32, 10, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.015))(l_merge)\n",
    "# since the text is too long we are maxooling over 100\n",
    "# and not GlobalMaxPool1D\n",
    "l_pool1 = MaxPool1D(3)(l_cov1)\n",
    "l_flat = Flatten()(l_pool1)\n",
    "l_flat = Concatenate(axis=1)([l_flat,l_global_pool, extra_feature])\n",
    "l_hidden = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.015))(l_flat)\n",
    "l_hidden = Dense(256, activation='relu',  kernel_regularizer=tf.keras.regularizers.l2(0.015))(l_hidden)\n",
    "l_hidden = Dense(128, activation='relu',  kernel_regularizer=tf.keras.regularizers.l2(0.015))(l_hidden)\n",
    "l_out = Dense(1, activation='sigmoid')(l_hidden)  #dims output\n",
    "model_cnn = Model(inputs=[text_seq_input, extra_feature], outputs=l_out)\n",
    "model_cnn.summary()\n",
    "tf.keras.utils.plot_model(model_cnn, \"my_first_model.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-RtVYAHM6ii"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mognZ4ph34PB"
   },
   "outputs": [],
   "source": [
    "def binary_macro_f1(y_true, y_pred):\n",
    "    tp = K.sum(y_true * K.round(y_pred))\n",
    "    tn = K.sum((K.round(y_pred) - 1) * (y_true - 1))\n",
    "    fp = K.sum(y_true * (1. - K.round(y_pred)))\n",
    "    fn = K.sum((1. - y_true) * K.round(y_pred))\n",
    "\n",
    "    precision_1 = tp / (tp +fp + K.epsilon())\n",
    "    recall_1 = tp / (tp +fn + K.epsilon())\n",
    "    f1_val_1 = 2*(precision_1*recall_1)/(precision_1+recall_1+K.epsilon())\n",
    "    \n",
    "    precision_0 = tn / (tn +fn + K.epsilon())\n",
    "    recall_0 = tn / (tn +fp + K.epsilon())\n",
    "    f1_val_0 = 2*(precision_0*recall_0)/(precision_0+recall_0+K.epsilon())\n",
    "    macro_f1 = (f1_val_0 + f1_val_1) / 2\n",
    "\n",
    "    return macro_f1\n",
    "\n",
    "def f1_sklean_mapping_macro(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "def f1_macro(y_true, y_pred):\n",
    "    return tf.py_function(f1_sklean_mapping_macro, (y_true, K.round(y_pred)), tf.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAeKX-jSI7v7",
    "outputId": "99d396b3-25b7-434c-e5b0-bdbd0ad2613a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "model_cnn = build_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2L2BLXXIv0Z",
    "outputId": "41e7fae4-e631-48e9-e313-e8778d7c38c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "49/49 - 2s - loss: 2.6833 - st_loss: 1.3145 - hs_loss: 1.7056 - val_loss: 2.0353 - val_st_loss: 0.7568 - val_hs_loss: 1.2634\n",
      "— val_f1: 0.37382493667323385\n",
      "Epoch 2/200\n",
      "49/49 - 1s - loss: 1.8760 - st_loss: 0.6817 - hs_loss: 0.8495 - val_loss: 1.8674 - val_st_loss: 0.7034 - val_hs_loss: 0.7057\n",
      "— val_f1: 0.5543586847934674\n",
      "Epoch 3/200\n",
      "49/49 - 1s - loss: 1.8482 - st_loss: 0.6801 - hs_loss: 0.7326 - val_loss: 1.8971 - val_st_loss: 0.7003 - val_hs_loss: 0.8836\n",
      "— val_f1: 0.3922161009418874\n",
      "Epoch 4/200\n",
      "49/49 - 1s - loss: 1.8213 - st_loss: 0.6690 - hs_loss: 0.6676 - val_loss: 1.8828 - val_st_loss: 0.7402 - val_hs_loss: 0.6268\n",
      "— val_f1: 0.3731375053214134\n",
      "Epoch 5/200\n",
      "49/49 - 1s - loss: 1.8242 - st_loss: 0.6726 - hs_loss: 0.6782 - val_loss: 1.8634 - val_st_loss: 0.7040 - val_hs_loss: 0.7249\n",
      "— val_f1: 0.5978991272109369\n",
      "Epoch 6/200\n",
      "49/49 - 1s - loss: 1.8096 - st_loss: 0.6678 - hs_loss: 0.6435 - val_loss: 1.8210 - val_st_loss: 0.6752 - val_hs_loss: 0.6703\n",
      "— val_f1: 0.41887791536240393\n",
      "Epoch 7/200\n",
      "49/49 - 1s - loss: 1.8067 - st_loss: 0.6661 - hs_loss: 0.6514 - val_loss: 1.8424 - val_st_loss: 0.7040 - val_hs_loss: 0.6477\n",
      "— val_f1: 0.4989928584508332\n",
      "Epoch 8/200\n",
      "49/49 - 1s - loss: 1.7966 - st_loss: 0.6606 - hs_loss: 0.6423 - val_loss: 2.0744 - val_st_loss: 0.9400 - val_hs_loss: 0.6412\n",
      "— val_f1: 0.35104364326375714\n",
      "Epoch 9/200\n",
      "49/49 - 1s - loss: 1.7933 - st_loss: 0.6644 - hs_loss: 0.6202 - val_loss: 2.0403 - val_st_loss: 0.8844 - val_hs_loss: 0.7628\n",
      "— val_f1: 0.3580401405400202\n",
      "Epoch 10/200\n",
      "49/49 - 1s - loss: 1.7922 - st_loss: 0.6679 - hs_loss: 0.6112 - val_loss: 1.8030 - val_st_loss: 0.6725 - val_hs_loss: 0.6494\n",
      "— val_f1: 0.5855122426474524\n",
      "Epoch 11/200\n",
      "49/49 - 1s - loss: 1.7881 - st_loss: 0.6607 - hs_loss: 0.6402 - val_loss: 1.8342 - val_st_loss: 0.6629 - val_hs_loss: 0.8667\n",
      "— val_f1: 0.6033603011078128\n",
      "Epoch 12/200\n",
      "49/49 - 1s - loss: 1.7822 - st_loss: 0.6598 - hs_loss: 0.6293 - val_loss: 2.4023 - val_st_loss: 1.2800 - val_hs_loss: 0.6356\n",
      "— val_f1: 0.35104364326375714\n",
      "Epoch 13/200\n",
      "49/49 - 1s - loss: 1.7917 - st_loss: 0.6758 - hs_loss: 0.6101 - val_loss: 1.8548 - val_st_loss: 0.6728 - val_hs_loss: 0.9475\n",
      "— val_f1: 0.5932194687842278\n",
      "Epoch 14/200\n",
      "49/49 - 1s - loss: 1.7709 - st_loss: 0.6568 - hs_loss: 0.6142 - val_loss: 1.7955 - val_st_loss: 0.6587 - val_hs_loss: 0.7348\n",
      "— val_f1: 0.6012841594236944\n",
      "Epoch 15/200\n",
      "49/49 - 1s - loss: 1.7662 - st_loss: 0.6544 - hs_loss: 0.6164 - val_loss: 1.7931 - val_st_loss: 0.6849 - val_hs_loss: 0.6052\n",
      "— val_f1: 0.5087719298245614\n",
      "Epoch 16/200\n",
      "49/49 - 1s - loss: 1.7605 - st_loss: 0.6544 - hs_loss: 0.6015 - val_loss: 1.7888 - val_st_loss: 0.6726 - val_hs_loss: 0.6588\n",
      "— val_f1: 0.3922161009418874\n",
      "Epoch 17/200\n",
      "49/49 - 1s - loss: 1.7535 - st_loss: 0.6502 - hs_loss: 0.6005 - val_loss: 1.8398 - val_st_loss: 0.7177 - val_hs_loss: 0.7019\n",
      "— val_f1: 0.481271432339752\n",
      "Epoch 18/200\n",
      "49/49 - 1s - loss: 1.7520 - st_loss: 0.6538 - hs_loss: 0.5889 - val_loss: 1.7956 - val_st_loss: 0.6839 - val_hs_loss: 0.6628\n",
      "— val_f1: 0.4287674584049197\n",
      "Epoch 19/200\n",
      "49/49 - 1s - loss: 1.7502 - st_loss: 0.6496 - hs_loss: 0.6138 - val_loss: 1.8560 - val_st_loss: 0.7303 - val_hs_loss: 0.7459\n",
      "— val_f1: 0.3931689706923838\n",
      "Epoch 20/200\n",
      "49/49 - 1s - loss: 1.7385 - st_loss: 0.6468 - hs_loss: 0.5825 - val_loss: 1.8000 - val_st_loss: 0.7091 - val_hs_loss: 0.5851\n",
      "— val_f1: 0.3731375053214134\n",
      "Epoch 21/200\n",
      "49/49 - 1s - loss: 1.7315 - st_loss: 0.6431 - hs_loss: 0.5792 - val_loss: 1.7593 - val_st_loss: 0.6634 - val_hs_loss: 0.6235\n",
      "— val_f1: 0.5902002534417214\n",
      "Epoch 22/200\n",
      "49/49 - 1s - loss: 1.7309 - st_loss: 0.6442 - hs_loss: 0.5838 - val_loss: 1.7494 - val_st_loss: 0.6642 - val_hs_loss: 0.5829\n",
      "— val_f1: 0.4333591003166845\n",
      "Epoch 23/200\n",
      "49/49 - 1s - loss: 1.7247 - st_loss: 0.6410 - hs_loss: 0.5818 - val_loss: 1.7670 - val_st_loss: 0.6441 - val_hs_loss: 0.7848\n",
      "— val_f1: 0.610413005660394\n",
      "Epoch 24/200\n",
      "49/49 - 1s - loss: 1.7229 - st_loss: 0.6419 - hs_loss: 0.5813 - val_loss: 1.7553 - val_st_loss: 0.6727 - val_hs_loss: 0.5961\n",
      "— val_f1: 0.36495956873315366\n",
      "Epoch 25/200\n",
      "49/49 - 1s - loss: 1.7128 - st_loss: 0.6361 - hs_loss: 0.5728 - val_loss: 1.9843 - val_st_loss: 0.8577 - val_hs_loss: 0.8287\n",
      "— val_f1: 0.3206373882627283\n",
      "Epoch 26/200\n",
      "49/49 - 1s - loss: 1.7215 - st_loss: 0.6465 - hs_loss: 0.5774 - val_loss: 1.7578 - val_st_loss: 0.6741 - val_hs_loss: 0.6278\n",
      "— val_f1: 0.40661309820095287\n",
      "Epoch 27/200\n",
      "49/49 - 1s - loss: 1.7136 - st_loss: 0.6401 - hs_loss: 0.5826 - val_loss: 1.9662 - val_st_loss: 0.8157 - val_hs_loss: 0.9744\n",
      "— val_f1: 0.3580401405400202\n",
      "Epoch 28/200\n",
      "49/49 - 1s - loss: 1.7059 - st_loss: 0.6365 - hs_loss: 0.5749 - val_loss: 2.2986 - val_st_loss: 1.2312 - val_hs_loss: 0.5717\n",
      "— val_f1: 0.35104364326375714\n",
      "Epoch 29/200\n",
      "49/49 - 1s - loss: 1.7279 - st_loss: 0.6630 - hs_loss: 0.5649 - val_loss: 1.8440 - val_st_loss: 0.7380 - val_hs_loss: 0.7773\n",
      "— val_f1: 0.42373708287331724\n",
      "Epoch 30/200\n",
      "49/49 - 1s - loss: 1.6976 - st_loss: 0.6358 - hs_loss: 0.5621 - val_loss: 1.7421 - val_st_loss: 0.6794 - val_hs_loss: 0.5735\n",
      "— val_f1: 0.40822259136212624\n",
      "Epoch 31/200\n",
      "49/49 - 1s - loss: 1.6845 - st_loss: 0.6263 - hs_loss: 0.5572 - val_loss: 1.7433 - val_st_loss: 0.6495 - val_hs_loss: 0.7421\n",
      "— val_f1: 0.603401623502126\n",
      "Epoch 32/200\n",
      "49/49 - 1s - loss: 1.6847 - st_loss: 0.6274 - hs_loss: 0.5654 - val_loss: 1.8287 - val_st_loss: 0.6982 - val_hs_loss: 0.9378\n",
      "— val_f1: 0.40586702443303496\n",
      "Epoch 33/200\n",
      "49/49 - 1s - loss: 1.6825 - st_loss: 0.6286 - hs_loss: 0.5611 - val_loss: 1.7061 - val_st_loss: 0.6523 - val_hs_loss: 0.5674\n",
      "— val_f1: 0.6124152229679868\n",
      "Epoch 34/200\n",
      "49/49 - 1s - loss: 1.6748 - st_loss: 0.6256 - hs_loss: 0.5500 - val_loss: 1.8745 - val_st_loss: 0.8223 - val_hs_loss: 0.5719\n",
      "— val_f1: 0.3441295546558705\n",
      "Epoch 35/200\n",
      "49/49 - 1s - loss: 1.6797 - st_loss: 0.6336 - hs_loss: 0.5474 - val_loss: 1.7902 - val_st_loss: 0.7175 - val_hs_loss: 0.6865\n",
      "— val_f1: 0.5096774193548387\n",
      "Epoch 36/200\n",
      "49/49 - 1s - loss: 1.6689 - st_loss: 0.6241 - hs_loss: 0.5533 - val_loss: 1.8057 - val_st_loss: 0.7530 - val_hs_loss: 0.5990\n",
      "— val_f1: 0.36430276931904093\n",
      "Epoch 37/200\n",
      "49/49 - 1s - loss: 1.6674 - st_loss: 0.6269 - hs_loss: 0.5445 - val_loss: 1.7898 - val_st_loss: 0.6969 - val_hs_loss: 0.8129\n",
      "— val_f1: 0.5779246277950434\n",
      "Epoch 38/200\n",
      "49/49 - 1s - loss: 1.6692 - st_loss: 0.6299 - hs_loss: 0.5509 - val_loss: 1.7323 - val_st_loss: 0.6758 - val_hs_loss: 0.6428\n",
      "— val_f1: 0.505439570963417\n",
      "Epoch 39/200\n",
      "49/49 - 1s - loss: 1.6559 - st_loss: 0.6196 - hs_loss: 0.5482 - val_loss: 1.7757 - val_st_loss: 0.7069 - val_hs_loss: 0.7170\n",
      "— val_f1: 0.3757874780683678\n",
      "Epoch 40/200\n",
      "49/49 - 1s - loss: 1.6557 - st_loss: 0.6226 - hs_loss: 0.5443 - val_loss: 1.6893 - val_st_loss: 0.6371 - val_hs_loss: 0.6460\n",
      "— val_f1: 0.6079340759451737\n",
      "Epoch 41/200\n",
      "49/49 - 1s - loss: 1.6379 - st_loss: 0.6084 - hs_loss: 0.5390 - val_loss: 3.0567 - val_st_loss: 1.9949 - val_hs_loss: 0.7063\n",
      "— val_f1: 0.35104364326375714\n",
      "Epoch 42/200\n",
      "49/49 - 1s - loss: 1.6763 - st_loss: 0.6477 - hs_loss: 0.5467 - val_loss: 1.6716 - val_st_loss: 0.6330 - val_hs_loss: 0.6028\n",
      "— val_f1: 0.6216604060270967\n",
      "Epoch 43/200\n",
      "49/49 - 1s - loss: 1.6381 - st_loss: 0.6142 - hs_loss: 0.5353 - val_loss: 1.7667 - val_st_loss: 0.7390 - val_hs_loss: 0.5604\n",
      "— val_f1: 0.5207767986658641\n",
      "Epoch 44/200\n",
      "49/49 - 1s - loss: 1.6286 - st_loss: 0.6091 - hs_loss: 0.5256 - val_loss: 1.6394 - val_st_loss: 0.6106 - val_hs_loss: 0.5781\n",
      "— val_f1: 0.6429054372106158\n",
      "Epoch 45/200\n",
      "49/49 - 1s - loss: 1.6232 - st_loss: 0.6051 - hs_loss: 0.5307 - val_loss: 1.7453 - val_st_loss: 0.7241 - val_hs_loss: 0.5523\n",
      "— val_f1: 0.38122605363984674\n",
      "Epoch 46/200\n",
      "49/49 - 1s - loss: 1.6183 - st_loss: 0.6034 - hs_loss: 0.5269 - val_loss: 1.7233 - val_st_loss: 0.6885 - val_hs_loss: 0.6326\n",
      "— val_f1: 0.5981484778351434\n",
      "Epoch 47/200\n",
      "49/49 - 1s - loss: 1.6275 - st_loss: 0.6138 - hs_loss: 0.5328 - val_loss: 1.7034 - val_st_loss: 0.6711 - val_hs_loss: 0.6318\n",
      "— val_f1: 0.626149079015321\n",
      "Epoch 48/200\n",
      "49/49 - 1s - loss: 1.6045 - st_loss: 0.5955 - hs_loss: 0.5210 - val_loss: 1.6278 - val_st_loss: 0.6138 - val_hs_loss: 0.5526\n",
      "— val_f1: 0.5895652833876066\n",
      "Epoch 49/200\n",
      "49/49 - 1s - loss: 1.6142 - st_loss: 0.6062 - hs_loss: 0.5281 - val_loss: 1.7962 - val_st_loss: 0.7250 - val_hs_loss: 0.8502\n",
      "— val_f1: 0.3791079501890384\n",
      "Epoch 50/200\n",
      "49/49 - 1s - loss: 1.6037 - st_loss: 0.5978 - hs_loss: 0.5296 - val_loss: 1.6284 - val_st_loss: 0.6115 - val_hs_loss: 0.5906\n",
      "— val_f1: 0.5760222764723832\n",
      "Epoch 51/200\n",
      "49/49 - 1s - loss: 1.6055 - st_loss: 0.6034 - hs_loss: 0.5225 - val_loss: 1.8216 - val_st_loss: 0.7934 - val_hs_loss: 0.6593\n",
      "— val_f1: 0.37718177948063003\n",
      "Epoch 52/200\n",
      "49/49 - 1s - loss: 1.5902 - st_loss: 0.5922 - hs_loss: 0.5141 - val_loss: 1.7201 - val_st_loss: 0.7163 - val_hs_loss: 0.5491\n",
      "— val_f1: 0.4944151069599716\n",
      "Epoch 53/200\n",
      "49/49 - 1s - loss: 1.6210 - st_loss: 0.6225 - hs_loss: 0.5285 - val_loss: 2.0671 - val_st_loss: 1.0003 - val_hs_loss: 0.8760\n",
      "— val_f1: 0.35104364326375714\n",
      "Epoch 54/200\n",
      "49/49 - 1s - loss: 1.5930 - st_loss: 0.5990 - hs_loss: 0.5178 - val_loss: 1.8818 - val_st_loss: 0.8206 - val_hs_loss: 0.8598\n",
      "— val_f1: 0.42090931972633455\n",
      "Epoch 55/200\n",
      "49/49 - 1s - loss: 1.5787 - st_loss: 0.5857 - hs_loss: 0.5246 - val_loss: 1.6491 - val_st_loss: 0.5945 - val_hs_loss: 0.8385\n",
      "— val_f1: 0.6467358576691218\n",
      "Epoch 56/200\n",
      "49/49 - 1s - loss: 1.5759 - st_loss: 0.5886 - hs_loss: 0.5075 - val_loss: 1.9477 - val_st_loss: 0.9441 - val_hs_loss: 0.5949\n",
      "— val_f1: 0.3441295546558705\n",
      "Epoch 57/200\n",
      "49/49 - 1s - loss: 1.5865 - st_loss: 0.5996 - hs_loss: 0.5176 - val_loss: 1.7987 - val_st_loss: 0.7982 - val_hs_loss: 0.5913\n",
      "— val_f1: 0.4441541935320599\n",
      "Epoch 58/200\n",
      "49/49 - 1s - loss: 1.5588 - st_loss: 0.5773 - hs_loss: 0.5019 - val_loss: 1.9398 - val_st_loss: 0.9439 - val_hs_loss: 0.5800\n",
      "— val_f1: 0.3878690107509155\n",
      "Epoch 59/200\n",
      "49/49 - 1s - loss: 1.5698 - st_loss: 0.5916 - hs_loss: 0.4970 - val_loss: 1.9142 - val_st_loss: 0.8848 - val_hs_loss: 0.7590\n",
      "— val_f1: 0.37973087065739686\n",
      "Epoch 60/200\n",
      "49/49 - 1s - loss: 1.5621 - st_loss: 0.5832 - hs_loss: 0.5122 - val_loss: 1.6266 - val_st_loss: 0.6437 - val_hs_loss: 0.5381\n",
      "— val_f1: 0.4993645215415307\n",
      "Epoch 61/200\n",
      "49/49 - 1s - loss: 1.5576 - st_loss: 0.5814 - hs_loss: 0.5098 - val_loss: 1.6943 - val_st_loss: 0.6607 - val_hs_loss: 0.8033\n",
      "— val_f1: 0.5109757712167351\n",
      "Epoch 62/200\n",
      "49/49 - 1s - loss: 1.5475 - st_loss: 0.5743 - hs_loss: 0.5064 - val_loss: 1.5949 - val_st_loss: 0.6039 - val_hs_loss: 0.6018\n",
      "— val_f1: 0.6341760961810468\n",
      "Epoch 63/200\n",
      "49/49 - 1s - loss: 1.5481 - st_loss: 0.5771 - hs_loss: 0.5069 - val_loss: 1.6499 - val_st_loss: 0.6708 - val_hs_loss: 0.5532\n",
      "— val_f1: 0.496578711496841\n",
      "Epoch 64/200\n",
      "49/49 - 1s - loss: 1.5452 - st_loss: 0.5780 - hs_loss: 0.4997 - val_loss: 1.7148 - val_st_loss: 0.6416 - val_hs_loss: 1.0354\n",
      "— val_f1: 0.5805477454961991\n",
      "Epoch 65/200\n",
      "49/49 - 1s - loss: 1.5376 - st_loss: 0.5711 - hs_loss: 0.5071 - val_loss: 1.5705 - val_st_loss: 0.5801 - val_hs_loss: 0.6328\n",
      "— val_f1: 0.6545699401070277\n",
      "Epoch 66/200\n",
      "49/49 - 1s - loss: 1.5308 - st_loss: 0.5687 - hs_loss: 0.4965 - val_loss: 1.7287 - val_st_loss: 0.7366 - val_hs_loss: 0.6526\n",
      "— val_f1: 0.47498873243283846\n",
      "Epoch 67/200\n",
      "49/49 - 1s - loss: 1.5341 - st_loss: 0.5731 - hs_loss: 0.5025 - val_loss: 1.6143 - val_st_loss: 0.6244 - val_hs_loss: 0.6526\n",
      "— val_f1: 0.608888475663614\n",
      "Epoch 68/200\n",
      "49/49 - 1s - loss: 1.5264 - st_loss: 0.5686 - hs_loss: 0.4976 - val_loss: 1.6299 - val_st_loss: 0.5926 - val_hs_loss: 0.9012\n",
      "— val_f1: 0.6560812956494124\n",
      "Epoch 69/200\n",
      "49/49 - 1s - loss: 1.5169 - st_loss: 0.5626 - hs_loss: 0.4913 - val_loss: 1.6823 - val_st_loss: 0.7192 - val_hs_loss: 0.5412\n",
      "— val_f1: 0.5420413460647074\n",
      "Epoch 70/200\n",
      "49/49 - 1s - loss: 1.5278 - st_loss: 0.5753 - hs_loss: 0.4935 - val_loss: 1.7049 - val_st_loss: 0.7046 - val_hs_loss: 0.7384\n",
      "— val_f1: 0.5803525889788225\n",
      "Epoch 71/200\n",
      "49/49 - 1s - loss: 1.5333 - st_loss: 0.5838 - hs_loss: 0.4899 - val_loss: 1.7186 - val_st_loss: 0.6961 - val_hs_loss: 0.8610\n",
      "— val_f1: 0.48511025400227054\n",
      "Epoch 72/200\n",
      "49/49 - 1s - loss: 1.4897 - st_loss: 0.5422 - hs_loss: 0.4905 - val_loss: 1.6423 - val_st_loss: 0.6415 - val_hs_loss: 0.7629\n",
      "— val_f1: 0.587078341013825\n",
      "Epoch 73/200\n",
      "49/49 - 1s - loss: 1.4911 - st_loss: 0.5465 - hs_loss: 0.4872 - val_loss: 2.7026 - val_st_loss: 1.6843 - val_hs_loss: 0.8615\n",
      "— val_f1: 0.35455162121828787\n",
      "Epoch 74/200\n",
      "49/49 - 1s - loss: 1.5242 - st_loss: 0.5811 - hs_loss: 0.4910 - val_loss: 1.7555 - val_st_loss: 0.7971 - val_hs_loss: 0.5729\n",
      "— val_f1: 0.450726106045255\n",
      "Epoch 75/200\n",
      "49/49 - 1s - loss: 1.4761 - st_loss: 0.5384 - hs_loss: 0.4746 - val_loss: 2.4167 - val_st_loss: 1.4501 - val_hs_loss: 0.6248\n",
      "— val_f1: 0.3580401405400202\n",
      "Epoch 76/200\n",
      "49/49 - 1s - loss: 1.5505 - st_loss: 0.6140 - hs_loss: 0.4800 - val_loss: 1.5291 - val_st_loss: 0.5776 - val_hs_loss: 0.5602\n",
      "— val_f1: 0.6620436184598315\n",
      "Epoch 77/200\n",
      "49/49 - 1s - loss: 1.4769 - st_loss: 0.5412 - hs_loss: 0.4868 - val_loss: 1.6102 - val_st_loss: 0.6010 - val_hs_loss: 0.8604\n",
      "— val_f1: 0.6475871844256584\n",
      "Epoch 78/200\n",
      "49/49 - 1s - loss: 1.4583 - st_loss: 0.5261 - hs_loss: 0.4804 - val_loss: 1.5108 - val_st_loss: 0.5680 - val_hs_loss: 0.5391\n",
      "— val_f1: 0.6978979957634024\n",
      "Epoch 79/200\n",
      "49/49 - 1s - loss: 1.4738 - st_loss: 0.5431 - hs_loss: 0.4837 - val_loss: 1.8289 - val_st_loss: 0.8891 - val_hs_loss: 0.5352\n",
      "— val_f1: 0.49878402344870065\n",
      "Epoch 80/200\n",
      "49/49 - 1s - loss: 1.4611 - st_loss: 0.5349 - hs_loss: 0.4720 - val_loss: 1.5357 - val_st_loss: 0.5677 - val_hs_loss: 0.6866\n",
      "— val_f1: 0.688724385423529\n",
      "Epoch 81/200\n",
      "49/49 - 1s - loss: 1.4687 - st_loss: 0.5450 - hs_loss: 0.4705 - val_loss: 1.5182 - val_st_loss: 0.5817 - val_hs_loss: 0.5398\n",
      "— val_f1: 0.6822488467314154\n",
      "Epoch 82/200\n",
      "49/49 - 1s - loss: 1.4501 - st_loss: 0.5279 - hs_loss: 0.4736 - val_loss: 1.7867 - val_st_loss: 0.8202 - val_hs_loss: 0.7004\n",
      "— val_f1: 0.5054731332253736\n",
      "Epoch 83/200\n",
      "49/49 - 1s - loss: 1.4473 - st_loss: 0.5272 - hs_loss: 0.4740 - val_loss: 1.7246 - val_st_loss: 0.7555 - val_hs_loss: 0.7242\n",
      "— val_f1: 0.6060050798258345\n",
      "Epoch 84/200\n",
      "49/49 - 1s - loss: 1.4530 - st_loss: 0.5336 - hs_loss: 0.4810 - val_loss: 1.6484 - val_st_loss: 0.6036 - val_hs_loss: 1.1138\n",
      "— val_f1: 0.6386116705652845\n",
      "Epoch 85/200\n",
      "49/49 - 1s - loss: 1.4613 - st_loss: 0.5440 - hs_loss: 0.4814 - val_loss: 1.6284 - val_st_loss: 0.6916 - val_hs_loss: 0.5844\n",
      "— val_f1: 0.44384092040488965\n",
      "Epoch 86/200\n",
      "49/49 - 1s - loss: 1.4337 - st_loss: 0.5233 - hs_loss: 0.4571 - val_loss: 1.5071 - val_st_loss: 0.5750 - val_hs_loss: 0.5713\n",
      "— val_f1: 0.677085152203015\n",
      "Epoch 87/200\n",
      "49/49 - 1s - loss: 1.4401 - st_loss: 0.5294 - hs_loss: 0.4694 - val_loss: 1.5309 - val_st_loss: 0.5872 - val_hs_loss: 0.6400\n",
      "— val_f1: 0.6693079134720701\n",
      "Epoch 88/200\n",
      "49/49 - 1s - loss: 1.4187 - st_loss: 0.5120 - hs_loss: 0.4602 - val_loss: 1.9621 - val_st_loss: 0.9650 - val_hs_loss: 0.9175\n",
      "— val_f1: 0.4355452632300325\n",
      "Epoch 89/200\n",
      "49/49 - 1s - loss: 1.4435 - st_loss: 0.5356 - hs_loss: 0.4765 - val_loss: 1.8923 - val_st_loss: 0.9535 - val_hs_loss: 0.6372\n",
      "— val_f1: 0.43385052975894184\n",
      "Epoch 90/200\n",
      "49/49 - 1s - loss: 1.4176 - st_loss: 0.5166 - hs_loss: 0.4528 - val_loss: 2.0108 - val_st_loss: 0.8872 - val_hs_loss: 1.5711\n",
      "— val_f1: 0.3612063780167365\n",
      "Epoch 91/200\n",
      "49/49 - 1s - loss: 1.4409 - st_loss: 0.5367 - hs_loss: 0.4793 - val_loss: 1.9538 - val_st_loss: 0.9899 - val_hs_loss: 0.7829\n",
      "— val_f1: 0.3887018331462776\n",
      "Epoch 92/200\n",
      "49/49 - 1s - loss: 1.4248 - st_loss: 0.5256 - hs_loss: 0.4644 - val_loss: 1.6165 - val_st_loss: 0.7048 - val_hs_loss: 0.5327\n",
      "— val_f1: 0.6083462732919254\n",
      "Epoch 93/200\n",
      "49/49 - 1s - loss: 1.3840 - st_loss: 0.4892 - hs_loss: 0.4528 - val_loss: 1.7083 - val_st_loss: 0.7995 - val_hs_loss: 0.5282\n",
      "— val_f1: 0.5692640306350085\n",
      "Epoch 94/200\n",
      "49/49 - 1s - loss: 1.3836 - st_loss: 0.4906 - hs_loss: 0.4542 - val_loss: 1.5176 - val_st_loss: 0.5798 - val_hs_loss: 0.6837\n",
      "— val_f1: 0.6924559678678919\n",
      "Epoch 95/200\n",
      "49/49 - 1s - loss: 1.3885 - st_loss: 0.4964 - hs_loss: 0.4601 - val_loss: 2.0712 - val_st_loss: 1.0973 - val_hs_loss: 0.8745\n",
      "— val_f1: 0.4192495546083287\n",
      "Epoch 96/200\n",
      "49/49 - 1s - loss: 1.4164 - st_loss: 0.5231 - hs_loss: 0.4767 - val_loss: 1.4752 - val_st_loss: 0.5738 - val_hs_loss: 0.5225\n",
      "— val_f1: 0.6933386112976139\n",
      "Epoch 97/200\n",
      "49/49 - 1s - loss: 1.4012 - st_loss: 0.5139 - hs_loss: 0.4567 - val_loss: 1.5851 - val_st_loss: 0.6782 - val_hs_loss: 0.5602\n",
      "— val_f1: 0.6801093643198907\n",
      "Epoch 98/200\n",
      "49/49 - 1s - loss: 1.3700 - st_loss: 0.4854 - hs_loss: 0.4534 - val_loss: 1.4922 - val_st_loss: 0.5943 - val_hs_loss: 0.5254\n",
      "— val_f1: 0.6631386136047486\n",
      "Epoch 99/200\n",
      "49/49 - 1s - loss: 1.3712 - st_loss: 0.4867 - hs_loss: 0.4629 - val_loss: 1.4852 - val_st_loss: 0.5819 - val_hs_loss: 0.5629\n",
      "— val_f1: 0.6977969184310764\n",
      "Epoch 100/200\n",
      "49/49 - 1s - loss: 1.3474 - st_loss: 0.4716 - hs_loss: 0.4301 - val_loss: 1.5604 - val_st_loss: 0.6311 - val_hs_loss: 0.7030\n",
      "— val_f1: 0.6191092070164093\n",
      "Epoch 101/200\n",
      "49/49 - 1s - loss: 1.3756 - st_loss: 0.4990 - hs_loss: 0.4444 - val_loss: 1.5011 - val_st_loss: 0.5974 - val_hs_loss: 0.5848\n",
      "— val_f1: 0.6822488467314154\n",
      "Epoch 102/200\n",
      "49/49 - 1s - loss: 1.4638 - st_loss: 0.5871 - hs_loss: 0.4549 - val_loss: 1.6321 - val_st_loss: 0.7392 - val_hs_loss: 0.5410\n",
      "— val_f1: 0.5728188725521182\n",
      "Epoch 103/200\n",
      "49/49 - 1s - loss: 1.3729 - st_loss: 0.4994 - hs_loss: 0.4490 - val_loss: 2.6608 - val_st_loss: 1.7583 - val_hs_loss: 0.5993\n",
      "— val_f1: 0.36495956873315366\n",
      "Epoch 104/200\n",
      "49/49 - 1s - loss: 1.3744 - st_loss: 0.5009 - hs_loss: 0.4590 - val_loss: 2.4339 - val_st_loss: 1.4480 - val_hs_loss: 1.0264\n",
      "— val_f1: 0.3265905593991758\n",
      "Epoch 105/200\n",
      "49/49 - 1s - loss: 1.3828 - st_loss: 0.5130 - hs_loss: 0.4503 - val_loss: 1.5038 - val_st_loss: 0.6174 - val_hs_loss: 0.5389\n",
      "— val_f1: 0.6982222414464294\n",
      "Epoch 106/200\n",
      "49/49 - 1s - loss: 1.3367 - st_loss: 0.4720 - hs_loss: 0.4350 - val_loss: 1.9089 - val_st_loss: 1.0061 - val_hs_loss: 0.6308\n",
      "— val_f1: 0.3412391184293722\n",
      "Epoch 107/200\n",
      "49/49 - 1s - loss: 1.3586 - st_loss: 0.4947 - hs_loss: 0.4412 - val_loss: 1.5935 - val_st_loss: 0.6800 - val_hs_loss: 0.6940\n",
      "— val_f1: 0.5941363578883885\n",
      "Epoch 108/200\n",
      "49/49 - 1s - loss: 1.3441 - st_loss: 0.4819 - hs_loss: 0.4426 - val_loss: 1.5249 - val_st_loss: 0.6466 - val_hs_loss: 0.5280\n",
      "— val_f1: 0.6085769980506822\n",
      "Epoch 109/200\n",
      "49/49 - 1s - loss: 1.3189 - st_loss: 0.4594 - hs_loss: 0.4388 - val_loss: 2.5202 - val_st_loss: 1.5335 - val_hs_loss: 1.0799\n",
      "— val_f1: 0.4049395691014188\n",
      "Epoch 110/200\n",
      "49/49 - 1s - loss: 1.3662 - st_loss: 0.5044 - hs_loss: 0.4606 - val_loss: 1.5011 - val_st_loss: 0.6039 - val_hs_loss: 0.6424\n",
      "— val_f1: 0.6329352664405784\n",
      "Epoch 111/200\n",
      "49/49 - 1s - loss: 1.3089 - st_loss: 0.4545 - hs_loss: 0.4335 - val_loss: 1.5792 - val_st_loss: 0.6982 - val_hs_loss: 0.5710\n",
      "— val_f1: 0.651931501393867\n",
      "Epoch 112/200\n",
      "49/49 - 1s - loss: 1.3275 - st_loss: 0.4756 - hs_loss: 0.4304 - val_loss: 1.4811 - val_st_loss: 0.6088 - val_hs_loss: 0.5373\n",
      "— val_f1: 0.7055557507818815\n",
      "Epoch 113/200\n",
      "49/49 - 1s - loss: 1.2901 - st_loss: 0.4379 - hs_loss: 0.4414 - val_loss: 1.4704 - val_st_loss: 0.6036 - val_hs_loss: 0.5195\n",
      "— val_f1: 0.7104406781132837\n",
      "Epoch 114/200\n",
      "49/49 - 1s - loss: 1.2956 - st_loss: 0.4477 - hs_loss: 0.4299 - val_loss: 1.4465 - val_st_loss: 0.5812 - val_hs_loss: 0.5222\n",
      "— val_f1: 0.6729737695254937\n",
      "Epoch 115/200\n",
      "49/49 - 1s - loss: 1.2986 - st_loss: 0.4507 - hs_loss: 0.4397 - val_loss: 1.5162 - val_st_loss: 0.6531 - val_hs_loss: 0.5209\n",
      "— val_f1: 0.6053265751750824\n",
      "Epoch 116/200\n",
      "49/49 - 1s - loss: 1.2842 - st_loss: 0.4381 - hs_loss: 0.4403 - val_loss: 1.7607 - val_st_loss: 0.8927 - val_hs_loss: 0.5551\n",
      "— val_f1: 0.576267942583732\n",
      "Epoch 117/200\n",
      "49/49 - 1s - loss: 1.2848 - st_loss: 0.4433 - hs_loss: 0.4269 - val_loss: 1.4632 - val_st_loss: 0.6037 - val_hs_loss: 0.5219\n",
      "— val_f1: 0.6958960328317374\n",
      "Epoch 118/200\n",
      "49/49 - 1s - loss: 1.2908 - st_loss: 0.4495 - hs_loss: 0.4355 - val_loss: 1.4985 - val_st_loss: 0.6277 - val_hs_loss: 0.5875\n",
      "— val_f1: 0.6921539703903097\n",
      "Epoch 119/200\n",
      "49/49 - 1s - loss: 1.2777 - st_loss: 0.4397 - hs_loss: 0.4286 - val_loss: 1.5776 - val_st_loss: 0.7212 - val_hs_loss: 0.5253\n",
      "— val_f1: 0.6755652688984661\n",
      "Epoch 120/200\n",
      "49/49 - 1s - loss: 1.2711 - st_loss: 0.4366 - hs_loss: 0.4206 - val_loss: 1.5233 - val_st_loss: 0.6469 - val_hs_loss: 0.6352\n",
      "— val_f1: 0.6885958256297356\n",
      "Epoch 121/200\n",
      "49/49 - 1s - loss: 1.3351 - st_loss: 0.5005 - hs_loss: 0.4304 - val_loss: 1.8821 - val_st_loss: 0.9937 - val_hs_loss: 0.7046\n",
      "— val_f1: 0.4646045210285562\n",
      "Epoch 122/200\n",
      "49/49 - 1s - loss: 1.2554 - st_loss: 0.4231 - hs_loss: 0.4288 - val_loss: 1.4856 - val_st_loss: 0.5895 - val_hs_loss: 0.7527\n",
      "— val_f1: 0.6561493628371036\n",
      "Epoch 123/200\n",
      "49/49 - 1s - loss: 1.2599 - st_loss: 0.4302 - hs_loss: 0.4251 - val_loss: 1.5478 - val_st_loss: 0.6586 - val_hs_loss: 0.7272\n",
      "— val_f1: 0.5764042848558937\n",
      "Epoch 124/200\n",
      "49/49 - 1s - loss: 1.2373 - st_loss: 0.4098 - hs_loss: 0.4236 - val_loss: 1.4865 - val_st_loss: 0.6397 - val_hs_loss: 0.5247\n",
      "— val_f1: 0.6842335447190029\n",
      "Epoch 125/200\n",
      "49/49 - 1s - loss: 1.2470 - st_loss: 0.4206 - hs_loss: 0.4271 - val_loss: 1.9037 - val_st_loss: 1.0298 - val_hs_loss: 0.6697\n",
      "— val_f1: 0.5354989787555129\n",
      "Epoch 126/200\n",
      "49/49 - 1s - loss: 1.2980 - st_loss: 0.4707 - hs_loss: 0.4412 - val_loss: 1.6506 - val_st_loss: 0.7881 - val_hs_loss: 0.6222\n",
      "— val_f1: 0.5395136239621103\n",
      "Epoch 127/200\n",
      "49/49 - 1s - loss: 1.2437 - st_loss: 0.4220 - hs_loss: 0.4225 - val_loss: 2.2897 - val_st_loss: 1.2740 - val_hs_loss: 1.3974\n",
      "— val_f1: 0.35839135432217833\n",
      "Epoch 128/200\n",
      "49/49 - 1s - loss: 1.2512 - st_loss: 0.4302 - hs_loss: 0.4281 - val_loss: 1.4721 - val_st_loss: 0.6247 - val_hs_loss: 0.5651\n",
      "— val_f1: 0.6812018339674178\n",
      "Epoch 129/200\n",
      "49/49 - 1s - loss: 1.2615 - st_loss: 0.4404 - hs_loss: 0.4384 - val_loss: 1.4617 - val_st_loss: 0.5938 - val_hs_loss: 0.6773\n",
      "— val_f1: 0.7046556367833783\n",
      "Epoch 130/200\n",
      "49/49 - 1s - loss: 1.2698 - st_loss: 0.4547 - hs_loss: 0.4174 - val_loss: 1.6070 - val_st_loss: 0.7662 - val_hs_loss: 0.5508\n",
      "— val_f1: 0.6419523021838816\n",
      "Epoch 131/200\n",
      "49/49 - 1s - loss: 1.1892 - st_loss: 0.3768 - hs_loss: 0.4134 - val_loss: 1.6133 - val_st_loss: 0.7335 - val_hs_loss: 0.7550\n",
      "— val_f1: 0.5690899489526492\n",
      "Epoch 132/200\n",
      "49/49 - 1s - loss: 1.2239 - st_loss: 0.4127 - hs_loss: 0.4163 - val_loss: 1.4535 - val_st_loss: 0.6198 - val_hs_loss: 0.5339\n",
      "— val_f1: 0.7084018479367317\n",
      "Epoch 133/200\n",
      "49/49 - 1s - loss: 1.2568 - st_loss: 0.4432 - hs_loss: 0.4376 - val_loss: 1.5072 - val_st_loss: 0.6245 - val_hs_loss: 0.7879\n",
      "— val_f1: 0.7101139984209315\n",
      "Epoch 134/200\n",
      "49/49 - 1s - loss: 1.2060 - st_loss: 0.3978 - hs_loss: 0.4200 - val_loss: 1.7375 - val_st_loss: 0.7973 - val_hs_loss: 1.0842\n",
      "— val_f1: 0.5690899489526492\n",
      "Epoch 135/200\n",
      "49/49 - 1s - loss: 1.1936 - st_loss: 0.3898 - hs_loss: 0.4070 - val_loss: 1.5128 - val_st_loss: 0.6518 - val_hs_loss: 0.6975\n",
      "— val_f1: 0.6829831402906311\n",
      "Epoch 136/200\n",
      "49/49 - 1s - loss: 1.2054 - st_loss: 0.4018 - hs_loss: 0.4151 - val_loss: 1.4669 - val_st_loss: 0.6402 - val_hs_loss: 0.5352\n",
      "— val_f1: 0.68000870369363\n",
      "Epoch 137/200\n",
      "49/49 - 1s - loss: 1.1855 - st_loss: 0.3846 - hs_loss: 0.4104 - val_loss: 2.0057 - val_st_loss: 1.1541 - val_hs_loss: 0.6685\n",
      "— val_f1: 0.5931479541172752\n",
      "Epoch 138/200\n",
      "49/49 - 1s - loss: 1.1988 - st_loss: 0.3979 - hs_loss: 0.4195 - val_loss: 2.0711 - val_st_loss: 1.2171 - val_hs_loss: 0.6898\n",
      "— val_f1: 0.5977393020543644\n",
      "Epoch 139/200\n",
      "49/49 - 1s - loss: 1.1811 - st_loss: 0.3845 - hs_loss: 0.4070 - val_loss: 2.7479 - val_st_loss: 1.9198 - val_hs_loss: 0.5693\n",
      "— val_f1: 0.4355452632300325\n",
      "Epoch 140/200\n",
      "49/49 - 1s - loss: 1.1855 - st_loss: 0.3916 - hs_loss: 0.4026 - val_loss: 1.4807 - val_st_loss: 0.6499 - val_hs_loss: 0.5922\n",
      "— val_f1: 0.5236983888945663\n",
      "Epoch 141/200\n",
      "49/49 - 1s - loss: 1.2283 - st_loss: 0.4378 - hs_loss: 0.3944 - val_loss: 2.0762 - val_st_loss: 1.2390 - val_hs_loss: 0.6331\n",
      "— val_f1: 0.3911641064960474\n",
      "Epoch 142/200\n",
      "49/49 - 1s - loss: 1.1836 - st_loss: 0.3918 - hs_loss: 0.4099 - val_loss: 2.2937 - val_st_loss: 1.3761 - val_hs_loss: 1.0437\n",
      "— val_f1: 0.5148377840886279\n",
      "Epoch 143/200\n",
      "49/49 - 1s - loss: 1.1687 - st_loss: 0.3797 - hs_loss: 0.4051 - val_loss: 2.1902 - val_st_loss: 1.3747 - val_hs_loss: 0.5415\n",
      "— val_f1: 0.5376629477392836\n",
      "Epoch 144/200\n",
      "49/49 - 1s - loss: 1.1735 - st_loss: 0.3891 - hs_loss: 0.3907 - val_loss: 1.5408 - val_st_loss: 0.7000 - val_hs_loss: 0.6772\n",
      "— val_f1: 0.4310016561524166\n",
      "Epoch 145/200\n",
      "49/49 - 1s - loss: 1.1281 - st_loss: 0.3428 - hs_loss: 0.4037 - val_loss: 2.4722 - val_st_loss: 1.5501 - val_hs_loss: 1.0925\n",
      "— val_f1: 0.3667999158426257\n",
      "Epoch 146/200\n",
      "49/49 - 1s - loss: 1.1549 - st_loss: 0.3704 - hs_loss: 0.4091 - val_loss: 1.5519 - val_st_loss: 0.7036 - val_hs_loss: 0.7328\n",
      "— val_f1: 0.668916797488226\n",
      "Epoch 147/200\n",
      "49/49 - 1s - loss: 1.1900 - st_loss: 0.4092 - hs_loss: 0.3990 - val_loss: 1.4181 - val_st_loss: 0.5754 - val_hs_loss: 0.7130\n",
      "— val_f1: 0.6904321847793371\n",
      "Epoch 148/200\n",
      "49/49 - 1s - loss: 1.1501 - st_loss: 0.3700 - hs_loss: 0.4041 - val_loss: 1.5138 - val_st_loss: 0.6679 - val_hs_loss: 0.7381\n",
      "— val_f1: 0.6958648288128058\n",
      "Epoch 149/200\n",
      "49/49 - 1s - loss: 1.1181 - st_loss: 0.3408 - hs_loss: 0.3991 - val_loss: 1.4594 - val_st_loss: 0.6524 - val_hs_loss: 0.5519\n",
      "— val_f1: 0.7038580246913582\n",
      "Epoch 150/200\n",
      "49/49 - 1s - loss: 1.1658 - st_loss: 0.3899 - hs_loss: 0.4005 - val_loss: 1.4831 - val_st_loss: 0.6793 - val_hs_loss: 0.5448\n",
      "— val_f1: 0.6939992257065428\n",
      "Epoch 151/200\n",
      "49/49 - 1s - loss: 1.1418 - st_loss: 0.3650 - hs_loss: 0.4140 - val_loss: 1.4994 - val_st_loss: 0.6997 - val_hs_loss: 0.5331\n",
      "— val_f1: 0.7002917570614826\n",
      "Epoch 152/200\n",
      "49/49 - 1s - loss: 1.1286 - st_loss: 0.3578 - hs_loss: 0.3928 - val_loss: 1.8188 - val_st_loss: 1.0168 - val_hs_loss: 0.5533\n",
      "— val_f1: 0.6734270876938917\n",
      "Epoch 153/200\n",
      "49/49 - 1s - loss: 1.1236 - st_loss: 0.3519 - hs_loss: 0.4057 - val_loss: 1.4844 - val_st_loss: 0.6886 - val_hs_loss: 0.5308\n",
      "— val_f1: 0.7207118807118806\n",
      "Epoch 154/200\n",
      "49/49 - 1s - loss: 1.1048 - st_loss: 0.3355 - hs_loss: 0.4025 - val_loss: 1.6904 - val_st_loss: 0.8361 - val_hs_loss: 0.8319\n",
      "— val_f1: 0.5698431488606097\n",
      "Epoch 155/200\n",
      "49/49 - 1s - loss: 1.1325 - st_loss: 0.3628 - hs_loss: 0.4129 - val_loss: 3.1492 - val_st_loss: 2.3581 - val_hs_loss: 0.5245\n",
      "— val_f1: 0.4368868095930233\n",
      "Epoch 156/200\n",
      "49/49 - 1s - loss: 1.1952 - st_loss: 0.4303 - hs_loss: 0.3978 - val_loss: 1.4689 - val_st_loss: 0.6637 - val_hs_loss: 0.6033\n",
      "— val_f1: 0.6743105003967951\n",
      "Epoch 157/200\n",
      "49/49 - 1s - loss: 1.1788 - st_loss: 0.4168 - hs_loss: 0.3916 - val_loss: 1.4957 - val_st_loss: 0.7045 - val_hs_loss: 0.5421\n",
      "— val_f1: 0.672535611139935\n",
      "Epoch 158/200\n",
      "49/49 - 1s - loss: 1.1197 - st_loss: 0.3611 - hs_loss: 0.3833 - val_loss: 1.5110 - val_st_loss: 0.6617 - val_hs_loss: 0.8408\n",
      "— val_f1: 0.6794869303274704\n",
      "Epoch 159/200\n",
      "49/49 - 1s - loss: 1.0967 - st_loss: 0.3365 - hs_loss: 0.3995 - val_loss: 1.5262 - val_st_loss: 0.7398 - val_hs_loss: 0.5348\n",
      "— val_f1: 0.6774777686523055\n",
      "Epoch 160/200\n",
      "49/49 - 1s - loss: 1.0871 - st_loss: 0.3280 - hs_loss: 0.4026 - val_loss: 1.5439 - val_st_loss: 0.7544 - val_hs_loss: 0.5585\n",
      "— val_f1: 0.7077996097297088\n",
      "Epoch 161/200\n",
      "49/49 - 1s - loss: 1.1004 - st_loss: 0.3455 - hs_loss: 0.3899 - val_loss: 1.8019 - val_st_loss: 1.0113 - val_hs_loss: 0.5725\n",
      "— val_f1: 0.6753796728971964\n",
      "Epoch 162/200\n",
      "49/49 - 1s - loss: 1.0718 - st_loss: 0.3191 - hs_loss: 0.3873 - val_loss: 2.2010 - val_st_loss: 1.4118 - val_hs_loss: 0.5738\n",
      "— val_f1: 0.6373591416921311\n",
      "Epoch 163/200\n",
      "49/49 - 1s - loss: 1.1298 - st_loss: 0.3783 - hs_loss: 0.3897 - val_loss: 1.4955 - val_st_loss: 0.6601 - val_hs_loss: 0.8135\n",
      "— val_f1: 0.6993419875554141\n",
      "Epoch 164/200\n",
      "49/49 - 1s - loss: 1.0246 - st_loss: 0.2773 - hs_loss: 0.3765 - val_loss: 1.6260 - val_st_loss: 0.8286 - val_hs_loss: 0.6311\n",
      "— val_f1: 0.6956956956956957\n",
      "Epoch 165/200\n",
      "49/49 - 1s - loss: 1.0431 - st_loss: 0.2941 - hs_loss: 0.3938 - val_loss: 1.6338 - val_st_loss: 0.8508 - val_hs_loss: 0.5680\n",
      "— val_f1: 0.7039473684210527\n",
      "Epoch 166/200\n",
      "49/49 - 1s - loss: 1.0118 - st_loss: 0.2654 - hs_loss: 0.3890 - val_loss: 1.7696 - val_st_loss: 0.9848 - val_hs_loss: 0.5850\n",
      "— val_f1: 0.5319004332682015\n",
      "Epoch 167/200\n",
      "49/49 - 1s - loss: 1.0192 - st_loss: 0.2746 - hs_loss: 0.3885 - val_loss: 1.7751 - val_st_loss: 1.0009 - val_hs_loss: 0.5410\n",
      "— val_f1: 0.6962406015037594\n",
      "Epoch 168/200\n",
      "49/49 - 1s - loss: 1.0435 - st_loss: 0.3021 - hs_loss: 0.3808 - val_loss: 2.2731 - val_st_loss: 1.3603 - val_hs_loss: 1.2423\n",
      "— val_f1: 0.4433996383363472\n",
      "Epoch 169/200\n",
      "49/49 - 1s - loss: 1.0302 - st_loss: 0.2848 - hs_loss: 0.4092 - val_loss: 2.0611 - val_st_loss: 1.2910 - val_hs_loss: 0.5372\n",
      "— val_f1: 0.6840303252629005\n",
      "Epoch 170/200\n",
      "49/49 - 1s - loss: 1.0183 - st_loss: 0.2773 - hs_loss: 0.3952 - val_loss: 2.1170 - val_st_loss: 1.3472 - val_hs_loss: 0.5438\n",
      "— val_f1: 0.655328798185941\n",
      "Epoch 171/200\n",
      "49/49 - 1s - loss: 1.0204 - st_loss: 0.2827 - hs_loss: 0.3870 - val_loss: 2.9410 - val_st_loss: 2.1197 - val_hs_loss: 0.8090\n",
      "— val_f1: 0.3206373882627283\n",
      "Epoch 172/200\n",
      "49/49 - 1s - loss: 1.1243 - st_loss: 0.3880 - hs_loss: 0.3887 - val_loss: 1.5214 - val_st_loss: 0.7393 - val_hs_loss: 0.6213\n",
      "— val_f1: 0.6647645720287064\n",
      "Epoch 173/200\n",
      "49/49 - 1s - loss: 0.9951 - st_loss: 0.2625 - hs_loss: 0.3781 - val_loss: 1.7595 - val_st_loss: 0.9582 - val_hs_loss: 0.7257\n",
      "— val_f1: 0.7091981086266395\n",
      "Epoch 174/200\n",
      "49/49 - 1s - loss: 1.1217 - st_loss: 0.3890 - hs_loss: 0.3867 - val_loss: 1.6550 - val_st_loss: 0.8961 - val_hs_loss: 0.5223\n",
      "— val_f1: 0.3949665649732958\n",
      "Epoch 175/200\n",
      "49/49 - 1s - loss: 1.0217 - st_loss: 0.2938 - hs_loss: 0.3707 - val_loss: 1.6533 - val_st_loss: 0.8908 - val_hs_loss: 0.5481\n",
      "— val_f1: 0.6981051670344722\n",
      "Epoch 176/200\n",
      "49/49 - 1s - loss: 1.0263 - st_loss: 0.2995 - hs_loss: 0.3739 - val_loss: 1.6649 - val_st_loss: 0.8219 - val_hs_loss: 0.9586\n",
      "— val_f1: 0.6869646311135673\n",
      "Epoch 177/200\n",
      "49/49 - 1s - loss: 1.0387 - st_loss: 0.3101 - hs_loss: 0.3909 - val_loss: 1.6551 - val_st_loss: 0.8981 - val_hs_loss: 0.5372\n",
      "— val_f1: 0.7054847015664425\n",
      "Epoch 178/200\n",
      "49/49 - 1s - loss: 0.9973 - st_loss: 0.2767 - hs_loss: 0.3589 - val_loss: 5.2491 - val_st_loss: 4.4296 - val_hs_loss: 0.8577\n",
      "— val_f1: 0.3845563832141014\n",
      "Epoch 179/200\n",
      "49/49 - 1s - loss: 1.2890 - st_loss: 0.5639 - hs_loss: 0.3891 - val_loss: 1.6444 - val_st_loss: 0.8245 - val_hs_loss: 0.8670\n",
      "— val_f1: 0.5168565584209268\n",
      "Epoch 180/200\n",
      "49/49 - 1s - loss: 1.0328 - st_loss: 0.3122 - hs_loss: 0.3740 - val_loss: 2.1547 - val_st_loss: 1.3334 - val_hs_loss: 0.8816\n",
      "— val_f1: 0.5646417842048984\n",
      "Epoch 181/200\n",
      "49/49 - 1s - loss: 0.9702 - st_loss: 0.2520 - hs_loss: 0.3697 - val_loss: 2.3909 - val_st_loss: 1.5741 - val_hs_loss: 0.8670\n",
      "— val_f1: 0.5574134176903168\n",
      "Epoch 182/200\n",
      "49/49 - 1s - loss: 0.9933 - st_loss: 0.2725 - hs_loss: 0.3908 - val_loss: 1.9079 - val_st_loss: 1.1265 - val_hs_loss: 0.6980\n",
      "— val_f1: 0.6883514520817575\n",
      "Epoch 183/200\n",
      "49/49 - 1s - loss: 0.9838 - st_loss: 0.2667 - hs_loss: 0.3799 - val_loss: 1.8012 - val_st_loss: 1.0556 - val_hs_loss: 0.5266\n",
      "— val_f1: 0.6629445400027894\n",
      "Epoch 184/200\n",
      "49/49 - 1s - loss: 0.9548 - st_loss: 0.2411 - hs_loss: 0.3710 - val_loss: 1.5148 - val_st_loss: 0.7683 - val_hs_loss: 0.5389\n",
      "— val_f1: 0.7178211790154991\n",
      "Epoch 185/200\n",
      "49/49 - 1s - loss: 0.9657 - st_loss: 0.2530 - hs_loss: 0.3740 - val_loss: 1.5668 - val_st_loss: 0.8172 - val_hs_loss: 0.5627\n",
      "— val_f1: 0.6466536562966365\n",
      "Epoch 186/200\n",
      "49/49 - 1s - loss: 0.9076 - st_loss: 0.1985 - hs_loss: 0.3643 - val_loss: 1.8370 - val_st_loss: 1.0844 - val_hs_loss: 0.5857\n",
      "— val_f1: 0.693233082706767\n",
      "Epoch 187/200\n",
      "49/49 - 1s - loss: 1.0038 - st_loss: 0.2940 - hs_loss: 0.3754 - val_loss: 1.8617 - val_st_loss: 1.0902 - val_hs_loss: 0.6885\n",
      "— val_f1: 0.6932405281285878\n",
      "Epoch 188/200\n",
      "49/49 - 1s - loss: 0.9348 - st_loss: 0.2280 - hs_loss: 0.3684 - val_loss: 1.8676 - val_st_loss: 1.1187 - val_hs_loss: 0.5833\n",
      "— val_f1: 0.7028409938651383\n",
      "Epoch 189/200\n",
      "49/49 - 1s - loss: 1.0762 - st_loss: 0.3684 - hs_loss: 0.3813 - val_loss: 1.6006 - val_st_loss: 0.8449 - val_hs_loss: 0.6250\n",
      "— val_f1: 0.7084834206273487\n",
      "Epoch 190/200\n",
      "49/49 - 1s - loss: 0.9578 - st_loss: 0.2531 - hs_loss: 0.3740 - val_loss: 1.6921 - val_st_loss: 0.9507 - val_hs_loss: 0.5611\n",
      "— val_f1: 0.6137132211489784\n",
      "Epoch 191/200\n",
      "49/49 - 1s - loss: 0.9168 - st_loss: 0.2170 - hs_loss: 0.3571 - val_loss: 1.6102 - val_st_loss: 0.8771 - val_hs_loss: 0.5275\n",
      "— val_f1: 0.7088512688655294\n",
      "Epoch 192/200\n",
      "49/49 - 1s - loss: 0.8852 - st_loss: 0.1843 - hs_loss: 0.3702 - val_loss: 2.1946 - val_st_loss: 1.4567 - val_hs_loss: 0.5592\n",
      "— val_f1: 0.6723523898781631\n",
      "Epoch 193/200\n",
      "49/49 - 1s - loss: 0.9519 - st_loss: 0.2504 - hs_loss: 0.3811 - val_loss: 1.7291 - val_st_loss: 0.9355 - val_hs_loss: 0.8455\n",
      "— val_f1: 0.7032168264769564\n",
      "Epoch 194/200\n",
      "49/49 - 1s - loss: 0.9651 - st_loss: 0.2692 - hs_loss: 0.3609 - val_loss: 1.9412 - val_st_loss: 1.1923 - val_hs_loss: 0.6298\n",
      "— val_f1: 0.6995903027389627\n",
      "Epoch 195/200\n",
      "49/49 - 1s - loss: 0.9831 - st_loss: 0.2894 - hs_loss: 0.3581 - val_loss: 4.0186 - val_st_loss: 3.2522 - val_hs_loss: 0.7256\n",
      "— val_f1: 0.4390461862153999\n",
      "Epoch 196/200\n",
      "49/49 - 1s - loss: 1.0576 - st_loss: 0.3660 - hs_loss: 0.3549 - val_loss: 1.7760 - val_st_loss: 1.0198 - val_hs_loss: 0.6816\n",
      "— val_f1: 0.6919544695919395\n",
      "Epoch 197/200\n",
      "49/49 - 1s - loss: 0.9319 - st_loss: 0.2394 - hs_loss: 0.3667 - val_loss: 1.6489 - val_st_loss: 0.9145 - val_hs_loss: 0.5805\n",
      "— val_f1: 0.7001071504190843\n",
      "Epoch 198/200\n",
      "49/49 - 1s - loss: 0.8843 - st_loss: 0.1960 - hs_loss: 0.3533 - val_loss: 1.7782 - val_st_loss: 0.9471 - val_hs_loss: 1.0718\n",
      "— val_f1: 0.7032005147157842\n",
      "Epoch 199/200\n",
      "49/49 - 1s - loss: 0.9190 - st_loss: 0.2290 - hs_loss: 0.3697 - val_loss: 3.4269 - val_st_loss: 2.6832 - val_hs_loss: 0.6420\n",
      "— val_f1: 0.518483231707317\n",
      "Epoch 200/200\n",
      "49/49 - 1s - loss: 1.0150 - st_loss: 0.3246 - hs_loss: 0.3794 - val_loss: 1.6489 - val_st_loss: 0.9273 - val_hs_loss: 0.5398\n",
      "— val_f1: 0.3176402522932679\n",
      "Restoring model weights from the end of the best epoch.\n"
     ]
    }
   ],
   "source": [
    "input_train = {\"text\": X_train, \"extra\": X_train_extra}\n",
    "input_val   = {\"text\": X_val, \"extra\": X_val_extra}\n",
    "input_test   = {\"text\": X_test, \"extra\": X_test_extra}\n",
    "output_train = {\"hs\": y_hs_train, \"st\": y_st_train}\n",
    "output_val   = {\"hs\": y_hs_val, \"st\": y_st_val}\n",
    "output_test   = {\"hs\": y_hs_test, \"st\": y_st_test}\n",
    "\n",
    "model_cnn.compile(\n",
    "    loss={\n",
    "        \"hs\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        \"st\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    },\n",
    "    loss_weights=[1.0, 0.2],\n",
    "    #optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=False),\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.007, momentum=0, nesterov=False),\n",
    "    #metrics=[f1_macro]\n",
    ")\n",
    "#mc = ModelCheckpoint('best_model.h5', monitor='val_f1_macro', mode='max', save_best_only=True, verbose=1)\n",
    "#es = EarlyStopping(monitor=\"val_f1_macro\", min_delta=0, patience=200, verbose=1, mode=\"max\", restore_best_weights=False)\n",
    "f1_callback = FCallback(validation = (input_val, output_val[\"st\"]), verbose=True)                                   \n",
    "best_callback = ReturnBestEarlyStopping(monitor=\"val_f1\", min_delta=0, patience=100, verbose=1, mode=\"max\", restore_best_weights=True)\n",
    "\n",
    "history = model_cnn.fit(input_train, output_train, batch_size=128, epochs=200, validation_data=(input_val, output_val), callbacks=[f1_callback, best_callback], verbose = 2)\n",
    "#model_cnn = load_model('best_model.h5', custom_objects={'f1_macro': f1_macro})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zXBE_F-y25e",
    "outputId": "ccc1b7a2-6dd8-44eb-dccc-edf4f8caa04c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7207118807118806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.72       370\n",
      "           1       0.67      0.77      0.72       314\n",
      "\n",
      "    accuracy                           0.72       684\n",
      "   macro avg       0.72      0.72      0.72       684\n",
      "weighted avg       0.73      0.72      0.72       684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = np.where(model_cnn.predict(input_val)[0] >0.5,1,0)\n",
    "print(f1_score(y_st_val, y_val_pred, average=\"macro\"))\n",
    "print(classification_report(y_st_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFKnxmfaQwpp",
    "outputId": "c934cd4d-63b0-4d03-c356-bf50d10ddfe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1263,)\n",
      "0.6270393376073187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.43      0.57       694\n",
      "           1       0.56      0.88      0.69       569\n",
      "\n",
      "    accuracy                           0.64      1263\n",
      "   macro avg       0.69      0.66      0.63      1263\n",
      "weighted avg       0.70      0.64      0.62      1263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_test   = {\"text\": X_test, \"extra\": X_test_extra}\n",
    "y_test_pred = np.where(model_cnn.predict(input_test)[0] > 0.5, 1, 0)\n",
    "print(y_st_test.shape)\n",
    "print(f1_score(y_st_test, y_test_pred,average=\"macro\"))\n",
    "print(classification_report(y_st_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tCRl-ykzgoW"
   },
   "source": [
    "|        \t|        \t|              \t|    0   \t|        \t|        \t|    1   \t|        \t|        \t|          \t|\n",
    "|--------\t|--------\t|--------------\t|:------:\t|:------:\t|:------:\t|:------:\t|:------:\t|:------:\t|:--------:\t|\n",
    "|        \t|        \t|              \t|    P   \t|    R   \t|   F1   \t|    P   \t|    R   \t|   F1   \t| **Macro-F1** \t|\n",
    "| **TASK B** \t|  **News**  \t| Baseline_MFC \t|   0,65 \t|      1 \t| 0,7878 \t|      0 \t|      0 \t|      0 \t|   0,3939 \t|\n",
    "|        \t|        \t| Baseline_SVC \t| 0,7467 \t| 0,8707 \t| 0,8039 \t| 0,6528 \t| 0,4514 \t| 0,5337 \t|   0,6688 \t|\n",
    "|        \t| **Tweets** \t| Baseline_MFC \t| 0,5494 \t|      1 \t| 0,7092 \t|      0 \t|      0 \t|      0 \t|   0,3546 \t|\n",
    "|        \t|        \t| Baseline_SVC \t| 0,7869 \t| 0,6599 \t| 0,7178 \t| 0,6534 \t|  0,782 \t|  0,712 \t|   0,7149 \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "xswAsFr4DfOv",
    "outputId": "705b6500-0bee-4bc4-9f39-b6b8ff07c066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'st_loss', 'hs_loss', 'val_loss', 'val_st_loss', 'val_hs_loss', 'val_f1'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3xb1d3/30fDlveKncRxhpNANhASIBD2XgVaRimFp3RBF9CnhZb21/m0T5+2dAMd0FIKFCizhUIgQNkJCVlAQva0s723rXF+f5x7dK9kyZbkIY/zfr38kixf6VzJup/7vZ/v93yPkFJiMBgMhpGHK907YDAYDIaBwQi8wWAwjFCMwBsMBsMIxQi8wWAwjFCMwBsMBsMIxQi8wWAwjFCMwBsMgBDifiHEjxPcdpcQ4uy+vo7BMNAYgTcYDIYRihF4g8FgGKEYgTcMGyxr5DYhxPtCiFYhxF+EEGOFEEuEEM1CiJeFEEWO7S8RQmwQQjQIIV4TQsxy/G2+EGKN9bx/AL6osS4WQqyznrtMCHFUivv8eSHENiFEnRDiGSFEufW4EEL8WghxSAjRJIT4QAgx1/rbhUKID6192yuEuDWlD8ww6jECbxhuXA6cAxwJfARYAnwbKEV9n28GEEIcCTwCfNX62/PAs0KIDCFEBvBP4EGgGHjcel2s584H7gNuBEqAPwHPCCEyk9lRIcSZwP8BVwHjgd3Ao9afzwVOtd5HgbVNrfW3vwA3SinzgLnAf5IZ12DQGIE3DDfulFIelFLuBd4EVkgp10opO4CngfnWdh8HnpNSviSl9AO/ALKAk4BFgBf4jZTSL6V8AnjXMcYNwJ+klCuklEEp5d+ATut5yfBJ4D4p5RopZSfwLeBEIcQUwA/kATMBIaXcKKXcbz3PD8wWQuRLKeullGuSHNdgAIzAG4YfBx3322P8nmvdL0dFzABIKUNAFTDB+tteGdlpb7fj/mTg65Y90yCEaAAmWs9Lhuh9aEFF6ROklP8B7gLuBg4JIe4RQuRbm14OXAjsFkK8LoQ4MclxDQbACLxh5LIPJdSA8rxRIr0X2A9MsB7TTHLcrwL+V0pZ6PjJllI+0sd9yEFZPnsBpJS/k1IuAGajrJrbrMfflVJeCpShrKTHkhzXYACMwBtGLo8BFwkhzhJCeIGvo2yWZcByIADcLITwCiE+BhzveO69wBeEECdYydAcIcRFQoi8JPfhEeDTQohjLP/+JyhLaZcQ4jjr9b1AK9ABhKwcwSeFEAWWtdQEhPrwORhGMUbgDSMSKeVm4FrgTqAGlZD9iJSyS0rZBXwMuB6oQ/n1Tzmeuwr4PMpCqQe2Wdsmuw8vA98FnkRdNUwDrrb+nI86kdSjbJxa4A7rb9cBu4QQTcAXUF6+wZA0wiz4YTAYDCMTE8EbDAbDCMUIvMFgMIxQjMAbDAbDCMUIvMFgMIxQPOneASdjxoyRU6ZMSfduGAwGw7Bh9erVNVLK0lh/G1ICP2XKFFatWpXu3TAYDIZhgxBid7y/GYvGYDAYRihG4A0Gg2GEYgTeYDAYRihDyoOPhd/vp7q6mo6OjnTvyoDi8/moqKjA6/Wme1cMBsMIYcgLfHV1NXl5eUyZMoXI5n8jBykltbW1VFdXU1lZme7dMRgMI4Qhb9F0dHRQUlIyYsUdQAhBSUnJiL9KMRgMg8uQF3hgRIu7ZjS8R4PBMLgMC4E3GAxpYPdyOLQx3Xth6ANG4HuhoaGB3//+90k/78ILL6ShoWEA9shgGCSe+zq8/vN074WhDxiB74V4Ah8IBHp83vPPP09hYeFA7ZbBMPAEOyHYle69MPSBIV9Fk25uv/12tm/fzjHHHIPX68Xn81FUVMSmTZvYsmULl112GVVVVXR0dHDLLbdwww03AHbbhZaWFi644AJOPvlkli1bxoQJE/jXv/5FVlZWmt+ZwdALMgRmQaBhzbAS+B8+u4EP9zX162vOLs/n+x+ZE/fvP/3pT1m/fj3r1q3jtdde46KLLmL9+vXhcsb77ruP4uJi2tvbOe6447j88sspKSmJeI2tW7fyyCOPcO+993LVVVfx5JNPcu211/br+zAY+p1QUIm8YdgyrAR+KHD88cdH1Kr/7ne/4+mnnwagqqqKrVu3dhP4yspKjjnmGAAWLFjArl27Bm1/DYaUkRJkMN17YegDw0rge4q0B4ucnJzw/ddee42XX36Z5cuXk52dzemnnx6zlj0zMzN83+12097ePij7ajD0CWki+OGOSbL2Ql5eHs3NzTH/1tjYSFFREdnZ2WzatIl33nlnkPfOYBhAZEjZNIZhy7CK4NNBSUkJixcvZu7cuWRlZTF27Njw384//3z++Mc/MmvWLGbMmMGiRYvSuKcGQz8jQyaCH+YYgU+Ahx9+OObjmZmZLFmyJObftM8+ZswY1q9fH3781ltv7ff9MxgGBJNkHfYYi8ZgMMTGRPDDHiPwBoMhNibJOuwxAm8wGGIjpRH4Yc6AevBCiF1AMxAEAlLKhQM5nsFg6EdCQVNFM8wZjCTrGVLKmkEYx2Aw9CfGgx/2GIvGYDDERobMTNZhzkALvASWCiFWCyFuiLWBEOIGIcQqIcSqw4cPD/DuJE+q7YIBfvOb39DW1tbPe2QwDBImyTrsGWiBP1lKeSxwAfBlIcSp0RtIKe+RUi6UUi4sLS0d4N1JHiPwhlGL6SY57BlQD15Kude6PSSEeBo4HnhjIMfsb5ztgs855xzKysp47LHH6Ozs5KMf/Sg//OEPaW1t5aqrrqK6uppgMMh3v/tdDh48yL59+zjjjDMYM2YMr776arrfisGQOLqCxiRZhzUDJvBCiBzAJaVstu6fC/xPn150ye1w4IP+2D2bcfPggp/G/bOzXfDSpUt54oknWLlyJVJKLrnkEt544w0OHz5MeXk5zz33HKB61BQUFPCrX/2KV199lTFjxvTvPhsMA42O3I1FM6wZSItmLPCWEOI9YCXwnJTyhQEcb8BZunQpS5cuZf78+Rx77LFs2rSJrVu3Mm/ePF566SW++c1v8uabb1JQUJDuXTUY+oZOro4WgW+thd/Mg4MfpntP+pUBi+CllDuAo/v1RXuItAcDKSXf+ta3uPHGG7v9bc2aNTz//PN85zvf4ayzzuJ73/teGvbQYOgntLCPliqaxipo2AO1W2Hs7HTvTb9hyiR7wdku+LzzzuO+++6jpaUFgL1793Lo0CH27dtHdnY21157Lbfddhtr1qzp9lyDYVgRGmUR/Ai9YjHdJHvB2S74ggsu4JprruHEE08EIDc3l4ceeoht27Zx22234XK58Hq9/OEPfwDghhtu4Pzzz6e8vNwkWQ3Di3AEP7IELy465zDCkspG4BMgul3wLbfcEvH7tGnTOO+887o976abbuKmm24a0H0zGAYELeyhUSLwI/SKxVg0BoOhOyPUsojLCH2/RuANBkN3wmWSabAsgn54+3cQ6Bq8MXUEP8IsmmEh8HIUzKYbDe/RMIxIp2VRtRJe+i5UDeIaxyO0amjIC7zP56O2tnZEC6CUktraWnw+X7p3xWBQpDPJGvKr26B/8MYcoRbNkE+yVlRUUF1dzVBsRNaf+Hw+Kioq0r0bBoNCptGyCJ9cBjGo08nkEWbRDHmB93q9VFZWpns3DIbRRTpEVhNKg10yQiP4IW/RGAyGNJBOiyYdVw8jtO7fCLzBYOhOOMmaBssiHWObKhqDwTBqSGsEnwY/3Fg0BoNh1JAOkQ2PnQaxTecVywBiBN5gMHQnrWWSaRD4dJ7QBhAj8AaDoTvpnOiUFovGJFkNBsNoISx0cvBLJdMxq9Q0GzMYDKMGp7gOtuilxaIxAm8wGEYLTqEbbNFLRx28KZM0GAyjBqeoD7bopaOixTQbMxgMo4ZQOiN4UwffXxiBNxgM3YmwaAY5qk1LHbwpkzQYDKOFtCZZ01CyaCJ4g8EwahhtSVZTB28wGEYNTnEd7IW301kHbywag8Ew4klnBJ8OsTUWjcFgGDUMBYvGNBvrM0bgDQZDd9JZRZOWmawycuwRghF4g8HQnbRG8GkQW2PRGAyGUUNEkjVddfCm2VhfMQJvMBi6M1qTrMaiMRgMI550TnRKh11i6uANBsOoIa0efDr7wZsIPimEEG4hxFohxL8HeiyDwdBPhNIYwYctGhPB95XBiOBvATYOwjgGg6G/cK7ilIroNVTB1pdSHNvMZO0vBlTghRAVwEXAnwdyHIPB0M/0tR/8qr/AY59Kbey0JFlNBJ8KvwG+AcT91IQQNwghVgkhVh0+fHiAd8dgMCREX5Os/g4ItKc4tukm2V8MmMALIS4GDkkpV/e0nZTyHinlQinlwtLS0oHaHYPBkAx9TbKG/Op5qUTh6ayDNxZNwiwGLhFC7AIeBc4UQjw0gOMZDIb+IiLJmoLoBf2Rt6mMnZaZrEbgE0JK+S0pZYWUcgpwNfAfKeW1AzWewWDoRyIieBl/u3iEAuo22JXC2KYOvr8wdfAGg6E7zkg2lUhaR+5a6JMaOw1iO0KX7PMMxiBSyteA1wZjLIPB0A/0tUwypC2aFCL4dIitSbIaDIZRQ1+TrH3x4E2zsX7DCLzBYOhOX5OsffHg01kHP8IsGiPwBoOhO2mN4NMwk9VYNAaDYdTQ14lO2oMP9cWiSaF6J1VMszGDwTBqiGhVkEoEbyyaoYAReIPB0J2+dpMMV9H0pUzS9KLpK0bgDQZDdyLKJPsykzWViU5piKZNFY3BYBg19LkXTT9YNKbZWJ8xAm8wGLrT1yRrn2aymmZj/YUReIPB0J2+9oPv00zWdK7oZATeYDCMdPqaZA1X0QyXOniTZDUYDKOF/ugHD30T+HQkWY1FYzAYRjz95cGbJGtaMQJvMBi601/dJPs0k9VYNH3FCLzBYOiO06pIqR98P3jwyY678d/wp1NTS84ai8ZgMIwa0unBp2rRHNwA+9+DYGfyY5ol+wwGw6ih37pJOjz4R66BD55IYOwUBT7eSaWjCd79c8/Ny0LGojEYDKOFiCRrklFtKAhYYuoU2+2vQPWqBJ6fokWjJ1VFT67avASe+zrU74z/XNNszGAwjBpkCBCO+0ngFHVnkjUUSGxma6p18PFmz2rLpie7KB0tigcBI/AGg6E7oSC4vep+sqLnFHVt0UhpCXwCnrxMMeEZL4JPZPER0w/eYDCMGqQElyXwyQqtU0h1NU3YAkkggk9VbENxKnfCFTI9jJ3qSWWIYwTeYDB0RwbB7bHuJ5vsdAipjuDD9kkCApqqXRLPogkl0PjM1MEbDIZRgwyBSwt8XyJ4S+DjRdfxxoYULJo4kXo86yZiG9NszGAwjBZkyLZoUi1XhO7impBFk6LYxovUE2ldbCwag8EwaohIsiZbRRPDoknEB9f0NcnazYNP4OohPJYcUZU0RuANBkN3nBZN0kLrtGiietIMZJI1ns8fvnro4fX6OrFriGIE3mAwdEf2oUwyGEvgk7BoUk14hseIitSjTzIxx+xj98whihF4g8HQHSkdSdY+ePApJVlTXNEp3kkkmSQrjCgf3gi8IXHe+AW8/IN074VhMAgFweVW95O2ShxCGoqyTRIRz1STrPEmNCUi8H1pzTCEMQJvSJydr8P2V9O9F4bBQIZAuEG4+hjBR4nuoMxkjXpeeB96q4NPsTXDEMYIvCFxgv7EPFTD8EdaEXwqAq8F1eMbGh58QhaNI+cw2iwaIcQtQoh8ofiLEGKNEOLcgd45wxAj2JXaEmyG4YcMKXEXrtQjaW9Wdw9+UKpo4nnwvSRZU637H8IkGsF/RkrZBJwLFAHXAT/t6QlCCJ8QYqUQ4j0hxAYhxA/7uK+GdBPsSm0BB8PwIyzw7tQjaW9O99LFnmyS8NiOfvDJVPDES+QmMtGpL3X/QxhPgttZ5hQXAg9KKTcIIURPTwA6gTOllC1CCC/wlhBiiZTynVR31pBmgn4j8KOFUDB1D15/R7xZ3evfk4ngwc4FJEI8D763OngpATl6LRpgtRBiKUrgXxRC5AE9/telosX61Wv9jJwpYqMRY9GMHqRU4u5KJYK3RD0j22HRJJhk1WKbyiSruB58L+2C9ftzZ0T+PgJIVOA/C9wOHCelbEOJ9ad7e5IQwi2EWAccAl6SUq6Isc0NQohVQohVhw8fTmLXDYNOMMF+3obhjwyCywVCpN6qwJtj3080gtdjpeKHx13wo5ex9Ukk1eZqQ5hEBf5EYLOUskEIcS3wHaCxtydJKYNSymOACuB4IcTcGNvcI6VcKKVcWFpamsy+GwYb48GPHvqUZHVYNN2SrL28lv57OJpOIYKPWwcfL4LXY45ei+YPQJsQ4mjg68B24IFEB5FSNgCvAucnvYeGoYMR+NFD2INPwaIJxrJogpF/i0dfxDbuTNZeetEbi4aAlFIClwJ3SSnvBvJ6eoIQolQIUWjdzwLOATb1ZWcNaSboVwfsCOq2Z4iDM4JPuYomu3tUnahFE65oSULg+82iGTkCn2gVTbMQ4luo8shThBAulA/fE+OBvwkh3KgTyWNSyn+nvquGtBPsAqRVUpboV8cwLNEC73KnXo/uzY5h0fQi8GGxTaHRWartgkewRZPoUfpx4BpUPfwBIcQk4I6eniClfB+Y38f9MwwVpIysiDACP7Lpy0zWkFPg/faC25BABD+QFk28CH6UWzRSygPA34ECIcTFQIeUMmEP3jACCAUJV7maUsmRjy6TFK7kuzo6PXh9xZfogh/dxLY/BL6Xk0u4cmeUVtEIIa4CVgJXAlcBK4QQVwzkjhmGGE5RN4nWkU8o2A8efJb1u7/3WnRN2INPoQ4+VQ9eRlfujJwIPtHr7P+HqoE/BCqBCrwMPDFQO2YYYhiBH130Jcka9KvneXzW710OcZUqSnfFiS27iW1/lEn2ZtGMXA8+0SoalxZ3i9oknmsYCUSs0mMsmn6hqxVaDvW+XTqI8OBTqIN3ee1EaXQX0p4my3VLsiZ4cnFaiPEsmt6SrKPVogFeEEK8KIS4XghxPfAc8PzA7ZZhyOEUddMyuH94/Wfw1wvSvRexiaiiSWEmq9trR8RBf2RUnMjCG8lG007xjmvRJFoHP3LKgBOyaKSUtwkhLgcWWw/dI6V8euB2yzDkiLBoRmAEv/99aKuBaWcO3pgth6B+l5XQ7K133yDTJw/er6JhLZjRE+R6FPjoOvhEI/hA7Pt6f3oadwRbNAnXukkpnwSeHMB9MQxlRrpF89av4MB6uGnV4I0Z6FSi09EAWUWDN24iSGnPZE1W8IL+yAg+FIgU155aBke3Kkh07FirSIX/1lurguj+N6NE4IUQzcTuAClQDSPzB2SvDEOPiAh+BFo0XW0Q6BjcMfVn2lqbHoEPBqCzCbKLu/8tIsmapGWhPfiwRdPVc4QdPS4kP5M1wgKKs2RfohH8CKqi6dGDl1LmSSnzY/zkGXEfZYx0iybQoSLqQR3TGq+tZnDH1ax9AO48NvYJO6KbZAqLbrs9jiRrV5QAD0CSNcICio7ge1lsJPqkMoIsGlMJY0iMkW7RBDogOMgCr8drTZPAN+2D9vrYVy59SbKGI3jtwUe1mU4mgk/YoumDBz+C6+CNwBsSI6KKZhjUwe9erhKYiRLogMAgn7j0eOmK4PUVRKwTdl9XdHJ77clK3SyaHkQ7uoomYYumBw8+UYtmFJdJGkY7w22i0xOfhrd+k/j2/lEYwWuBj2VN9XXRbWcEH4qqg+/p+9MtyZqoRRPnBCKlLdi9RvD6qsFE8IbRRrCHCGko0lYHXS29b6cJdChRG8wEcjiCrx28MZ3oE0ysE5vUZZIp9oN3R5VJJlsHn/REpzgTqRJJ7uoksrFoDKOW4ZRk1dF4MlUxgR7EbqAY0hG87Fs3SZfXtjyiF2vvyeILi20fLJp4VwvxAhNj0QxtrvjDMu57a2e6d2NkM5wsms4mdZtMVUygPfnn9JWh4sH3ZNH0aSarjuCjWxX0IKCpTjqKJ+oJRfAp1t4PA0aEwG852Mzu2tZ078bIJtEIbCjQoQU+lQh+EK9OhkoEHzfJmuKi2+GZrKnUwafYbCyeB5/IiWW01sEPF/J8Xpo7RuDkm6HEcLJoOqz14BONxqW0TwaDGsHrOvg0e/D9nWSNOZM1GPn3ePQ1gnd54lfU9DqT1Vg0Q5I8n4cmI/D9x8Mfh5d/EPnYcJrJ2qkFPsEI3ilwgxrB65msNelpcNVT3iHcTbIPdfCuPsxkTXbJPi3enqz4YyVcBz/Kmo0NdfJ8Hlo6h7htMJw4sL77Y8NpolNHkh689t+TeU5/EOxSohLsVBU/mT2uY9//JFommZIHH11Fk+hEpxTr4HXQ4fVF2TWJJFnNTNYhjbFo+pmuZtWr3MmwtGhSieAHSeBDISV0eePV7+nw4eNZNFL2bdHtbr1oAolH8NFL9iVr0XSL4B1ll3HbBZsqmiFNns9jBL6/kBI6W2IIfIIR2FAg2Soa54lgsGazanHNn6Bu0+HDx0uyaouizzNZo3vRWC2RByKCDy/07YsdtXuzerBoRvmi20Od3EwPLZ1DXHSGC4EOdVD52yIfD3YBQh30Iy2C9zu2G6wIXotrfrm6TUcEH8+i0aKacjfJODNZ9RqtPSVZu3nwSU508vhiT3ryRAl/xHNHbj/4ESHwyqLxI0dQciRtdFqzP7tiCLw7w/KMowR+/3vwwQAsz9tyOLXnJe3BpyOCt8bRAp+OCF7vQ/RJLSyyfaiicbltkQ5YC354MtXvA1EHH/bgs6I8eIc3n2irAmPRDC3yfB78QUlnYORcWqXM4S1QtTL153c1W7dR0/yDfofARx0oK+6Bf/936mPGon4X/PJI2L0s+ed2OurgEznpB9IYweeOVbft9YMzbsQ+xCkN1QLflxWd3F51gnBnqHFCAeWP67/Ho5vYJhrBOyP1GIuLeLK6f2/Dz02xPcIwYMQIPJA+H75mGzQfSM/Y0bz6v/CvL6f+fO29x7JodIfA6Ai+q1mJamdz6uNG07RfHWgNVck/V1s0MpRYviAigh8kgdefoV7ow98ef9uBQl+tRL9nLXgpe/ABWyw9WZbABx0R/ECs6OSI4EMxat97jOCjq2iMwA8pbIFPU6nkY/8FS76ZnrGjaauF9obUn68tGn1QapwWTXQEpp/TtD/1caPxWyearhROGtqigcR8+AgPfpAsGi2qGdlKDP1pmImtP5tuSVZHBJ9qFY1uFez12RG89uCTqoNPctHt6GRqvOqaWGOaJOvQJC9TfRnSlmht2guHPux5m3f/Aku/07dxNr8A1at73qajIbkuitE4n+uM4sMWjbd7kkxH/U17Ux+32360Rt4mg47gIbGIPC0RvDWOO1OJvL9d2UlPfxF2vT3w44dC9om61yRrClU04Qg+U51AQ35ln0DPE+VSXnTb2udoK8Yp/DIY27IzzcaGNrnptGiC1qLJdTt7/uJueg7WP923sZ69WS0O3RMdjUqYU60EcNosTnENdtnLsHWzaKztmvsxgtdJ3s4UTladToFPIIIPpCOCt8bxZIA3W32G/nZ472HY8erAjx+MM3u3rc7+7PVM1mQsC91/3e20aNotD94S+IQsmmRbFcSxYvTze7p66NYPfuQI/IiZyQppsmh0cizkh4bdUDIt9nZttepEAMpXbt4PE49PfJy2Omg5GBmdal77mbIyzv2x/ffOZsgqTPz1Nc4IvpvAZwAiRgSvLZp+jODDFk0KAt/RpLzt9vqeI/Jnv6pyB5Wn2o+lI4L3WhF8+KqlLf7z+gvn+3Se4O6/CMYfo+6nEsHr74Yzgg90KtH0aoFPJMmaZLOxCCsmhgfv9P+1kIfHNBbNkCbfp/5haYngneVtNVt72M5agCLohzd+Do9+Mrlxarao244Y/vrWpbD1JRVpaf85VZumsyeLRidZ41k0+1IbMxZdKQq8lEq0c8rU7z1F8PvWqLYM6ZjJGo7gtcC32Se16AT3QOCM2p3vv3GvClQgtW6SWlDDHnyWOnlFVNEk4sFbz0+4TNIRwcuQfdURfjw78veIfTYWzZAmNzONFo1T4Gt7Enhru/YGaDkErYeS66t+eJO6jRXBt9WoiTKdTYDlMaZa0RIRwTsFvock64AIfIoWTVeLOsBzExD4lkOWpeXsRTPIM1ndGZYH3xa/gikWBz6An05O/TOPZUtJqT4//R1LJckaL4JPug4+yWjaGcE7f3dOgHL+7qRbP3gTwQ8pcjPUFOj0CLxjBmLNVlUyGb3Ys7/djs46Gmyxb01iIs/hzdbzYwh8ay2010XWUqfiXUOUB+94jXgTnUIh+731p8D7U0yy6iuYsMDHichDIUvgG+xtXN7Br4P3ZKoot6stucTy4c1W7mdHiuPHiOCDXUrs9HcslRWd9HcmI0fdhj14XSYZw+Jz0s2iSULghSuyRbHzticPPrrZWH9YNI174eGrIyu60sDwF3h/B94HLuILGS/Q0pGGKfRarAsnq6jq/ovgua/H3gaUCLfVqfsthxIfJxzBN0VGGIFO5b/LkH1pDamVF0IvVTTWMmzOA9RZ3hdP4O89E1b9Ncn9SNGi0eLUm0XTXqfEJNChnuPyKhEY7Jms7gzw5lgevPVeE6mJ1/+bVAUkVgSvP3NnBJ9su2DdciGnVN16fZYHH7Bmt3oSbDaWgkXjXGREX2Xq72qPEfwA9IPf9RZsWQKHNsbfpr0B/ngyVK/q+3hxGDCBF0JMFEK8KoT4UAixQQhxy4AMJIOQW8btrgc4b/uPB7+XsxbvSYuUp9tyoPvknAiBd0TwSQm8FcEjI8Xb+drOaC5Vi6YzAYvGKfBaFPLGq6uZ6Ig56Ie9q1U7g2TQYycr8J0JRvAtByPve7Ps1r2DQXQE72+133MiFo3+3DtTFPhgjCRr9GumsuCHvqLNGaNuPT7bg9frtPaYZE2xDt75+mBXtDlbFTh/jxizj2WhsdDfr57mN+xZroLCqhV9Hy8OAxnBB4CvSylnA4uALwshZvf7KBk5cOUDPOn9CAvrn3cI4SDRVgcZuTB2jv2YUzwgspFUW40dIUVvF4+OJlWhUmxV6DhtGudrRwh8ihZNVytkWwen88sZ8se2aLQojDlC3UaXSupJV7GspZ7QYyf7PvQ4vXnw0QLvybT84sGO4B118JD3zD8AACAASURBVMl48PrEl3IE39X9frQ1FJ7JmkTQpL+P2Q6BD3QSsYxfTyeMVNdHDQXU64eTs9EefA+NzsJJVl0W2g8RvP5+9VQRpVuKJGPVJsmACbyUcr+Uco11vxnYCEwYkMFcLl7J/Yi6X92HPiyp0FYL2SUwYYH6chxxrvJGnbMjtSUDlj9vHTCJCryuztFllU6xjEjyOgQ+1Sqarma7P0pEmWScKho9zpgj1W20TaPzAslGmilbNNY4Ob1F8I6rp+YDSgDSEsHrOvg2+70mUiYZjraTPHGGx7e+n54s+z1HR5upJFnDFk2J9fo+24N3edTrJVMHn4wH73YKfJRFE47gY7yXcASvT2j9IfDW96unk3X1u9a2w1DgnQghpgDzgW7XIkKIG4QQq4QQqw4fTv2NNmVPplnkqcudpn3w6k+Sq1JJFS3wU06Gb+6CWdaJxineESK83b6fqEWjSyQrFqrbeAIfEcGnGNl1tkCu5Z8mUkUTjuB7EfhkI82wRZNkklVbBElF8IccEXy66uDbbDFIxIMP++WpWjRW1J6ZZ7/n6M/alUKZZFuNElmfNQfD67NmsmoPPsZMaCepJln17Nl4SdaeGp11a83QjxZNvAAlGIC9a9T94RjBa4QQucCTwFellN2+jVLKe6SUC6WUC0tLS1MeJy/Ly3r3TKh6F978Fbz+M9j4TB/2PEG0wAP48iF3nLrfTeCFsnLqnAKfYATfbIlmmWUDxbNo6neqcVzePlg0LeArUMLjj47gM7rPZNWioO2j6KZrqUbwzolOyZStbV4CxVOhYKL6PZEIvrNRRZqxWiEPFAFnkjVbCY+2sxLpSxMuI+1jktWX70iyRkWbqXjSrTXKnhHW4h4enzqZOZP0A1EHH23RhD346Ai+hyqaVGbuxkN/v+JdjR36UP2fhWv4CrwQwosS979LKZ8ayLHyfB7WyiOhZjO896h68N371ESWx6+H+t09Pj9lnAIPduToFLq2Gsguhqxi20bJyEs8gm8+oEQ3zzp5RETwNYBQghzoUAdsZl7fJjpl5ClfuFsEb0VIzpYMOpmbP14dHNFtb/sawUPiE3+aD8CuN2HuFfYBHS8abjlkT34Btf1gR/Auq6VuhrUfrT2IQkdj5L71lwefmedIskZ9Z0QKnnRbrZ1gBUf1it8W4B7r4B1ii0guyeqO5cE7etQ4H3cyIBaNTrLG+e5qK3niouEp8EIIAfwF2Cil7KWBSt/JzfSy0m9FkV3NcMR5sPsteOhy2PC0ul37d/jnl6A5KnIOBdXfOhrVF+ytX0PDnsQGbquLFPi8OBF8dglkFdieadlM+4Dujeb9qkpFtx6Itmiyi+2yNF+BOmj7MtEpM1eV7sVqVRAvyZqZp/YvnsCn6sHrfXKy+QVY+1D3xmsbnlYR4LwrbGHpqYpGJ4bBiuAzBzeC1xN/dI22vhoLdnYXwfsugLtPgH1r1e99raLRop6Zb4t9tBilHME7jgf9fwArydpbFY1DbF1JnFyC/sgIvluZpF4APFYEr5OsLvXTV4sm0KXKcCG+xbh3jTpmKxYogR+g6r+BjOAXA9cBZwoh1lk/Fw7UYHk+Dyu6piCFW/nBl96lIqT2ejj/p0qw//UlWPd3WHJb5JNX/FH97e3fwc7X4OUfwMp7ex/U36HEJ7vYfiynVB0UEQJfpy5bde9vgNKZyUXweePUwQjdLZrsMfY++AotgU8hgtczGTNyVXVSLIsmupuk/gJn5Kr3F91KQQt8V0vPzdii8bfZQuE8SFpr4JGPq573D1xiHxjBAKx7GMYdBaUzbL83rgd/CAonKVEHJUSejMHtB699Zq81KcgZyTnFNhRSV6b1O5XQt9X1gwdvvU9fvn2/WxVNCp50W01kBO91CnwidfAOsU2mBj+8TGC0B295866ox53oBcahf6ponP/HeALfsEfZmjll6jvan2spOBiwZmNSyrcIr7I78BRkeWnHx4vjPs+c+SdRkVOKuOwPKps/7UyYsFBF9ntXw39+rKLAGeerpOcr/6N2de1DdkJz11u9D6oTnM6IxeVWIu+0aFprVBMy/SVyZ0LRFBV9dbXZl+jxaNoPlaeo187MjxHBl9jRoK9ACXAqkZ2/TX3ZM3PjWzRCRCVZHTMXdYMvJ+2OCqLOpsiTYTykVAdG6Qz1/pxfft3QrPxYNe+gq0UJ5ROfgQPvw6W/t7fVJXqxaDkIUxarqw5dJhkKDl4VTbAzRgTvFPh2daIG9ZmGAjDlFGVBNeyxP/eUI3ht0eTHL5PUM1mR6n/SckidWC+9G/LGxn5dHXBotDUCttD2tiarcNvjJ1UH74jga7fDh89Eev8QJ8katMfsjzp4Z3AXz6JpPgBjZ9tX3q2H1cm2nxn+M1ktLps/gasWVnDzntM55SkPp93xGjdvmM69e6ewbHsNjWOOUUJ/0i1QOgueuUnNMnv8U+pAu+iXapLSxmeUN7t/Xffa7ZX3ws437d+1wDsjFlBlhi0HVU3+/vcdFo0VwWcX21ZOLJvmvUfV0ntgTak/YG/vK4hc0KOtVp3EwonegtQ9eB31Z1gWjf5yStmzRePyqMdjCryzfUKCYhToDE9gU2M43os+cZZbHQ9ba2DVfbDp3+pKbb6jiZsnM3YEry+hc8eqzwscEfwgWjQ6gtcn+JY4kZ/+jpTOVLcdDf03kzWzlwheByUypGZmbnsp/sScQKfV6M1RLKFPYuAok+ylDj4cTbsST3hGe/Drn1Sttet3Rc1wjWPRuFI4qUTTWgOv/zyyq2q8CL75gLJddcXaAC26PiLaBQMU52Tw8yuO5rbzZrL0wwO8vvkwq3bV8cx7dtne5JJs5pYXcPKUH3Hluk/j/sNidYlxzT9g6hnwxh3K7z7zO/Dit2HPO3DkeerJm1+A529VX7qzvg8nfzV2BA9KjJsPwJOfVT0pOhrVNvqLk11i15q3HFLRvKZhDzx9Ixz1cfjYPWqMUEB9GUAJUrRFM+lEWyyyCtWXKrofTiJoIc20kqw6EtEHhdsLCPW7lCqa72pV0bsQSuD1FZDGKfCJipEWr1j1+HoilZ5Y1larykN9BbDoi5GvEy+C15Fybpkt8F4fBFxpiuAti8ZpiTmTw/r/oEtROxr7YSarI8ka7LJ6CsXx4EGJoJ6YE6/vfzjgcRwPXmcE70nMotFiK5IQW10mqQVel+u2HLRPLPr1o3FeNSRzUonmw3+pJTN1qXT2mNgRfGeLchPyxjki+CRmtSfBiBF4TWleJp88YTKfPGEyALUtnazf18T6vY2s39vI+3sbeO6DLp5zfYXfee/kD95Ps+edImbv3sn8CZ9nYuv7ZB95DWNf/gHsfEMJvL8dlnwDxsyAslnw8vdh4gn2mTorynbILVPPdUaPOWPsgyq72I5OWw4qEfrDSXDCF+wDds876lYfTM4IXgt8KKQi0ewSW+B9hepLmoqn1+WI4DNybIvG2TdFu25Bv4p4u1rV9hA/gvdYy7YlKkZavPRn5HwvOkFeZk2Kbq1Rn2FuDMsgXgSvBTN3rF2vHU7KDmYEH2XRgH2F5BQGnasptQS+vSFyIphTFBMev8Pqv6NXWeqy8y/6exAdwevIvWmf2qe/XgAff0gdE9B9FitERfBuayZrohZNEnaJtmh0pK6v9JoPRFo08WayOj34VC2axmp1u3mJui2ujF0Rpb9/eePtCXkDVEkz4gQ+mpLcTE47spTTjrQvGxvautiw7wQeq76c/fua2by3kRc3HARmqp87lvGEbzoTVzzOof2tHNHwJr6G3fCpZ5WXv/tteO0nyhsvntZ9kY/ccbawTDoJ9ixTIqyjsqxiW5Aa9qiTQe02lQvQdk/DbvXl1F/UvHJ16yuwe910NKgvY84Y+wvsK7CTpfEIBe16dyfaosmMsmgiBB77MU+GikR050BfoVWJ5BCc9nqVzKzZkngEHxb4OBF8dol9RdNWq6wNfaA40SeWaPTBlFMaadFI2X8R/IEPlBBOXqw+z2iCnXZlh/789D417Y0SeB3Bz1C3ellGr5UI72yKTOAngq7i0SeZYKcSo5xS+7vjctv/x9ZD6jsK6n+wb536ff/7tsBH96GBSA8+XAfviKKjT04yFBnBJ1MH7xRyHRE3H1CBQm9JVpcl8K4+lElqgQ8F1LGQVRS7kMIZtOmr/wGazTriBT4WhdkZLJ4+hsXT7S9ie1eQoJRU1bXx1tYaqrdcxLzq31K6837Wyuk8GPoKB17yku/7kLO8l/HxnarK5uBHHqTM5aGpzc+OmhaOrijEpaPtsfPgst+rOvzyY+11W7VFUzoL1j8F4+aqA6HLOliPvkYt3bbnHTtaj4jg16v7zohJHxS+QvUF0xOEXDHSLO/+Wc30/drGyARvOILXdfCWsOqox7kSjo7CtEUDtsh0NNrJ1PZ6qDheCXyiEbw/WuAdJ6uWg1bkY/3v2qwIfvzR3V8nXl27U+CzHBG8DPVPBB/ogoeuULkTTxbc+IYdfYe36YwdwWuB74oSeI9PfQeEW4mWDKm5B7Xb7BWsktrHDnv2rt7nrlZl2egThzOC11eUnix14tKdSyPsQm1ZxovgLatEf8Y7Xod/XAvX/VOVC4IVTVtXicn44UG/+hzDHSGtKDwUXT4Zpw4+wqLpo8CD+u7qGcrRhIO28eok7ys0EfxAk5Wh/sGzxucza3w+nPod4DtsP9TM4UMtlFc3smXzYepb/TwqzuUM+Tjvhabx+cfd5D27lLauIMGQZOHkIr4x2cvxQGDGhXiKK+HG1wGo2r2dicD21gymCQHHXqe8/sOblBWUM0bV45/9fdjwlLok1haCFjsdJYNtEeWUREbw4aRZS+zMfNUKFQXuXweTT7If11ZIZq69TijEieCdAu+waECJenaxOlA6GqFI2WUJNxzT4qb9SWfJZ/N+9Vlk5CqBbK1RB0duKhH8mEgPPhTonwh+w1NK3E+6CZbdqTppRgt8sMsWdueEK/2enX58i/X+hFAnJP1/z7MEPhUfPmidYDyOCN7fpj7XzNwYAr9cRcFTT1cL2+gcT7dJd0SVSUZ78F4IWf/fQx+qfX/ierjxTfXeUhXb6G6STtxehwffS5K1rxbNtDNh+6vq/+W0OZ1E2665ZUbg08W0sjymleVx/tzxfOP8meHHO5tXM65e8JP9rWzY10hhtpfS3Ex++8pWbt7t4p6MSr779iQqD63F5RKcUFnMCy/v5a/A4xvb+WRdGxOPuhpe+r4S4pkXw5zLYPFX1T9+wgIVNY0/SkVE+nLeV6AOilBIJXW82VBxnBLa3LEq+VjtaLUbS+B1j+qqlZECv/MN9Xr5E9SBHvIrIXcKfLjuXJfWtUB+hbrvFHiwD/5CLfBJWjS+fCXS0VU0ZXOU2OWMUQdVZ1Mcgc+MXcXQWqPeZ0ZOpEWj36tOIKeClLD8bmWnnP4tJfANMWZRBzod/VpiCXxUktV5gm+0BD7fsu1SqaSJtmgCnVYn0WIVxbcctGeygmoBMv4o5SvvfMMh8I6KrtbDanv9viBGBO9IsuoWHk37VHLywjtSF1tt0USvt6rH7amKxlm5k2oVTSioTrxHXaUs2FJrImOsthPNB9T/XM9rySk1Aj/UyMwbw7w8mDcpsoLmyoUT2dfQTnXD+YxZvptVu+vp8Ad5as1eyjwFBHzZbA1M4iN3vcXYPB935J7M3Ja3EEecjXB7odDqoTLxBFj2u8gKGqBF5JCLpLV+Hznrn4JZl6gDMjMPbrUqWPTiIM7Id8U9sPk5+OQTdndK3c0OVKSx/imYfaldBw/qoHdaNGGB78Gi0QKvb3PHKgGN7nz4+PXKrrrol7D6fnVgzL7EPii8OZFJv1BQeZo68skusU9W8Tx4ZzM2TatjMo4zyepc2cgpTMlQtVLV41/8G/W55JTGnhWtcxigxtL113q/uqKSrMWV6n5EBG99DqlE8GGLxtoHLfCFE+0rMmcEX7cDZl2sxvS3qhwDxJh0VxJpC0bUweska8CxfbEKSnTjrT4lWd2xI3hXL0nW6CqaVCL45gPqxFBQAWd9Vz320vdjBxjN+y27zQoickpt+7afMQLfz+RkejhibB5HjM3jjBlKdKSUrK1qIMPtwlO+jxt21vHIyj00dQS4eecnKO06hdD9G+kKhNhZ08qs8XmcNuZEbvD8hYwD78P0c5BSEgxJ/ra2ni8Drz58Bxd3NsIxn+i+E3qCjLZcDm+Gpf9Picrm51VknpGrxEhHqxufVQnTY6w6ch1V+tviRPBJCHxWkZU7cAhRMACbnrcbUa35m8pTzL7EFreMHHWy0QdJa406iLSw5YxR0ST0UEUTx4N3tnYAqw7esnMCnakL/NYXlVjMvVz9XjgptsA7PXghLEusxRHBt9mfdctBu1W0r9BuV5DXhwg+PK8hyqLx5tjfH91NEpSo50+wx4zlwUf3oYGomazeyHbBen5IUaWaxwBRdfDJtiqIZ9F4ek6yhqITuykIvPbfdZM7UN/fYJf6rrsd+9V8wG5KCOp/HisQ6QdGzESnoYwQgmMnFTF3QgEIwQlTS/jN1fO57/rjWPrdK7n00is41NyBxy245JhyAiHJ79YFuar567STyRM7vRz5nSVc8Ns3efew+qJcXHs/TRlj6ahY3H1AHYF1NasD5Jmb7AP5nT+q27mXq0vIht3qC736r6oef7L1elq0u1ptgY+YCt6LBw92H/ysInU56ow067ZblSQ+Je6gZh/qWax6HzJy7SuRFp2cckTw+oDVE0ac9OTBxxL4sNj1kmh969d2TXg0O15TbZ21NaYFvqNJXbFoIXC2KgD7hKqrKvxt8PsT4c1fqINfn8CyCu0IM9+6sks5gvc5InhdJukQeN2qQFMwwR5TEyuCdxLdi8a55GNbrbIfiyvVfb0cZSqTjqKTqWBbIBF18L1YNCKJBmdOGq3KtgLHkhfhICkqitcRvObcH8GtW5MfMwFMBJ9mMjwurl00mWsXTY543B8MsWz7Qn68fAbtrlw+WVDE8u21TFt0CXLGAp5/aSkP7RvH9jte55KjyynKyWD7oRauXDiRyX4P5cDufQcp2PgjCqtW8O6xP+O4nX9QJZvCBfOvVcK6eYlK1O1ZDhf9yr681rbQP7+oqmDAEneHBx8K2aIAtlhGR/DZxUrwnJHmQasS6KN/hK0vqcjv7d+qAz1s0WRHWjS6+kBHP85qjaQi+BrlJ4Pt3fvy7XF76kcT6ISXfwjzrrSjalAn0s4mFV2f+g378YKJsOk52P4f1Qxt6hmw4FPWVYJT4C0rIzNPve/m/XB4o/LwkY79dPjb4QjeIbKBTnjzl7DoS3aFUMz3YdlQWoB1mWRGtsOicdvCByrX4rALEe5ID76tBsbNixynm8B7IyP4kukqggfl63ebyZqowAdVlOz04EtnKBvSGdknNJO1DxF8vkPgw0FSm31sSKm+x0c6PkdnIrqfMQI/RPG6XVb9/vkx/jqPi2acT9H2Gu59YwcPLN9NVzBEXqaHp9buZaK7jje9kPHS7eSLeh4PnMptyyayZOoCZtXvUgdU+bFKLF64Xb3kSTfDws+ER2grX8S2Y3/MvC13IarfVQdz/gRHBOq3S8D0F9ntgcyC2BZNdA+dgxvUa864EOZ8FLYsVQJfu00dEMKtBCgjxxaR6OoD54zJnAQjeCkjI/iy2XDNYzDtLPjgceu99SDwDVWAtD3omm1q4tv2V2Hhp5U4TD3d3r5wkm2NgW1tBB0WDdifYUaOOuD10pPhJQh1ktUxdyG7WL2GM4Lf9rJaC6FwkjqJxyPYCRlF9j50tVmP5UZG8CI6gi+3fy+dEbvxnRNhtbIOdjqajVmi3Var1jLW+YX6nd3r4ONF06EgPHsLnHCjOqnEsmjCAu/uJcnaS7OxRCaSNVar/42zqMF5FazpbFLHjTOCH0CMwA9jTpo2hpOmjaGlM0AwKMnwuLjr1a3UNE9k94T7aHn1NxzylzH78/dy1qtV3LllAr/PgLUd41j51m78M/7K4qzdHF1RhGvOpRGVI9975kOeWD2V7134LJ9ZUKzEVldXgDqg9MHtnKiTVWj3yjm4Xnm6+ovv7NFxcIOaeq+9bj1ZrHZ7ZPsDX761kAn2LFYtdlpMsopjV0/EiuA7GtXlvH6uEHY7CqddEY+GXeq2ZouqdHngEpXryCqE5XcpgdQrb4FdQbTJEni9LkGgK3YEn5GjPjOdKNfoCN4ZlXuzu18Z7Xjd2r9eLvl1DkDvg24K5822J2Y5k6ygTvDeLKtUt0F17tz6ovpb0K8ei3Wi9VqLfuglH0N+dfWnW23rCL5uZ+SsUpc7fhvd+l2w9kEllOPmdW82BnbvnoiZrAnUwTtPKm11cOex6up27sdi7wuo77bTf4fYFk1zlM04wBiBHwHkZtr/xtvO06WcRyEXfYxgSOJxu7j3v8ayZlMRwcfu5t3OCv5vySbcLsEvQmVMLM5i7JvLmVOez+kzythd28oTq6vJ93n49Ss7ueTYyYzJVEK8dl8r84EDb/yFcQetBKfzi617wne1woZ/qtJPl7t7kvXgBlUppCmcrA7Cuu3qgLAOjmUt4zip7ml1oDXssTpnWqKkE3qx7BmwI3hn2WN4zdAYQuRMOMZDC7QMqqqjpr2qg+XE4+Hes1TXT+fJpnCSuu2yEt7xIngtBjqCb7QSs5WnqkRyLIsmI6d7bkMnnfWs02iC1spR2iLS+6CvtjJyIKCTrA6Lxptt51jyy9XjBRXqhCll7D40Go8PaIwsk+xoUJ9h9hh1ksousSL4BOvgdZmmXgIzeiarJ8v+7Pti0Wx7RX02+9bEF/j2enXCL5ke+Xi4Ei3GrOR439l+xgj8CEYIgcethM3lEiycPR1ufI3PFVVyZTCTPJ+HZ9/fx5IPDtDY7ufRd6v423IlQPMmFPCLK4/mot+9ybV/XsEZM8twCVjx5jae8MC4HU8SrFiE+4r7lAhpdD+a6KocbdGseVCdBBqr4LjP2s9ze1SSt3abOiCtq4LHDlVwErBv/euU73oj8qSgo/BYCVawrg6k3TcHYk/GidieXiJ4R0378rvV7dTTlX3xxbcjr2bALnsFdZKr362iSBmKrNRxCrwWBm82nPMjNU54rkGUwPsKbHFtPqh8e4gfwb9wu+q06PJGJll1Qjwjx77qcSZZ8yfYJ8kxR1rdUa2Eb1dL7D40Gu3Dh5Osge6N+ooqlWhn5CaWZNX/B70OsS6T1CfXnFJ7X1wete8izoLfPTUb2/KCNV6cBYD2rIAHL1O2y3Gfj/xbrCZyunVBrHkbA4AR+NHG+KNwAXpi+0fnV/DR+Uo8WjsDrN+rbJejKgrJynDzs8uP4v5lu/jT69sJSfjIhHFQC88Fj+fl7B9xW9E8QvVtrNxZx7LttXy9K4vxbXtgzQNKsCedqAbyFUCgHZ75ir0vY+dG7lvxNLWkYcEEyMimrrWLF+rL6cp00/zO39RBdtLN9vZaHHqK4MGqGLGEzNmmIBpd1RJvkRBQAl08VYnpoQ0qatOVE4UTu2+fkaOEpq1GzVlY+6CdU3BW0YRFPccW+8JJqi3y5Y7FZ8J1+1lK0MqPgfcfUycxHb1Xnga7l3Uvz2vap/4vzrJX/Rk5I3gdwboz7AjeWR1yyZ1qG73mcUdjzydObT85F/yIjviLpii/fOzc5CL4up1WD6EoDz6nxN4X/Rm44qwmJUP2ycs5ZjCg2iNDfIF//x/qOTe+aSfuNbE8+LDAmwjeMMjkZHo4YWrkJfblCyq4fEEFgWAICXhcAg7MZON7bp5+bRdPv/+f8LY+r4tjZBfXenZA3Q42zLmV2UIgAJk7VvWhPO2byiLY/LxK9Dopma7KDGu3wuzLWLO7ng4y2eSaxlF1r6ptpp3p2GHrAI41yQkcEbnDculJ4D1RFs2hTUp4nLXcOkmdXaIEqfK02GM70VZB5WlK4HV0HS+C1/ejPV2wI3gtHtPOVP3wq1aqz85XoEpgd76ubJ7iqfZzl92lxCsjT11dOUtDnR781DOUGBZNVgvkgH0FAXYiUSd8Oxp7ieCtMbQAhwKO7a3vW3GlqjIqme4Q2x4qWrRV1tlotdT2R0bq2WMcEbwV1bu9sU8YOvoHdatPgFUr1HvLHRtf4KtWqJnk0eIOsS2a1kNqP309VDj1I0bgDQnhcTuSbeOP4tbxcOVxU3hh/QHyfF5ml+cze3w+Lz9VxQdbDvIP14X8ffVRnNy0koWTi3lt42Ta/b/krK6T+fLZ08m+8OfdBymZqsQ1Mx/O/j6r3q7H6xZkVp4IO7bQnjuRLGfnTl+hWuVoyslxdtoS5q5mqG+Dt39jPxZdrw2OCL5LWUnPfEVV2Sz8jGoLcdLNyhqYYJ2Yqt+NrJiJx8JPq0Ss7smje+bHqoP3ZtvCoE8MTrSo6m0qT1WC9sHjqgfO7MtU9QioCh8t8O31aq7DUVcBQjWz82TEsGisWczzrlC/x4rgo/elo9ERkcc6ceoI3mO3Cw5bNJYIF1UqO6Z+l30C6c2i0a2Vdy9Tj+mTn8tjNZIrsmwmj/16sWay1u20O2I6I/itL6qTwzGfVIuHdLZEdgbtaFK5pNNvj72PMS0aq/NprCaAA4AReEPKTC7J4cbTIlslX3jVDcANzA5JKt7cwYPLd/Pm1hoqx+QwY+Z87n51O0+t2cvXz53B3An5jMv3UZDlRQiheswAnPU9yC9n9e5dzCkvoPLYs2HH33jFP48LQhK3S3cbdMH1/46/gxMWAALe/YuqStn2shIFX0FkBYtGR5rvPaxq1yeeoJJ4z9+qXqet1mp/PNmKOP+pkqq9cex/qVtdBaRnojrrn0umqx+XK9KiiUZHfrpW3VegEryr/6p+X3yzfUVTuxU4V91f97DyiU/8sirpe+9hFb1HR/DRy0dqgc/vReBba9S2sbpahiN4y4OXIftKSp9odf183XbVkht6nlVav0uVWO58A961LCydCyquVB1a6vW7BAAAHnFJREFUXS4l9PoKzFmDr+loVGPqGeHOq4adb6qKKL24TGOVfSIAq9WHjJwP4SRekjVezmgAMAJvGBDcLsEXTpvGF06bRktngGyvG5dLsGpXHd/71wZuffy98LYel8DlElQWZ3Pd0Q/R3j4b3zu7ea+6kesWTSZj2hwaC2Zx36HjqV+5h+uiJoXFZexs1bFz+d2AVJN0mvfHFk6wxW7js8pO+cQjamH1+l1qDYCXrB4jRVNUz55ZlyTXlCy3TEWzax9UJxqnvXPCjXD8Dep+TwKfmQ+IyGTutDPVRLUZF9oClFVkW0GhkDrJTTxBCWnJEapVddkse5UuXdoanSTW1kVvEXzrYVWuGisyjfDgrddrORh5tTJ2rhL7tlqHXeKKHXF3NKkTbeWpau3knW9YjfasnM6X3rG3vfzP9skp1mpS+99Xt+OPsfdRBtUV1/734OT/tv8PDXsiBb5qpTqpTVhITPT/sX4nPPpJuPjXyqIZJP8djMAbBgFnGefCKcU8e9PJrKtq4EBjBwebOqht7SQQlKzaXc93VrgAuwb8pGmqWiP/q8vJvHcFP/73h/g8LqaW5tDhD7Foaokd0cfizO+qSDtvHHziUfj9otg+MSjhXvgZ1a5h7uVKvDNyVMKucBK88kMlENpqSbbjpBDquYc3KTF3iqYQ9utpQSyMcSJzuZSwOoV41iWw8h449Tb7sZIj1JWCv10JYN1220rw+uDLDhH0ZNpT7fX0fk3ZLCV+WgCd6KsJnWSNlWDVrw9WGaNjxaWIxepd6oS34aneZ7LqCpqSI1SeomG3mqgW9u4d/xdnhVdMgV+nbvX701U0VSuV0E9ZHCnwTqreUVed8RbLdrnVCf29fyibZvalyqIZOy/29gOAEXjDoON2CRZMjr1ARUunOgBbOwM0tPk5cqyyIoQQ3HnNfG56eC23PfF+ePuKoizOmT2W8oIsthxsJifTw4xxeZw5s4yx+T4VNX/uFXUQ5o2Dj/zOnqnZbcc8KsqKRW4pzLhARfexhDdRiqYoz/fkr8XfRot3vCuNrCLbogEomwm3RdW9z7hAnZB+MUMlIvPGK4GJxam3KuGpPMVeqEVTPDW8nkE39MmgvUEt9hHvxOn04LUf3nKoex5k2hmWwKsIPogb/B24QV1Jrfu7WpBe2zlFk9UEuYbdMP2s2GM7cXtUcvaNX6he/Z5MFaXnT7BtE91NcvcytR8Vx6tI3J0ZWSLbWqtOAroMOB4Z2Xa+4fAmK4I3Fo1hlKKj/dxMjxJoB2NyM3nws8fzzHv7yM300BUM8ejKKh5ZuYcOf4gxuRl0+EPhk8SEwiyml+Vy81nTWVBqCVes7puJcvYPYfLJ3UUwGc74Niz8bPemXU5mXKAsgngR8dk/6L2O+pSvWd78/SpRfPTV8TtkOiP/ZHB71IlGR/BO+8KJ04PXdeo1W6B8fuR2U8+wtlMR/H8axnJm3avK33/hdpVI9uZAtdXorWiKKq3d/qr93J5weWD3W+qndIZaHHvfusirEz2TdffbqgRVJ1ULJ0ZG8G/9SpXTHve5nsf05gCWwO9Zoa4gjEVjMMTG43bxsWPtkr2LjyrHHwzR1O6nJDcTKSXbDrXw0saDbDnQzPIdtVz+h+VcNG88Hzt2Ahv3NyGEYO6EApZtr2FXTStet4vPnTKViUVZ/H3FHq5aOJFxBb7ug5fEWH83WcYfDT1oO6CEL1r8nMy5LLGxppwcv8KoH+gMBMnUi8C31sSuoIFID37GhbDiT8oyit6+cKJKNFsR/7OhkziHh5HL70asf0o1UDvxK3DvmUpcs4rgpK/A1NNiz6CNJnuM1exrv7Ktpp6uJtbNu9LexuVWUX7tNlj0Rce+Odo+N+6FlffCUVerq6ee0FdjJUfY6y/E+5wGACPwhmGP1+2iJFdFiUKIcD9+UFbPH1/fzv3LdvHcB/tVN1irvYnHJZhWmsvhlk5eWH+AnEwPje1+lm2v4eHPLcLVk7c/ytlZ08p5v36DtWNyyGmvs5ZpjGfROKpYCifCl5bDe4/YXUqdXPVA2Kdf3V7OltAEjnzrVyqyPuELKm9x/XN2K4eiKeonEa59QiW3H/mEEvjdywEZeTKdf626emqtsXv6gxL46tUqwbvkG8rGOf2bvY+ZmaeuMmZepMp0wUTwBkN/kZPp4evnzuCGU6eyenc98yYUEJLw4f4mjqkopCDbS2O7nx88s4H9je0snFzMXa9u43+f38jY/ExaOoPkZXq4YN44xhdkIaWMnBMwAthZ08qY3AzyfDEatsXh9c2H6AqGONjlY+rB9YDsIclqtyoIBEMcaA5SseD62NvqkkSgvt3PM8GTuNX1OBx5gZ3cHjNd/SSLzr1Unqo6gL72f0pspzqqmaafrX6imXu5mhvxx8Uqkj/3x4mdWM77X+Xl12y2HxukNgVgBN4wSsjzeTl9hn1gnZZnXyYXZHn59ceVDyul5P29jfzlrZ0Rz//f51WPF7dLMK00h0VTS5hbXsCWg80smlrC2bMjo7K3t9VwuLmTBZOLqCjKUnX+Q5BgSHLJXW9xzfGT+NaFcTz0GLyzQ9XNV7V7mdq+R1XUTD099sbTzlS1955MnlpdzXf+uZ6V3z6LwuwYcxEsOvxB2rqCPMUpfCF/GbknfzWJd9ULurJm3xo44/8ltnpX5alw2e/h6RuV37/oy4mNFatG3lg0BkN6EEJwz3ULqK5voyzfR16mh+r6dpas309bV5DOQIiN+5t4fFU1D/h34xLw57d28l8nTmZScTadgRBr9zTw8saD4dccl+/jUydN4YunJ+bfh0KS+rausO00kOytb6e5I8DGA80JPycUkqzcVUdZXiYvtR3FvCmFFF/529i9eAAmnaB+gG2HWsJLU86fFF/g69tUu4B9jOHOeU/yrYmJn3x6ZfzRdLhz8YS68DjWQOiVo69WLYj1hLRkGHOEunV5Y08GGyCMwBsMUfi8bqaX2aWUE4uzueHUSHHu8Aeprm9nQmEW//PvD3lguV1Cl53h5hvnz+C0I0tZs7ueJesP8LMXNnF0RQFzJhTQGQhSlqdsi65AiHVVDSzbXoOUcMtZR/Cj5z7koXd2c89/LQyv6ztQbDushH1nTUsvW9psPdRCXWsX/3PpHP73ufNwl03kh/HEPYq9De0AVNW3M39SfKGrbbE7eu6oibFwdV9wuXnQdSnC6+Vz8WyleJTHmA+QCL58tQKXc77DIGAE3mBIAXUSUCV0//exedx+/kwQkOlxkelxhS2ZOeUFXLlwIhf89k1u+cc62ruCtPuDXDhvPI3tft7dWUe7357Mc7Cpg8dWVeF1u/jCg6t56HMncNyUPpRl9sK2Q0rYq+vbVVWMp5eVi4AVO1XZ3xkzynhzaw1LPzzIDy6Zk5ANtU8LfF1bj9vpCL4kJ4MdhxM/+SRCKCS5o+0jZHhcfFbKwbPPxh+d2vq5fWBkZYsMhjRRkO2lIMuLz+vuJhg+r2q73Njm58RpJfzXiZN5+cOD7G9o56qFFfzpugWs+945nD9nHI++W0VBlpclt5zChMIsvvLwGupau2jpDPCDZzaw4Ecvcc8b2/EHVb+UX7y4mXveUIteNHX4qW/tZcHwKLTASwl7ansWXc07O2qZUJhFRVEW580Zx/7GDt6vbuz9icD+BtWKubq+57HqrPdx7OQi9tS1EQimsE5qHGpaO+kKqvkSje0xWiEMFJfeBVfcN3jjYSJ4g2FQOL6ymA9+eG44Qv7exbO7nQh+dsVRtHYF+MTxk5hamsud18zno3cv4+p7lnO4uZOGdj9zyvP5yfObWLrhIFcsqOCuV7fhcQnOmjWWLz60mvo2P898ZTHjC7Jo7QzwvX9t4IunT42wnJxsO9RCvs9DU0eA7Ydbw+Wlmu2HW/jGE+/zh08eS1m+DyklK3bUcdqRpQghOHtWGW6X4MUNBzh6YmQL3K5AiMvufpvPnVLJx46twB8McbBZCfye3iJ4S+AXTC7ipQ8PUl3fzpQxOT0+J1H2Ndj9/qvr23tM9vYrydpB/cCARfBCiPuEEIeEEOsHagyDYTjhtD9i2QIFWV4e/OwJXDhPzYSaU17Ady+eRXV9O4unj+GpL57Ev286hd9efQzrqhq4/akPmDkuD5dLcPU977DlYAtN7X5ueGA17V1B7ntrJ0+uqebnL2xGSsnSDQfCFgkQnhR25kzl8++M4XU/9/5+Vu+u51/r9gHqhFDb2sUia92AwuwMTqgs5sUNB7o9d8XOWj7c38SD76j8xIHGDqQEl4CquvZu2zupa+1CCDjW8um3HEw8Cdwbzs+gur7n/RjuDGQEfz9wF/DAAI5hMIxorjtxCtedOCXisUuPmUB2hoe7/rOVX151DA+9s5v7l+3iwnnj+Nj8Cj7/4Co+c/+7rN/bSE6Gm6UfHuT7z2zggeW78XldXHJ0OZkeN+fOGUtTR4CjJxby9vbamInW5duV3/7vD/bz+VOn8s5OVR55wlQ7L3DenHF8/xnVIXRaaW64WmjpBlVJtHZPA3sb2sPCOndCAR/uayLobP0cRV1bF4VZXo6qKCA7w81rWw5z7pzkF6peu6ee3ExPxJVJpMAnZksNVwZM4KWUbwghpgzU6xsMo5lzZo/lHKv2/uazjsDtEtx42lTK8nz84oqjufWJ9xDAP248kU/dt5IHlu/mzJll5GR6eGH9AToDIR59V80GnV6WS+WYHLYcbOGHz25gw94m8rO8/PTyeazeU0+ez8N7VQ1U1bXxzo5axhf4mFRs940/f+447nhxM8+8t4+uQIizZ5UxvSyXlz48yJzyfDbsa2LJB/spyVVWyAmVxbxf3cj+xnYqiiL7z39Q3YjHLahv9VOUk4HP6+b0GaW89OFBfnzp3KRmF4dCkk/dt5J2f5D/PudIPrO4Ep/Xzd6GdnIyVK5kpEfwaU+yCiFuEEKsEkKsOnz4cLp3x2AYdhTnZPDdi2eHSy8vX1DBPdct5KcfO4rjphTztXOOZNHUYn73ifnc+Yn5vP+D83j2ppPDltH0slymleawrqqBv769i6CUvLzxIDc9vJauQIj/PvtIAJ5as5cVO+o4obI4wmIam+9j3ffO4Z1vnUWmx8V9b+/kg72NHGjq4NOLK5lTns9zH+wPe9/HVyp7J9qm6QqE+Mzf3uWbT75PbWsnJTnqhHDenHEcbu5kbVVDUp/LjpoWmjoCVBRl8/MXNnPi/73C8x/sZ19DOxOKVJJ4pAt82pOsUsp7gHsAFi5cKNO8OwbDiOAcx8zaz50ylc+dMjXi70eOzeNP1y3ghfUHGJfvY1qpKvn86tlH8NWzj+RLf1/N8x8cwCXgioUVvLD+AL9+WS01uGhq98ZeHreL4pwMPnbsBJ5as5cN+5pwuwRnziyjpqWTny7ZhMclKMr2hltAV9W3cSL2ay1Zv5/DzZ3UtXYxvsDH7PGqHfHpM8rwuARLNxxgweQilm2rYcvBZq5fXNnjZ7CuSlX23HPdAmpauvjev9Zzx4ubycl0U16Yhcclulk07V1BugIhCrITb9swlEl7BG8wGNLD4ulj+NFlcxFC8PHjJvLX64/jlrPUjMuvnTMDl1B+eb7Py5+vX8hPPjqPKxdUcP7c+F74ZxZX4g+G2N/Ywa+uOprinAyuPm4i2Rlu3t1VT3lhFuWFWbgEVEdV0vz17V1kuF0EQ5Lq+naKrQi+IMvLSdPH8OSaatbvbeRLD6/hR89tpLmj5xLHdVXKf59WmsuJ00r41ElT2FnTyqb9zZQXZlFRlM3e+naktOPKHzyzgavvfaeHVx1eGIE3GAzk+bycMbMsbL1ML8vlJx+dx9fOUfZMvs/LNSdM4o4rj+6xrPCIsXks/e9Tee3W07n0GLViVWF2Bp84Xi1eUl6YhdftYnpZLs+vP0BnQE3yendXHeuqGrjpzOnhiZ5a4AG+feFMOvwhLr37bRra/ARDkpVWwjce71U1clRFQdi3P3f2WISAQEiG6/ibOwM0tdurPL1X3cDG/U3UtnQm8/ENWQayTPIRYDkwQwhRLYT47ECNZTAY+p+rj58U0aAtUaaX5ZGTGen+fvbkSrxuEU7OfuuCWWw71MKfXt9Bhz/I7U++T3mBj0+fXMnMccqacQr8zHH53HnNfFwCvnH+DHxeF29tq6GlM8Ddr27jzF++xmOr1LKDq3bVsbOmlY37mzjGUZtflu8Ll12OL/BRUaT6zu+sVeWhwZAMl4qu3ZOc3z9UGcgqmj4snWMwGEYS5YVZPPnFk8JVM2fMLOMjR5fz21e28s91e9lxuJUHP3s8uZkeFk4uYuP+JoqirhTOmFHGe98/l+wMD8u31/LW1hr21LbxyqZDFGV7+fG/PyQ308NXHl6Dx+UiEJLdJl+dN2csq3crq2hySTbZGW5++OwGHr1hEQcbO+kMqBmza/bUd+sQmijv7qoj3+dlxrg8DjR24HIRToAPNsaiMRgMg8JRFYURUfmPL53LZ0+uJMPt4sbTpnLKEaqN7sIpKsouzu1uBWVnqJj05Olj2HqohVc2HeL/XTiLx248kdauIF/6+xqmlOQwY1weGW4X8ydFCvzHj5vE1845kgWTixhfkMUvrzyatXsa+L/nN7Hd6nmT6XGxZk993Pexdk89v315K6FQ7JqQrz22jp+9oBaOv/mRtXzhwdU9fi7/eHcP33rqA4JxXq8vpL2KxmAwjE4Ksr18+8JZfDuqD/25s8fx32cfyYkxqnU0i6eraf/zJhTw6cVT8Lhd/7+9ew+Oqr4COP49eUEIJDERwismAcLTByCgFlHaYAW1QKtVqrVaaa0z1kqtUx/U1jrT6WintqN1ijo6Qkt9VkbaTh0UHRy0vA3yECRAxGAChmB4JYEkp3/c34bNwiIJ2Xs3m/OZ2cnNb+9uzv7u3ZN7f/vbc5l9aREL/lfOkzeOobhPL/YerD/hyDkrPZWfuQ+SAaa5Szm+traCPpleeeYrR/Xlrc17aGxqPuHiLqWffcnNz63iUEMj/bK7c/241lU0Gxqb2L2/jjT3uLIvvMqbFfuPnDDnP+Q/G6qoqq2L+qWvM2FH8MaYuJKelszdU4rpnhq9suXIfpnMmVLMn2eNbknCD0wbzsoHpzCqfxZpKUlRE2qkqaP6cqihkVfXVJCTkUbJiD7UHWtiS1iN/CWbqrjk90uZ+dT75GSkcf7ALB57cwsHImbyVOyvo1m9nwfqj7UUTXvTfaAcOfOnsamZteU1TCiKTcVQS/DGmE4nKUmYM2Voy/x98Or7ZKW3ff76xCFnk5acxM7qwwzp3ZPxhTmIwIurvG/6Hmtq5rf/2kx6WjK/uGIor/zkEn438zz2HT7KM8t2tHquUEXOhsZm1pbvd3HBa2sruOLx9/jR/DWt1t9ceYDDR5u4qOg0LhreDpbgjTFdWka3lJbaOoP7ZNA/O53ZE4tYuHIXy7dV80bp5+z+so6Hrh7JXSXF9M3qznkDsygZnsdLqz9rKd0MUL7veMG25WXVgDc9c0vVQXbVHGF1eQ2HGo5PywxN9YzVEbyNwRtjurzJ7uIloTOCe68cxjtb9zJ7/mq6pSQxol8mk4e1vpbqrPH5vP3xHpZs2sPGz2uZUJjDp2E19d93Cf7nVwzlQF0j4wvP4ol3yijd9SWj+mdyoP4YK3fWUJjbg7zM2MyysQRvjOnypp7bl2ff29EyVNI9NZkXbp3A8+/vZMWOfdw3ddgJJZ4nD+tNXmY37nmllIbGZj4YWM1ZGWkU9+lJ2ReH2FJ1kKz0VIb3zeTF2y/mYP0x/vJuGavLa5i3bDsfbK8mJSmJmWP6x+x1WYI3xnR5A7LTWfFgSau2c3J78PD0UVEfk5KcxA3j8nninTKG9+3F+opacjPSuGhQDocaGqmsracg9/gHvb26pzKiXyaLPtzNrpojnDvAq7T5jeHtm29/OizBG2NMO91VUkzJiDySk4RrnlzOvsNHKcjNoPrgUSpr68nPaT2TZ3xhDi98UE5aShILbruI7qlJLXP7Y8E+ZDXGmHZKTU7igvxsRvXPJM/Noy/M7dGS2M+JSPChL3FNv6A/ORlpMU3uYAneGGPOmIjwdVe3pyA3g/wcr85NZIKfNKQ3lw/tzR2XD/YlLhuiMcaYDnDthQNZubOGEX0zWy4LGJngs3qkMv+2Cb7FZAneGGM6wPjCHN69dzIAJcPz+PGkIi4sOCvQmCzBG2NMB8vqkcrcq0cGHYaNwRtjTKKyBG+MMQnKErwxxiQoS/DGGJOgLMEbY0yCsgRvjDEJyhK8McYkKEvwxhiToES146/k3V4i8gXwaTsffjZQ3YHhdBSLq+3iNTaLq20srrZrT2wFqtr7ZHfEVYI/EyKyRlXHBR1HJIur7eI1NourbSyutuvo2GyIxhhjEpQleGOMSVCJlOCfCTqAKCyutovX2CyutrG42q5DY0uYMXhjjDGtJdIRvDHGmDCW4I0xJkF1+gQvIlNFZKuIlInI/QHGkS8i74rIZhHZJCJ3u/aHRWS3iJS621UBxVcuIhtcDGtcW46IvCUi29xPXy8/IyLDwvqlVEQOiMicIPpMRJ4Xkb0isjGs7aT9I54n3D73kYiMDSC2P4jIFvf3F4lItmsvFJG6sL6b53NcUbediDzg+myriFzpc1wvh8VULiKlrt3P/oqWI2K3n6lqp70BycB2YBCQBqwHRgYUSz9grFvuBXwCjAQeBu6Ng74qB86OaHsMuN8t3w88GvC2rAIKgugz4DJgLLDxq/oHuAr4LyDAxcDKAGL7JpDilh8Ni60wfL0A4jrptnPvhfVAN6DIvW+T/Yor4v4/Ar8OoL+i5YiY7Wed/Qh+AlCmqjtU9SjwEjAjiEBUtVJV17nlg8DHwIAgYmmDGcB8tzwfmBlgLCXAdlVt7zeZz4iqvgfURDRH658ZwAL1rACyRaSfn7Gp6hJVbXS/rgAGxurvtyWuU5gBvKSqDaq6EyjDe//6GpeICHA98GIs/vapnCJHxGw/6+wJfgDwWdjvFcRBUhWRQmAMsNI1/dSdYj3v9zBIGAWWiMhaEbndteWpaqVbrgLyggkNgFm0ftPFQ59F65942+9uwzvSCykSkQ9FZJmITAognpNtu3jps0nAHlXdFtbme39F5IiY7WedPcHHHRHpCfwTmKOqB4C/AoOB0UAl3ulhEC5V1bHANOBOEbks/E71zgkDmTMrImnAdOBV1xQvfdYiyP45FRGZCzQCC11TJXCOqo4B7gH+ISKZPoYUd9suwvdofSDhe3+dJEe06Oj9rLMn+N1AftjvA11bIEQkFW/DLVTV1wFUdY+qNqlqM/AsMTot/Sqqutv93AsscnHsCZ3yuZ97g4gN75/OOlXd42KMiz4jev/ExX4nIrcC1wA3ucSAGwLZ55bX4o11D/UrplNsu8D7TERSgO8AL4fa/O6vk+UIYrifdfYEvxooFpEidxQ4C1gcRCBubO854GNVfTysPXzM7NvAxsjH+hBbhoj0Ci3jfUC3Ea+vbnGr3QK84XdsTqujqnjoMyda/ywGfuBmOVwM1IadYvtCRKYCvwSmq+qRsPbeIpLslgcBxcAOH+OKtu0WA7NEpJuIFLm4VvkVlzMF2KKqFaEGP/srWo4glvuZH58ex/KG90nzJ3j/eecGGMeleKdWHwGl7nYV8Ddgg2tfDPQLILZBeDMY1gObQv0E5AJLgW3A20BOALFlAPuArLA23/sM7x9MJXAMb6xzdrT+wZvV8JTb5zYA4wKIrQxvfDa0r81z617rtnEpsA74ls9xRd12wFzXZ1uBaX7G5dpfAO6IWNfP/oqWI2K2n1mpAmOMSVCdfYjGGGNMFJbgjTEmQVmCN8aYBGUJ3hhjEpQleGOMSVCW4I3pACIyWUT+HXQcxoSzBG+MMQnKErzpUkTk+yKyytX+flpEkkXkkIj8ydXoXioivd26o0VkhRyvuR6q0z1ERN4WkfUisk5EBrun7ykir4lXp32h++aiMYGxBG+6DBEZAdwATFTV0UATcBPet2nXqOooYBnwG/eQBcB9qno+3jcJQ+0LgadU9QLga3jfmgSvOuAcvBrfg4CJMX9RxpxCStABGOOjEuBCYLU7uE7HK+zUzPECVH8HXheRLCBbVZe59vnAq66mzwBVXQSgqvUA7vlWqatzIt4VgwqB5bF/WcacnCV405UIMF9VH2jVKPJQxHrtrd/RELbchL2/TMBsiMZ0JUuB60SkD7RcC7MA731wnVvnRmC5qtYC+8MuAHEzsEy9K/FUiMhM9xzdRKSHr6/CmNNkRximy1DVzSLyK7wrWyXhVRu8EzgMTHD37cUbpwevdOs8l8B3AD907TcDT4vII+45vuvjyzDmtFk1SdPlicghVe0ZdBzGdDQbojHGmARlR/DGGJOg7AjeGGMSlCV4Y4xJUJbgjTEmQVmCN8aYBGUJ3hhjEtT/AWyHevWelAuYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yusc82JxLlCI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNJ-dBNaStU_",
    "outputId": "b753c966-a7fd-44e8-8d0d-ff54b87f1b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7020388105758069\n",
      "0.7397104473272389\n",
      "0.7321598940789346\n",
      "0.7251979246114018\n",
      "0.7258058638290028\n"
     ]
    }
   ],
   "source": [
    "cv_kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "models = []\n",
    "X = X_train\n",
    "X_extra = X_train2\n",
    "y = y_train\n",
    "test   = {\"text\": X_val, \"extra\": X_val2}\n",
    "\n",
    "y_test = y_val\n",
    "\n",
    "for train_index, test_index in cv_kfold.split(X, y):\n",
    "  model = build_model()\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      #optimizer=tf.keras.optimizers.Adam(learning_rate=0.007, amsgrad=True),\n",
    "      optimizer = tf.keras.optimizers.SGD(learning_rate=0.007, momentum=0.4, nesterov=True),\n",
    "      metrics=[\"accuracy\"]\n",
    "  )\n",
    "  input_train = {\"text\": X[train_index], \"extra\": X_extra[train_index]}\n",
    "  input_val   = {\"text\": X[test_index], \"extra\": X_extra[test_index]}\n",
    "  y_train = y[train_index]\n",
    "  y_val = y[test_index]\n",
    "  f1_callback1 = ROCCallback(validation = (input_val, y_val), verbose = 0)                                   \n",
    "  best_callback1 = ReturnBestEarlyStopping(monitor=\"val_f1\",\n",
    "                                          min_delta=0,\n",
    "                                          patience=200,\n",
    "                                          verbose=0,\n",
    "                                          mode=\"max\",\n",
    "                                          #baseline=0.72,\n",
    "                                          restore_best_weights=True)\n",
    "  model.fit(input_train, y_train, batch_size=128, epochs=200, validation_data=(input_val, y_val), callbacks=[f1_callback1, best_callback1], verbose=0)\n",
    "  y_test_pred = np.where(model.predict(test) >0.5,1,0)\n",
    "  print(f1_score(y_test,y_test_pred,average=\"macro\"))\n",
    "  models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDgupccPUMPY"
   },
   "outputs": [],
   "source": [
    "def ensemble_predictions(members, testX):\n",
    "  # make predictions\n",
    "  results = []\n",
    "  yhats = [np.squeeze(np.where(models[0].predict(test) > 0.5, 1,0).reshape(1,-1)) for model in members]\n",
    "  # sum across ensemble members\n",
    "  yhats = np.array(yhats)\n",
    "  print(yhats.shape)\n",
    "\n",
    "  for i in range(yhats.shape[1]):\n",
    "    counts = np.bincount(yhats[:,i])\n",
    "    results.append(np.argmax(counts))\n",
    "  # argmax across classes\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kv5bzfLMbQaw",
    "outputId": "e3754d39-2c1d-4887-b552-237df1bea241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 684)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72       370\n",
      "           1       0.67      0.70      0.68       314\n",
      "\n",
      "    accuracy                           0.70       684\n",
      "   macro avg       0.70      0.70      0.70       684\n",
      "weighted avg       0.70      0.70      0.70       684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = ensemble_predictions(models, test)\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjZoaZNzggCy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
