{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON4ys1VjOxiF"
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir transformers sentencepiece\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gByuQVqQK3KH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\r\n",
    "from transformers import AutoModel, AutoTokenizer\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import torch\r\n",
    "import wget\r\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUeWrF5U54LQ"
   },
   "outputs": [],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvrtsRpk6Gwc"
   },
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylnZfvXoKoLX"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\", do_lower_case=True, use_fast=False)\r\n",
    "model = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "220v8iDwO8u9"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"@user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user Infatti io per questo ho votato lega ðŸ’š sentii le sue promesse e prioritÃ  contro i clandestini e l' Islam ðŸ’š \")).unsqueeze(0)  \r\n",
    "token_list = tokenizer.convert_ids_to_tokens(tokenizer.encode(\"@user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user Infatti io per questo ho votato lega ðŸ’š sentii le sue promesse e prioritÃ  contro i clandestini e l' Islam ðŸ’š \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6PfZ6YNRttB"
   },
   "outputs": [],
   "source": [
    "print(input_ids)\r\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zgp_PEAZefz"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/alessandrocuda/SaRaH/main/dataset/haspeede2_dev/haspeede2_dev_taskAB.tsv'\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(url, delimiter=r'\\t', header=None, engine='python')\n",
    "df.columns =['id', 'sentences', 'hs', 'stereotype'] \n",
    "df = df.drop([0])\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uutmSd_KZ4qs"
   },
   "outputs": [],
   "source": [
    "sentences = df.sentences.values\n",
    "labels = np.array(df.hs.values, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i7ptlb-cZAQ"
   },
   "outputs": [],
   "source": [
    "print(sentences[6215])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zq99sfAyZzrj"
   },
   "outputs": [],
   "source": [
    "max_len_token_sent = 0\n",
    "max_token_sent = 0\n",
    "max_len_str_sent = 0\n",
    "max_str_sent = 0\n",
    "id = 0\n",
    "ids = []\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len_token_sent = max(max_len_token_sent, len(input_ids))\n",
    "    if max_len_token_sent == len(input_ids):\n",
    "        max_token_sent = sent\n",
    "    max_len_str_sent = max(max_len_str_sent, len(sent))\n",
    "    if max_len_str_sent == len(sent):\n",
    "        max_str_sent = sent\n",
    "    if len(sent) >280:\n",
    "      ids.append(id)\n",
    "    id +=1\n",
    "\n",
    "print('Max Token sentence length: ', max_len_token_sent)\n",
    "print(max_token_sent)\n",
    "print('Max string sentence length: ', max_len_str_sent)\n",
    "print(max_str_sent)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Mwu27T5pnNz"
   },
   "outputs": [],
   "source": [
    "sentences = np.delete(sentences, ids)\n",
    "labels = np.delete(labels, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzYtiQUJqDlu"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "max_sent = 0\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if max_len == len(input_ids):\n",
    "        max_sent = sent\n",
    "\n",
    "print('Max sentence length: ', max_len)\n",
    "print(max_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l69s4a11f7Hu"
   },
   "outputs": [],
   "source": [
    "input_ids = []\r\n",
    "attention_masks = []\r\n",
    "\r\n",
    "# For every sentence...\r\n",
    "for sent in sentences:\r\n",
    "    # `encode_plus` will:\r\n",
    "    #   (1) Tokenize the sentence.\r\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\r\n",
    "    #   (3) Append the `[SEP]` token to the end.\r\n",
    "    #   (4) Map tokens to their IDs.\r\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\r\n",
    "    #   (6) Create attention masks for [PAD] tokens.\r\n",
    "    encoded_dict = tokenizer.encode_plus(\r\n",
    "                        sent,                      # Sentence to encode.\r\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n",
    "                        max_length = 90,           # Pad & truncate all sentences.\r\n",
    "                        pad_to_max_length = True,\r\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\r\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\r\n",
    "                   )\r\n",
    "    \r\n",
    "    # Add the encoded sentence to the list.    \r\n",
    "    input_ids.append(encoded_dict['input_ids'])\r\n",
    "    \r\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\r\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
    "\r\n",
    "# Convert the lists into tensors.\r\n",
    "input_ids = torch.cat(input_ids, dim=0)\r\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
    "labels = torch.tensor(labels)\r\n",
    "\r\n",
    "# Print sentence 0, now as a list of IDs.\r\n",
    "print('Original: ', sentences[0])\r\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5V5Kj-NndhEy"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\r\n",
    "\r\n",
    "# Combine the training inputs into a TensorDataset.\r\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\r\n",
    "\r\n",
    "# Create a 90-10 train-validation split.\r\n",
    "\r\n",
    "# Calculate the number of samples to include in each set.\r\n",
    "train_size = int(0.9 * len(dataset))\r\n",
    "val_size = len(dataset) - train_size\r\n",
    "\r\n",
    "# Divide the dataset by randomly selecting samples.\r\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\r\n",
    "\r\n",
    "print('{:>5,} training samples'.format(train_size))\r\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhZJrZdM1uAh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
    "\r\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \r\n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n",
    "# size of 16 or 32.\r\n",
    "batch_size = 32\r\n",
    "\r\n",
    "# Create the DataLoaders for our training and validation sets.\r\n",
    "# We'll take training samples in random order. \r\n",
    "train_dataloader = DataLoader(\r\n",
    "            train_dataset,  # The training samples.\r\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\r\n",
    "            batch_size = batch_size # Trains with this batch size.\r\n",
    "        )\r\n",
    "\r\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\r\n",
    "validation_dataloader = DataLoader(\r\n",
    "            val_dataset, # The validation samples.\r\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\r\n",
    "            batch_size = batch_size # Evaluate with this batch size.\r\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fSA6oeP2waL"
   },
   "outputs": [],
   "source": [
    "input_ids.to(device)\n",
    "attention_masks.to(device)\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_masks)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMRBUqKJKZ+rn6mybBtMihP",
   "collapsed_sections": [],
   "name": "NLP_example.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
